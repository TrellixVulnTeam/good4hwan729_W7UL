{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: pyvirtualdisplay in /opt/conda/lib/python3.7/site-packages (2.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: EasyProcess in /opt/conda/lib/python3.7/site-packages (from pyvirtualdisplay) (0.3)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.1.6-1~deb10u1).\n",
      "python-opengl is already the newest version (3.1.0+dfsg-2).\n",
      "xvfb is already the newest version (2:1.20.4-1+deb10u2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /home/good4hwan/.local/lib/python3.7/site-packages (56.1.0)\n",
      "Requirement already satisfied: ez_setup in /opt/conda/lib/python3.7/site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in /opt/conda/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.19.5)\n",
      "Requirement already satisfied: atari-py~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: opencv-python>=3. in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (4.5.1.48)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 227   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 2.0   memory length: 448   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 2   score: 0.0   memory length: 570   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 2.0   memory length: 768   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 1.0   memory length: 918   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 5   score: 0.0   memory length: 1041   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 6   score: 0.0   memory length: 1164   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
      "episode: 7   score: 1.0   memory length: 1334   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.125\n",
      "episode: 8   score: 2.0   memory length: 1531   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.2222222222222223\n",
      "episode: 9   score: 1.0   memory length: 1682   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 10   score: 1.0   memory length: 1852   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.1818181818181819\n",
      "episode: 11   score: 0.0   memory length: 1975   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0833333333333333\n",
      "episode: 12   score: 0.0   memory length: 2098   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 13   score: 1.0   memory length: 2267   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 14   score: 2.0   memory length: 2483   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.0666666666666667\n",
      "episode: 15   score: 1.0   memory length: 2654   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.0625\n",
      "episode: 16   score: 3.0   memory length: 2921   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.1764705882352942\n",
      "episode: 17   score: 1.0   memory length: 3091   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 18   score: 2.0   memory length: 3288   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.2105263157894737\n",
      "episode: 19   score: 4.0   memory length: 3566   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 20   score: 3.0   memory length: 3833   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 21   score: 2.0   memory length: 4031   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 22   score: 3.0   memory length: 4276   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5217391304347827\n",
      "episode: 23   score: 0.0   memory length: 4399   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4583333333333333\n",
      "episode: 24   score: 3.0   memory length: 4666   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 25   score: 0.0   memory length: 4789   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 26   score: 1.0   memory length: 4940   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 27   score: 3.0   memory length: 5186   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 28   score: 4.0   memory length: 5485   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.5862068965517242\n",
      "episode: 29   score: 1.0   memory length: 5636   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5666666666666667\n",
      "episode: 30   score: 6.0   memory length: 5979   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.7096774193548387\n",
      "episode: 31   score: 2.0   memory length: 6177   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71875\n",
      "episode: 32   score: 1.0   memory length: 6346   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.696969696969697\n",
      "episode: 33   score: 1.0   memory length: 6498   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6764705882352942\n",
      "episode: 34   score: 2.0   memory length: 6696   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6857142857142857\n",
      "episode: 35   score: 2.0   memory length: 6914   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6944444444444444\n",
      "episode: 36   score: 0.0   memory length: 7037   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
      "episode: 37   score: 1.0   memory length: 7187   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.631578947368421\n",
      "episode: 38   score: 0.0   memory length: 7310   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5897435897435896\n",
      "episode: 39   score: 3.0   memory length: 7538   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 40   score: 1.0   memory length: 7710   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6097560975609757\n",
      "episode: 41   score: 1.0   memory length: 7880   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5952380952380953\n",
      "episode: 42   score: 0.0   memory length: 8002   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.558139534883721\n",
      "episode: 43   score: 0.0   memory length: 8125   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5227272727272727\n",
      "episode: 44   score: 4.0   memory length: 8384   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.5777777777777777\n",
      "episode: 45   score: 2.0   memory length: 8601   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5869565217391304\n",
      "episode: 46   score: 1.0   memory length: 8769   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.574468085106383\n",
      "episode: 47   score: 2.0   memory length: 8967   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 48   score: 2.0   memory length: 9165   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5918367346938775\n",
      "episode: 49   score: 2.0   memory length: 9363   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 50   score: 1.0   memory length: 9533   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 51   score: 2.0   memory length: 9751   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5961538461538463\n",
      "episode: 52   score: 2.0   memory length: 9951   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6037735849056605\n",
      "episode: 53   score: 0.0   memory length: 10073   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5740740740740742\n",
      "episode: 54   score: 0.0   memory length: 10196   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
      "episode: 55   score: 2.0   memory length: 10376   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.5535714285714286\n",
      "episode: 56   score: 1.0   memory length: 10526   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.543859649122807\n",
      "episode: 57   score: 4.0   memory length: 10819   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.5862068965517242\n",
      "episode: 58   score: 5.0   memory length: 11125   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.6440677966101696\n",
      "episode: 59   score: 0.0   memory length: 11247   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6166666666666667\n",
      "episode: 60   score: 1.0   memory length: 11397   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6065573770491803\n",
      "episode: 61   score: 2.0   memory length: 11614   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6129032258064515\n",
      "episode: 62   score: 1.0   memory length: 11764   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6031746031746033\n",
      "episode: 63   score: 2.0   memory length: 11963   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 0.0   memory length: 12086   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5846153846153845\n",
      "episode: 65   score: 0.0   memory length: 12208   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5606060606060606\n",
      "episode: 66   score: 4.0   memory length: 12483   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.5970149253731343\n",
      "episode: 67   score: 0.0   memory length: 12605   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5735294117647058\n",
      "episode: 68   score: 0.0   memory length: 12727   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5507246376811594\n",
      "episode: 69   score: 1.0   memory length: 12878   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.542857142857143\n",
      "episode: 70   score: 5.0   memory length: 13182   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.591549295774648\n",
      "episode: 71   score: 0.0   memory length: 13304   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5694444444444444\n",
      "episode: 72   score: 0.0   memory length: 13427   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.547945205479452\n",
      "episode: 73   score: 1.0   memory length: 13596   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5405405405405406\n",
      "episode: 74   score: 2.0   memory length: 13814   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5466666666666666\n",
      "episode: 75   score: 3.0   memory length: 14028   epsilon: 1.0    steps: 214    lr: 0.0001     evaluation reward: 1.5657894736842106\n",
      "episode: 76   score: 2.0   memory length: 14225   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 77   score: 5.0   memory length: 14541   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 78   score: 3.0   memory length: 14784   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.6329113924050633\n",
      "episode: 79   score: 3.0   memory length: 15029   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 80   score: 3.0   memory length: 15279   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 81   score: 1.0   memory length: 15450   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6585365853658536\n",
      "episode: 82   score: 1.0   memory length: 15619   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6506024096385543\n",
      "episode: 83   score: 1.0   memory length: 15788   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 84   score: 3.0   memory length: 16013   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6588235294117648\n",
      "episode: 85   score: 1.0   memory length: 16182   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6511627906976745\n",
      "episode: 86   score: 0.0   memory length: 16305   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.632183908045977\n",
      "episode: 87   score: 1.0   memory length: 16456   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 88   score: 1.0   memory length: 16607   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6179775280898876\n",
      "episode: 89   score: 0.0   memory length: 16729   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 90   score: 3.0   memory length: 16977   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 91   score: 3.0   memory length: 17223   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6304347826086956\n",
      "episode: 92   score: 3.0   memory length: 17490   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.6451612903225807\n",
      "episode: 93   score: 0.0   memory length: 17613   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.627659574468085\n",
      "episode: 94   score: 0.0   memory length: 17736   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6105263157894736\n",
      "episode: 95   score: 1.0   memory length: 17905   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6041666666666667\n",
      "episode: 96   score: 2.0   memory length: 18120   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.6082474226804124\n",
      "episode: 97   score: 1.0   memory length: 18289   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6020408163265305\n",
      "episode: 98   score: 2.0   memory length: 18488   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.606060606060606\n",
      "episode: 99   score: 1.0   memory length: 18658   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 100   score: 2.0   memory length: 18856   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 101   score: 2.0   memory length: 19074   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 102   score: 0.0   memory length: 19197   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 103   score: 1.0   memory length: 19368   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 104   score: 0.0   memory length: 19491   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 105   score: 6.0   memory length: 19826   epsilon: 1.0    steps: 335    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 106   score: 1.0   memory length: 19976   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 107   score: 1.0   memory length: 20145   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 108   score: 0.0   memory length: 20267   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 109   score: 0.0   memory length: 20390   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 110   score: 3.0   memory length: 20638   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 111   score: 1.0   memory length: 20788   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 112   score: 2.0   memory length: 20986   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 113   score: 0.0   memory length: 21108   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 114   score: 3.0   memory length: 21355   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 115   score: 2.0   memory length: 21552   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 116   score: 0.0   memory length: 21674   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 117   score: 1.0   memory length: 21824   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 118   score: 2.0   memory length: 22040   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 119   score: 3.0   memory length: 22285   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 120   score: 1.0   memory length: 22455   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 121   score: 2.0   memory length: 22652   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 122   score: 1.0   memory length: 22803   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 123   score: 2.0   memory length: 23003   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 124   score: 1.0   memory length: 23173   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 125   score: 4.0   memory length: 23486   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 126   score: 2.0   memory length: 23702   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 127   score: 5.0   memory length: 24040   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 128   score: 4.0   memory length: 24315   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   score: 2.0   memory length: 24512   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 130   score: 1.0   memory length: 24682   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 131   score: 0.0   memory length: 24805   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 132   score: 1.0   memory length: 24956   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 133   score: 0.0   memory length: 25078   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 134   score: 1.0   memory length: 25247   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 135   score: 0.0   memory length: 25370   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 136   score: 0.0   memory length: 25493   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 137   score: 2.0   memory length: 25691   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 138   score: 2.0   memory length: 25871   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 139   score: 2.0   memory length: 26089   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 140   score: 2.0   memory length: 26271   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 141   score: 2.0   memory length: 26488   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 142   score: 1.0   memory length: 26659   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 143   score: 0.0   memory length: 26782   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 144   score: 2.0   memory length: 26983   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 145   score: 1.0   memory length: 27151   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 146   score: 2.0   memory length: 27369   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 147   score: 1.0   memory length: 27539   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 148   score: 4.0   memory length: 27834   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 149   score: 2.0   memory length: 28052   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 150   score: 2.0   memory length: 28267   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 151   score: 2.0   memory length: 28465   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 152   score: 0.0   memory length: 28588   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 153   score: 2.0   memory length: 28786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 154   score: 3.0   memory length: 29032   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 155   score: 0.0   memory length: 29155   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 156   score: 0.0   memory length: 29278   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 157   score: 3.0   memory length: 29545   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 158   score: 1.0   memory length: 29715   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 159   score: 1.0   memory length: 29884   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 160   score: 0.0   memory length: 30007   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 161   score: 0.0   memory length: 30129   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 162   score: 2.0   memory length: 30327   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 163   score: 3.0   memory length: 30593   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 164   score: 3.0   memory length: 30858   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 165   score: 2.0   memory length: 31056   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 166   score: 1.0   memory length: 31207   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 167   score: 4.0   memory length: 31482   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 168   score: 1.0   memory length: 31650   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 169   score: 0.0   memory length: 31773   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 170   score: 2.0   memory length: 31971   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 171   score: 0.0   memory length: 32093   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 172   score: 0.0   memory length: 32216   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 173   score: 1.0   memory length: 32367   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 174   score: 0.0   memory length: 32490   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 175   score: 2.0   memory length: 32706   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 176   score: 0.0   memory length: 32828   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 177   score: 2.0   memory length: 33025   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 178   score: 2.0   memory length: 33243   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 179   score: 0.0   memory length: 33366   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 180   score: 0.0   memory length: 33489   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 181   score: 3.0   memory length: 33753   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 182   score: 0.0   memory length: 33876   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 183   score: 3.0   memory length: 34103   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 184   score: 0.0   memory length: 34226   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 185   score: 3.0   memory length: 34455   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 186   score: 0.0   memory length: 34578   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 187   score: 1.0   memory length: 34729   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 188   score: 1.0   memory length: 34880   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 189   score: 1.0   memory length: 35050   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 190   score: 1.0   memory length: 35200   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 191   score: 0.0   memory length: 35323   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 192   score: 2.0   memory length: 35521   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 193   score: 1.0   memory length: 35690   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 194   score: 2.0   memory length: 35889   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 195   score: 1.0   memory length: 36059   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 196   score: 0.0   memory length: 36181   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 197   score: 1.0   memory length: 36332   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 198   score: 2.0   memory length: 36530   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 199   score: 2.0   memory length: 36749   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 200   score: 0.0   memory length: 36871   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 201   score: 0.0   memory length: 36994   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 202   score: 2.0   memory length: 37213   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 203   score: 2.0   memory length: 37411   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 204   score: 0.0   memory length: 37534   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 205   score: 1.0   memory length: 37704   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 206   score: 0.0   memory length: 37827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 207   score: 2.0   memory length: 38011   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 208   score: 2.0   memory length: 38208   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 209   score: 3.0   memory length: 38437   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 210   score: 3.0   memory length: 38681   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 211   score: 3.0   memory length: 38928   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 212   score: 2.0   memory length: 39126   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 213   score: 4.0   memory length: 39400   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 214   score: 0.0   memory length: 39523   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 215   score: 3.0   memory length: 39789   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 216   score: 2.0   memory length: 39986   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 217   score: 2.0   memory length: 40204   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 218   score: 3.0   memory length: 40430   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 219   score: 2.0   memory length: 40647   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 220   score: 1.0   memory length: 40815   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 221   score: 2.0   memory length: 40996   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 222   score: 1.0   memory length: 41168   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 223   score: 1.0   memory length: 41338   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 224   score: 0.0   memory length: 41460   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 225   score: 1.0   memory length: 41611   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 226   score: 4.0   memory length: 41887   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 227   score: 1.0   memory length: 42056   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 228   score: 2.0   memory length: 42272   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 229   score: 0.0   memory length: 42395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 230   score: 0.0   memory length: 42518   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 231   score: 2.0   memory length: 42698   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 232   score: 2.0   memory length: 42896   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 233   score: 4.0   memory length: 43212   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 234   score: 1.0   memory length: 43381   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 235   score: 1.0   memory length: 43552   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 236   score: 3.0   memory length: 43797   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 237   score: 0.0   memory length: 43920   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 238   score: 4.0   memory length: 44215   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 239   score: 0.0   memory length: 44337   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 240   score: 0.0   memory length: 44460   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 241   score: 0.0   memory length: 44583   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 242   score: 0.0   memory length: 44705   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 243   score: 0.0   memory length: 44828   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 244   score: 0.0   memory length: 44951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 245   score: 0.0   memory length: 45074   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 246   score: 3.0   memory length: 45336   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 247   score: 6.0   memory length: 45681   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 248   score: 1.0   memory length: 45831   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 249   score: 2.0   memory length: 46029   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 250   score: 2.0   memory length: 46226   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 251   score: 0.0   memory length: 46348   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 252   score: 3.0   memory length: 46594   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 253   score: 0.0   memory length: 46717   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 254   score: 3.0   memory length: 46965   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 255   score: 0.0   memory length: 47088   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 256   score: 3.0   memory length: 47332   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 257   score: 2.0   memory length: 47551   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 258   score: 2.0   memory length: 47737   epsilon: 1.0    steps: 186    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 259   score: 0.0   memory length: 47860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 260   score: 2.0   memory length: 48077   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 261   score: 3.0   memory length: 48340   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 262   score: 2.0   memory length: 48558   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 263   score: 5.0   memory length: 48862   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 264   score: 1.0   memory length: 49032   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 265   score: 5.0   memory length: 49341   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 266   score: 2.0   memory length: 49522   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 267   score: 0.0   memory length: 49645   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 268   score: 5.0   memory length: 49988   epsilon: 1.0    steps: 343    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 269   score: 0.0   memory length: 50111   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 270   score: 1.0   memory length: 50261   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 271   score: 2.0   memory length: 50479   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 272   score: 1.0   memory length: 50649   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 273   score: 0.0   memory length: 50771   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 274   score: 1.0   memory length: 50942   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 275   score: 2.0   memory length: 51139   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 276   score: 1.0   memory length: 51289   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 277   score: 1.0   memory length: 51440   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 278   score: 0.0   memory length: 51563   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 279   score: 1.0   memory length: 51714   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 280   score: 4.0   memory length: 52011   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 281   score: 4.0   memory length: 52288   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 282   score: 1.0   memory length: 52458   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 283   score: 1.0   memory length: 52609   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 284   score: 0.0   memory length: 52731   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 285   score: 3.0   memory length: 52975   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 286   score: 3.0   memory length: 53223   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 287   score: 1.0   memory length: 53373   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 288   score: 1.0   memory length: 53542   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 289   score: 0.0   memory length: 53665   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 290   score: 2.0   memory length: 53883   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 291   score: 1.0   memory length: 54034   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 292   score: 0.0   memory length: 54157   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 293   score: 3.0   memory length: 54401   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 294   score: 0.0   memory length: 54523   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 295   score: 2.0   memory length: 54738   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 296   score: 1.0   memory length: 54889   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 297   score: 2.0   memory length: 55105   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 298   score: 0.0   memory length: 55228   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 299   score: 1.0   memory length: 55396   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 300   score: 2.0   memory length: 55594   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 301   score: 0.0   memory length: 55717   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 302   score: 1.0   memory length: 55889   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 303   score: 0.0   memory length: 56012   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 304   score: 1.0   memory length: 56181   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 305   score: 2.0   memory length: 56378   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 306   score: 0.0   memory length: 56500   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 307   score: 3.0   memory length: 56747   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 308   score: 0.0   memory length: 56870   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 309   score: 2.0   memory length: 57086   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 310   score: 0.0   memory length: 57209   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 311   score: 3.0   memory length: 57453   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 312   score: 0.0   memory length: 57576   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 313   score: 1.0   memory length: 57726   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 314   score: 3.0   memory length: 57973   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 315   score: 0.0   memory length: 58095   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 316   score: 1.0   memory length: 58264   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 317   score: 0.0   memory length: 58387   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 318   score: 1.0   memory length: 58538   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 319   score: 4.0   memory length: 58835   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 320   score: 1.0   memory length: 59005   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 321   score: 0.0   memory length: 59128   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 322   score: 2.0   memory length: 59325   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 323   score: 1.0   memory length: 59495   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 324   score: 0.0   memory length: 59618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 325   score: 0.0   memory length: 59741   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 326   score: 4.0   memory length: 60023   epsilon: 1.0    steps: 282    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 327   score: 1.0   memory length: 60174   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 328   score: 1.0   memory length: 60343   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 329   score: 5.0   memory length: 60668   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 330   score: 1.0   memory length: 60836   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 331   score: 0.0   memory length: 60958   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 332   score: 0.0   memory length: 61081   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 333   score: 1.0   memory length: 61250   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 334   score: 2.0   memory length: 61447   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 335   score: 2.0   memory length: 61627   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 336   score: 0.0   memory length: 61750   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 337   score: 1.0   memory length: 61922   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 338   score: 2.0   memory length: 62104   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 339   score: 1.0   memory length: 62254   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 340   score: 3.0   memory length: 62521   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 341   score: 2.0   memory length: 62718   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 342   score: 1.0   memory length: 62887   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 343   score: 2.0   memory length: 63105   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 344   score: 2.0   memory length: 63302   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 345   score: 3.0   memory length: 63536   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 346   score: 1.0   memory length: 63704   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 347   score: 2.0   memory length: 63884   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 348   score: 1.0   memory length: 64053   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 349   score: 2.0   memory length: 64252   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 350   score: 0.0   memory length: 64375   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 351   score: 2.0   memory length: 64572   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 352   score: 0.0   memory length: 64694   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 353   score: 0.0   memory length: 64817   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 354   score: 2.0   memory length: 65015   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 355   score: 3.0   memory length: 65282   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 356   score: 2.0   memory length: 65479   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 357   score: 2.0   memory length: 65677   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 358   score: 2.0   memory length: 65875   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 359   score: 3.0   memory length: 66142   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 360   score: 2.0   memory length: 66340   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 361   score: 0.0   memory length: 66462   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 362   score: 1.0   memory length: 66613   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 363   score: 1.0   memory length: 66782   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 364   score: 2.0   memory length: 66982   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 365   score: 2.0   memory length: 67203   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 366   score: 2.0   memory length: 67385   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 367   score: 3.0   memory length: 67632   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 368   score: 0.0   memory length: 67755   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 369   score: 1.0   memory length: 67908   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 370   score: 1.0   memory length: 68059   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 371   score: 2.0   memory length: 68256   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 372   score: 1.0   memory length: 68427   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 373   score: 0.0   memory length: 68550   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 374   score: 0.0   memory length: 68672   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 375   score: 3.0   memory length: 68901   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 376   score: 4.0   memory length: 69176   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 377   score: 1.0   memory length: 69345   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 378   score: 2.0   memory length: 69543   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 379   score: 1.0   memory length: 69695   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 380   score: 1.0   memory length: 69866   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 381   score: 0.0   memory length: 69989   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 382   score: 1.0   memory length: 70140   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 383   score: 3.0   memory length: 70405   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 384   score: 2.0   memory length: 70621   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 385   score: 0.0   memory length: 70743   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 386   score: 2.0   memory length: 70941   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 387   score: 4.0   memory length: 71201   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 388   score: 3.0   memory length: 71426   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 389   score: 0.0   memory length: 71548   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 390   score: 2.0   memory length: 71749   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 391   score: 5.0   memory length: 72057   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 392   score: 6.0   memory length: 72405   epsilon: 1.0    steps: 348    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 393   score: 4.0   memory length: 72662   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 394   score: 3.0   memory length: 72909   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 395   score: 0.0   memory length: 73032   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 396   score: 1.0   memory length: 73201   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 397   score: 0.0   memory length: 73323   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 398   score: 1.0   memory length: 73473   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 399   score: 0.0   memory length: 73595   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 400   score: 0.0   memory length: 73717   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 401   score: 1.0   memory length: 73888   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 402   score: 2.0   memory length: 74106   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 403   score: 0.0   memory length: 74229   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 404   score: 1.0   memory length: 74379   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405   score: 3.0   memory length: 74623   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 406   score: 0.0   memory length: 74746   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 407   score: 2.0   memory length: 74929   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 408   score: 0.0   memory length: 75052   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 409   score: 4.0   memory length: 75350   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 410   score: 0.0   memory length: 75473   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 411   score: 1.0   memory length: 75624   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 412   score: 1.0   memory length: 75794   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 413   score: 0.0   memory length: 75916   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 414   score: 4.0   memory length: 76233   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 415   score: 3.0   memory length: 76476   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 416   score: 2.0   memory length: 76692   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 417   score: 0.0   memory length: 76815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 418   score: 1.0   memory length: 76966   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 419   score: 3.0   memory length: 77181   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 420   score: 3.0   memory length: 77429   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 421   score: 0.0   memory length: 77552   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 422   score: 0.0   memory length: 77675   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 423   score: 0.0   memory length: 77798   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 424   score: 3.0   memory length: 78026   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 425   score: 1.0   memory length: 78177   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 426   score: 1.0   memory length: 78346   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 427   score: 0.0   memory length: 78469   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 428   score: 2.0   memory length: 78667   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 429   score: 4.0   memory length: 78944   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 430   score: 3.0   memory length: 79191   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 431   score: 0.0   memory length: 79313   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 432   score: 1.0   memory length: 79463   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 433   score: 1.0   memory length: 79632   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 434   score: 0.0   memory length: 79755   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 435   score: 2.0   memory length: 79973   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 436   score: 3.0   memory length: 80219   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 437   score: 1.0   memory length: 80387   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 438   score: 2.0   memory length: 80605   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 439   score: 0.0   memory length: 80728   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 440   score: 1.0   memory length: 80899   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 441   score: 1.0   memory length: 81069   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 442   score: 0.0   memory length: 81192   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 443   score: 0.0   memory length: 81315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 444   score: 2.0   memory length: 81533   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 445   score: 2.0   memory length: 81731   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 446   score: 2.0   memory length: 81931   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 447   score: 4.0   memory length: 82206   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 448   score: 1.0   memory length: 82357   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 449   score: 3.0   memory length: 82604   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 450   score: 1.0   memory length: 82754   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 451   score: 5.0   memory length: 83112   epsilon: 1.0    steps: 358    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 452   score: 0.0   memory length: 83235   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 453   score: 2.0   memory length: 83452   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 454   score: 0.0   memory length: 83575   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 455   score: 3.0   memory length: 83842   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 456   score: 2.0   memory length: 84040   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 457   score: 3.0   memory length: 84288   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 458   score: 0.0   memory length: 84410   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 459   score: 2.0   memory length: 84594   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 460   score: 0.0   memory length: 84716   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 461   score: 1.0   memory length: 84888   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 462   score: 0.0   memory length: 85011   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 463   score: 3.0   memory length: 85258   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 464   score: 1.0   memory length: 85429   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 465   score: 1.0   memory length: 85598   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 466   score: 5.0   memory length: 85921   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 467   score: 1.0   memory length: 86090   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 468   score: 3.0   memory length: 86337   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 469   score: 3.0   memory length: 86581   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 470   score: 1.0   memory length: 86752   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 471   score: 1.0   memory length: 86903   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 472   score: 2.0   memory length: 87101   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 473   score: 1.0   memory length: 87269   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 474   score: 2.0   memory length: 87486   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 475   score: 3.0   memory length: 87752   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 476   score: 2.0   memory length: 87951   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 477   score: 2.0   memory length: 88148   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 478   score: 0.0   memory length: 88270   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 479   score: 0.0   memory length: 88392   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 480   score: 2.0   memory length: 88609   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 481   score: 2.0   memory length: 88807   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 482   score: 2.0   memory length: 89008   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 483   score: 3.0   memory length: 89236   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 484   score: 1.0   memory length: 89407   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 485   score: 0.0   memory length: 89530   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 486   score: 2.0   memory length: 89730   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 487   score: 0.0   memory length: 89852   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 488   score: 1.0   memory length: 90021   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 489   score: 3.0   memory length: 90251   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 490   score: 0.0   memory length: 90374   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 491   score: 1.0   memory length: 90544   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 492   score: 3.0   memory length: 90772   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 493   score: 1.0   memory length: 90923   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 494   score: 0.0   memory length: 91046   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 495   score: 2.0   memory length: 91226   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 496   score: 1.0   memory length: 91396   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 497   score: 3.0   memory length: 91622   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 498   score: 2.0   memory length: 91820   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 499   score: 0.0   memory length: 91943   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 500   score: 0.0   memory length: 92065   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 501   score: 2.0   memory length: 92263   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 502   score: 2.0   memory length: 92461   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 503   score: 1.0   memory length: 92633   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 504   score: 2.0   memory length: 92833   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 505   score: 2.0   memory length: 93019   epsilon: 1.0    steps: 186    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 506   score: 1.0   memory length: 93189   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 507   score: 0.0   memory length: 93312   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 508   score: 2.0   memory length: 93496   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 509   score: 2.0   memory length: 93695   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 510   score: 2.0   memory length: 93893   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 511   score: 2.0   memory length: 94075   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 512   score: 1.0   memory length: 94243   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 513   score: 0.0   memory length: 94366   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 514   score: 1.0   memory length: 94534   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 515   score: 1.0   memory length: 94685   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 516   score: 1.0   memory length: 94836   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 517   score: 2.0   memory length: 95053   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 518   score: 2.0   memory length: 95251   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 519   score: 2.0   memory length: 95451   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 520   score: 1.0   memory length: 95620   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 521   score: 2.0   memory length: 95817   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 522   score: 1.0   memory length: 95985   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 523   score: 2.0   memory length: 96182   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 524   score: 3.0   memory length: 96408   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 525   score: 5.0   memory length: 96750   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 526   score: 0.0   memory length: 96873   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 527   score: 3.0   memory length: 97137   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 528   score: 2.0   memory length: 97354   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 529   score: 2.0   memory length: 97569   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 530   score: 1.0   memory length: 97739   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 531   score: 0.0   memory length: 97861   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 532   score: 3.0   memory length: 98111   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 533   score: 0.0   memory length: 98233   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 534   score: 2.0   memory length: 98451   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 535   score: 1.0   memory length: 98602   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 536   score: 2.0   memory length: 98800   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 537   score: 2.0   memory length: 98998   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 538   score: 0.0   memory length: 99121   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 539   score: 0.0   memory length: 99244   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 540   score: 1.0   memory length: 99412   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 541   score: 5.0   memory length: 99743   epsilon: 1.0    steps: 331    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 542   score: 0.0   memory length: 99866   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/good4hwan/assignment5/memory.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sample = np.array(sample)\n",
      "/home/good4hwan/assignment5/agent_double.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 543   score: 3.0   memory length: 100133   epsilon: 0.9997346800000058    steps: 267    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 544   score: 0.0   memory length: 100255   epsilon: 0.999493120000011    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 545   score: 1.0   memory length: 100406   epsilon: 0.9991941400000175    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 546   score: 2.0   memory length: 100625   epsilon: 0.9987605200000269    steps: 219    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 547   score: 0.0   memory length: 100748   epsilon: 0.9985169800000322    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 548   score: 3.0   memory length: 101014   epsilon: 0.9979903000000436    steps: 266    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 549   score: 5.0   memory length: 101380   epsilon: 0.9972656200000594    steps: 366    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 550   score: 2.0   memory length: 101560   epsilon: 0.9969092200000671    steps: 180    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 551   score: 2.0   memory length: 101742   epsilon: 0.9965488600000749    steps: 182    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 552   score: 1.0   memory length: 101894   epsilon: 0.9962479000000815    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 553   score: 1.0   memory length: 102063   epsilon: 0.9959132800000887    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 554   score: 0.0   memory length: 102185   epsilon: 0.995671720000094    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 555   score: 0.0   memory length: 102308   epsilon: 0.9954281800000992    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 556   score: 3.0   memory length: 102554   epsilon: 0.9949411000001098    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 557   score: 1.0   memory length: 102722   epsilon: 0.994608460000117    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 558   score: 0.0   memory length: 102845   epsilon: 0.9943649200001223    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 559   score: 1.0   memory length: 103014   epsilon: 0.9940303000001296    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 560   score: 2.0   memory length: 103229   epsilon: 0.9936046000001388    steps: 215    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 561   score: 0.0   memory length: 103351   epsilon: 0.9933630400001441    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 562   score: 1.0   memory length: 103501   epsilon: 0.9930660400001505    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 563   score: 2.0   memory length: 103699   epsilon: 0.992674000000159    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 564   score: 1.0   memory length: 103867   epsilon: 0.9923413600001663    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 565   score: 1.0   memory length: 104018   epsilon: 0.9920423800001728    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 566   score: 4.0   memory length: 104293   epsilon: 0.9914978800001846    steps: 275    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 567   score: 2.0   memory length: 104511   epsilon: 0.9910662400001939    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 568   score: 1.0   memory length: 104680   epsilon: 0.9907316200002012    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 569   score: 2.0   memory length: 104878   epsilon: 0.9903395800002097    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 570   score: 1.0   memory length: 105029   epsilon: 0.9900406000002162    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 571   score: 2.0   memory length: 105226   epsilon: 0.9896505400002247    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 572   score: 1.0   memory length: 105376   epsilon: 0.9893535400002311    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 573   score: 0.0   memory length: 105498   epsilon: 0.9891119800002364    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 574   score: 2.0   memory length: 105679   epsilon: 0.9887536000002441    steps: 181    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 575   score: 4.0   memory length: 105931   epsilon: 0.988254640000255    steps: 252    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 576   score: 2.0   memory length: 106129   epsilon: 0.9878626000002635    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 577   score: 1.0   memory length: 106301   epsilon: 0.9875220400002709    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 578   score: 2.0   memory length: 106498   epsilon: 0.9871319800002794    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 579   score: 1.0   memory length: 106666   epsilon: 0.9867993400002866    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 580   score: 1.0   memory length: 106836   epsilon: 0.9864627400002939    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 581   score: 0.0   memory length: 106958   epsilon: 0.9862211800002991    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 582   score: 2.0   memory length: 107176   epsilon: 0.9857895400003085    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 583   score: 2.0   memory length: 107373   epsilon: 0.985399480000317    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 584   score: 2.0   memory length: 107570   epsilon: 0.9850094200003254    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 585   score: 1.0   memory length: 107740   epsilon: 0.9846728200003327    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 586   score: 2.0   memory length: 107938   epsilon: 0.9842807800003412    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 587   score: 0.0   memory length: 108061   epsilon: 0.9840372400003465    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 588   score: 3.0   memory length: 108308   epsilon: 0.9835481800003572    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 589   score: 2.0   memory length: 108525   epsilon: 0.9831185200003665    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 590   score: 0.0   memory length: 108648   epsilon: 0.9828749800003718    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 591   score: 1.0   memory length: 108819   epsilon: 0.9825364000003791    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 592   score: 2.0   memory length: 109037   epsilon: 0.9821047600003885    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 593   score: 2.0   memory length: 109235   epsilon: 0.981712720000397    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 594   score: 2.0   memory length: 109433   epsilon: 0.9813206800004055    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 595   score: 0.0   memory length: 109556   epsilon: 0.9810771400004108    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 596   score: 0.0   memory length: 109679   epsilon: 0.9808336000004161    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 597   score: 1.0   memory length: 109849   epsilon: 0.9804970000004234    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 598   score: 3.0   memory length: 110076   epsilon: 0.9800475400004331    steps: 227    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 599   score: 1.0   memory length: 110245   epsilon: 0.9797129200004404    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 600   score: 1.0   memory length: 110415   epsilon: 0.9793763200004477    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 601   score: 0.0   memory length: 110538   epsilon: 0.979132780000453    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 602   score: 5.0   memory length: 110904   epsilon: 0.9784081000004687    steps: 366    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 603   score: 2.0   memory length: 111102   epsilon: 0.9780160600004772    steps: 198    lr: 0.0001     evaluation reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 604   score: 0.0   memory length: 111225   epsilon: 0.9777725200004825    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 605   score: 0.0   memory length: 111347   epsilon: 0.9775309600004878    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 606   score: 3.0   memory length: 111613   epsilon: 0.9770042800004992    steps: 266    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 607   score: 2.0   memory length: 111828   epsilon: 0.9765785800005085    steps: 215    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 608   score: 0.0   memory length: 111951   epsilon: 0.9763350400005137    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 609   score: 3.0   memory length: 112218   epsilon: 0.9758063800005252    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 610   score: 2.0   memory length: 112436   epsilon: 0.9753747400005346    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 611   score: 0.0   memory length: 112558   epsilon: 0.9751331800005398    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 612   score: 2.0   memory length: 112755   epsilon: 0.9747431200005483    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 613   score: 2.0   memory length: 112953   epsilon: 0.9743510800005568    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 614   score: 1.0   memory length: 113104   epsilon: 0.9740521000005633    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 615   score: 0.0   memory length: 113227   epsilon: 0.9738085600005686    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 616   score: 0.0   memory length: 113350   epsilon: 0.9735650200005739    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 617   score: 2.0   memory length: 113548   epsilon: 0.9731729800005824    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 618   score: 3.0   memory length: 113796   epsilon: 0.972681940000593    steps: 248    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 619   score: 2.0   memory length: 113994   epsilon: 0.9722899000006016    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 620   score: 2.0   memory length: 114192   epsilon: 0.9718978600006101    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 621   score: 0.0   memory length: 114314   epsilon: 0.9716563000006153    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 622   score: 2.0   memory length: 114534   epsilon: 0.9712207000006248    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 623   score: 1.0   memory length: 114686   epsilon: 0.9709197400006313    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 624   score: 3.0   memory length: 114956   epsilon: 0.9703851400006429    steps: 270    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 625   score: 1.0   memory length: 115124   epsilon: 0.9700525000006501    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 626   score: 0.0   memory length: 115247   epsilon: 0.9698089600006554    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 627   score: 2.0   memory length: 115448   epsilon: 0.9694109800006641    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 628   score: 0.0   memory length: 115570   epsilon: 0.9691694200006693    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 629   score: 5.0   memory length: 115893   epsilon: 0.9685298800006832    steps: 323    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 630   score: 0.0   memory length: 116016   epsilon: 0.9682863400006885    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 631   score: 2.0   memory length: 116214   epsilon: 0.967894300000697    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 632   score: 1.0   memory length: 116385   epsilon: 0.9675557200007043    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 633   score: 0.0   memory length: 116507   epsilon: 0.9673141600007096    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 634   score: 1.0   memory length: 116657   epsilon: 0.967017160000716    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 635   score: 0.0   memory length: 116780   epsilon: 0.9667736200007213    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 636   score: 3.0   memory length: 117008   epsilon: 0.9663221800007311    steps: 228    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 637   score: 0.0   memory length: 117130   epsilon: 0.9660806200007364    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 638   score: 1.0   memory length: 117300   epsilon: 0.9657440200007437    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 639   score: 1.0   memory length: 117469   epsilon: 0.9654094000007509    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 640   score: 3.0   memory length: 117735   epsilon: 0.9648827200007624    steps: 266    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 641   score: 0.0   memory length: 117858   epsilon: 0.9646391800007676    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 642   score: 8.0   memory length: 118312   epsilon: 0.9637402600007872    steps: 454    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 643   score: 3.0   memory length: 118540   epsilon: 0.963288820000797    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 644   score: 2.0   memory length: 118738   epsilon: 0.9628967800008055    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 645   score: 2.0   memory length: 118939   epsilon: 0.9624988000008141    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 646   score: 1.0   memory length: 119090   epsilon: 0.9621998200008206    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 647   score: 1.0   memory length: 119240   epsilon: 0.961902820000827    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 648   score: 0.0   memory length: 119363   epsilon: 0.9616592800008323    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 649   score: 0.0   memory length: 119486   epsilon: 0.9614157400008376    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 650   score: 0.0   memory length: 119609   epsilon: 0.9611722000008429    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 651   score: 2.0   memory length: 119807   epsilon: 0.9607801600008514    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 652   score: 0.0   memory length: 119930   epsilon: 0.9605366200008567    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 653   score: 2.0   memory length: 120112   epsilon: 0.9601762600008645    steps: 182    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 654   score: 0.0   memory length: 120235   epsilon: 0.9599327200008698    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 655   score: 0.0   memory length: 120358   epsilon: 0.9596891800008751    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 656   score: 1.0   memory length: 120527   epsilon: 0.9593545600008824    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 657   score: 2.0   memory length: 120742   epsilon: 0.9589288600008916    steps: 215    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 658   score: 3.0   memory length: 120967   epsilon: 0.9584833600009013    steps: 225    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 659   score: 2.0   memory length: 121164   epsilon: 0.9580933000009098    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 660   score: 0.0   memory length: 121286   epsilon: 0.957851740000915    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 661   score: 0.0   memory length: 121409   epsilon: 0.9576082000009203    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 662   score: 2.0   memory length: 121625   epsilon: 0.9571805200009296    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 663   score: 2.0   memory length: 121823   epsilon: 0.9567884800009381    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 664   score: 1.0   memory length: 121994   epsilon: 0.9564499000009454    steps: 171    lr: 0.0001     evaluation reward: 1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 665   score: 1.0   memory length: 122162   epsilon: 0.9561172600009527    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 666   score: 2.0   memory length: 122360   epsilon: 0.9557252200009612    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 667   score: 0.0   memory length: 122483   epsilon: 0.9554816800009664    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 668   score: 4.0   memory length: 122744   epsilon: 0.9549649000009777    steps: 261    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 669   score: 2.0   memory length: 122942   epsilon: 0.9545728600009862    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 670   score: 0.0   memory length: 123065   epsilon: 0.9543293200009915    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 671   score: 1.0   memory length: 123216   epsilon: 0.954030340000998    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 672   score: 2.0   memory length: 123434   epsilon: 0.9535987000010073    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 673   score: 1.0   memory length: 123605   epsilon: 0.9532601200010147    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 674   score: 1.0   memory length: 123775   epsilon: 0.952923520001022    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 675   score: 2.0   memory length: 123994   epsilon: 0.9524899000010314    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 676   score: 3.0   memory length: 124243   epsilon: 0.9519968800010421    steps: 249    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 677   score: 2.0   memory length: 124459   epsilon: 0.9515692000010514    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 678   score: 0.0   memory length: 124581   epsilon: 0.9513276400010566    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 679   score: 2.0   memory length: 124799   epsilon: 0.950896000001066    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 680   score: 0.0   memory length: 124921   epsilon: 0.9506544400010712    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 681   score: 0.0   memory length: 125044   epsilon: 0.9504109000010765    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 682   score: 0.0   memory length: 125167   epsilon: 0.9501673600010818    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 683   score: 1.0   memory length: 125318   epsilon: 0.9498683800010883    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 684   score: 1.0   memory length: 125486   epsilon: 0.9495357400010955    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 685   score: 2.0   memory length: 125684   epsilon: 0.949143700001104    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 686   score: 0.0   memory length: 125807   epsilon: 0.9489001600011093    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 687   score: 2.0   memory length: 126023   epsilon: 0.9484724800011186    steps: 216    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 688   score: 1.0   memory length: 126174   epsilon: 0.9481735000011251    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 689   score: 1.0   memory length: 126324   epsilon: 0.9478765000011316    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 690   score: 1.0   memory length: 126474   epsilon: 0.947579500001138    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 691   score: 0.0   memory length: 126597   epsilon: 0.9473359600011433    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 692   score: 1.0   memory length: 126766   epsilon: 0.9470013400011505    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 693   score: 2.0   memory length: 126987   epsilon: 0.94656376000116    steps: 221    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 694   score: 1.0   memory length: 127157   epsilon: 0.9462271600011674    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 695   score: 2.0   memory length: 127375   epsilon: 0.9457955200011767    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 696   score: 4.0   memory length: 127650   epsilon: 0.9452510200011885    steps: 275    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 697   score: 3.0   memory length: 127897   epsilon: 0.9447619600011992    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 698   score: 2.0   memory length: 128079   epsilon: 0.944401600001207    steps: 182    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 699   score: 0.0   memory length: 128201   epsilon: 0.9441600400012122    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 700   score: 0.0   memory length: 128323   epsilon: 0.9439184800012175    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 701   score: 0.0   memory length: 128446   epsilon: 0.9436749400012228    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 702   score: 0.0   memory length: 128569   epsilon: 0.943431400001228    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 703   score: 1.0   memory length: 128739   epsilon: 0.9430948000012354    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 704   score: 0.0   memory length: 128861   epsilon: 0.9428532400012406    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 705   score: 2.0   memory length: 129058   epsilon: 0.9424631800012491    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 706   score: 3.0   memory length: 129324   epsilon: 0.9419365000012605    steps: 266    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 707   score: 0.0   memory length: 129446   epsilon: 0.9416949400012657    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 708   score: 1.0   memory length: 129597   epsilon: 0.9413959600012722    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 709   score: 0.0   memory length: 129719   epsilon: 0.9411544000012775    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 710   score: 3.0   memory length: 129965   epsilon: 0.940667320001288    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 711   score: 2.0   memory length: 130183   epsilon: 0.9402356800012974    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 712   score: 1.0   memory length: 130352   epsilon: 0.9399010600013047    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 713   score: 5.0   memory length: 130720   epsilon: 0.9391724200013205    steps: 368    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 714   score: 1.0   memory length: 130891   epsilon: 0.9388338400013279    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 715   score: 2.0   memory length: 131107   epsilon: 0.9384061600013371    steps: 216    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 716   score: 2.0   memory length: 131287   epsilon: 0.9380497600013449    steps: 180    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 717   score: 2.0   memory length: 131484   epsilon: 0.9376597000013533    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 718   score: 1.0   memory length: 131654   epsilon: 0.9373231000013607    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 719   score: 1.0   memory length: 131825   epsilon: 0.936984520001368    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 720   score: 0.0   memory length: 131947   epsilon: 0.9367429600013732    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 721   score: 2.0   memory length: 132148   epsilon: 0.9363449800013819    steps: 201    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 722   score: 2.0   memory length: 132329   epsilon: 0.9359866000013897    steps: 181    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 723   score: 0.0   memory length: 132452   epsilon: 0.935743060001395    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 724   score: 0.0   memory length: 132574   epsilon: 0.9355015000014002    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 725   score: 2.0   memory length: 132792   epsilon: 0.9350698600014096    steps: 218    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 726   score: 0.0   memory length: 132915   epsilon: 0.9348263200014149    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 727   score: 2.0   memory length: 133132   epsilon: 0.9343966600014242    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 728   score: 3.0   memory length: 133361   epsilon: 0.933943240001434    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 729   score: 3.0   memory length: 133607   epsilon: 0.9334561600014446    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 730   score: 3.0   memory length: 133837   epsilon: 0.9330007600014545    steps: 230    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 731   score: 3.0   memory length: 134063   epsilon: 0.9325532800014642    steps: 226    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 732   score: 2.0   memory length: 134260   epsilon: 0.9321632200014727    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 733   score: 3.0   memory length: 134522   epsilon: 0.9316444600014839    steps: 262    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 734   score: 0.0   memory length: 134645   epsilon: 0.9314009200014892    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 735   score: 0.0   memory length: 134767   epsilon: 0.9311593600014945    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 736   score: 3.0   memory length: 134994   epsilon: 0.9307099000015042    steps: 227    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 737   score: 1.0   memory length: 135163   epsilon: 0.9303752800015115    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 738   score: 0.0   memory length: 135286   epsilon: 0.9301317400015168    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 739   score: 4.0   memory length: 135562   epsilon: 0.9295852600015286    steps: 276    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 740   score: 0.0   memory length: 135685   epsilon: 0.9293417200015339    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 741   score: 2.0   memory length: 135903   epsilon: 0.9289100800015433    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 742   score: 2.0   memory length: 136100   epsilon: 0.9285200200015518    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 743   score: 0.0   memory length: 136222   epsilon: 0.928278460001557    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 744   score: 2.0   memory length: 136441   epsilon: 0.9278448400015664    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 745   score: 2.0   memory length: 136621   epsilon: 0.9274884400015742    steps: 180    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 746   score: 2.0   memory length: 136819   epsilon: 0.9270964000015827    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 747   score: 1.0   memory length: 136991   epsilon: 0.9267558400015901    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 748   score: 1.0   memory length: 137142   epsilon: 0.9264568600015965    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 749   score: 2.0   memory length: 137339   epsilon: 0.926066800001605    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 750   score: 2.0   memory length: 137537   epsilon: 0.9256747600016135    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 751   score: 2.0   memory length: 137734   epsilon: 0.925284700001622    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 752   score: 0.0   memory length: 137857   epsilon: 0.9250411600016273    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 753   score: 0.0   memory length: 137980   epsilon: 0.9247976200016326    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 754   score: 3.0   memory length: 138246   epsilon: 0.924270940001644    steps: 266    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 755   score: 2.0   memory length: 138444   epsilon: 0.9238789000016525    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 756   score: 3.0   memory length: 138669   epsilon: 0.9234334000016622    steps: 225    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 757   score: 0.0   memory length: 138791   epsilon: 0.9231918400016674    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 758   score: 2.0   memory length: 138989   epsilon: 0.9227998000016759    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 759   score: 2.0   memory length: 139189   epsilon: 0.9224038000016845    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 760   score: 0.0   memory length: 139312   epsilon: 0.9221602600016898    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 761   score: 2.0   memory length: 139510   epsilon: 0.9217682200016983    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 762   score: 0.0   memory length: 139633   epsilon: 0.9215246800017036    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 763   score: 2.0   memory length: 139851   epsilon: 0.921093040001713    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 764   score: 1.0   memory length: 140023   epsilon: 0.9207524800017204    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 765   score: 1.0   memory length: 140194   epsilon: 0.9204139000017277    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 766   score: 1.0   memory length: 140345   epsilon: 0.9201149200017342    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 767   score: 1.0   memory length: 140514   epsilon: 0.9197803000017415    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 768   score: 1.0   memory length: 140666   epsilon: 0.919479340001748    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 769   score: 1.0   memory length: 140817   epsilon: 0.9191803600017545    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 770   score: 0.0   memory length: 140940   epsilon: 0.9189368200017598    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 771   score: 0.0   memory length: 141063   epsilon: 0.9186932800017651    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 772   score: 0.0   memory length: 141185   epsilon: 0.9184517200017703    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 773   score: 5.0   memory length: 141525   epsilon: 0.917778520001785    steps: 340    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 774   score: 2.0   memory length: 141722   epsilon: 0.9173884600017934    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 775   score: 2.0   memory length: 141920   epsilon: 0.9169964200018019    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 776   score: 1.0   memory length: 142070   epsilon: 0.9166994200018084    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 777   score: 2.0   memory length: 142289   epsilon: 0.9162658000018178    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 778   score: 3.0   memory length: 142553   epsilon: 0.9157430800018291    steps: 264    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 779   score: 0.0   memory length: 142676   epsilon: 0.9154995400018344    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 780   score: 1.0   memory length: 142845   epsilon: 0.9151649200018417    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 781   score: 0.0   memory length: 142968   epsilon: 0.914921380001847    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 782   score: 2.0   memory length: 143166   epsilon: 0.9145293400018555    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 783   score: 2.0   memory length: 143364   epsilon: 0.914137300001864    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 784   score: 3.0   memory length: 143632   epsilon: 0.9136066600018755    steps: 268    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 785   score: 0.0   memory length: 143755   epsilon: 0.9133631200018808    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 786   score: 1.0   memory length: 143924   epsilon: 0.9130285000018881    steps: 169    lr: 0.0001     evaluation reward: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 787   score: 0.0   memory length: 144047   epsilon: 0.9127849600018934    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 788   score: 1.0   memory length: 144217   epsilon: 0.9124483600019007    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 789   score: 2.0   memory length: 144415   epsilon: 0.9120563200019092    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 790   score: 0.0   memory length: 144537   epsilon: 0.9118147600019144    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 791   score: 0.0   memory length: 144660   epsilon: 0.9115712200019197    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 792   score: 1.0   memory length: 144829   epsilon: 0.911236600001927    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 793   score: 2.0   memory length: 145026   epsilon: 0.9108465400019354    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 794   score: 2.0   memory length: 145243   epsilon: 0.9104168800019448    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 795   score: 0.0   memory length: 145365   epsilon: 0.91017532000195    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 796   score: 2.0   memory length: 145544   epsilon: 0.9098209000019577    steps: 179    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 797   score: 5.0   memory length: 145862   epsilon: 0.9091912600019714    steps: 318    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 798   score: 0.0   memory length: 145985   epsilon: 0.9089477200019767    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 799   score: 2.0   memory length: 146167   epsilon: 0.9085873600019845    steps: 182    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 800   score: 3.0   memory length: 146415   epsilon: 0.9080963200019951    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 801   score: 4.0   memory length: 146691   epsilon: 0.907549840002007    steps: 276    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 802   score: 1.0   memory length: 146842   epsilon: 0.9072508600020135    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 803   score: 0.0   memory length: 146965   epsilon: 0.9070073200020188    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 804   score: 3.0   memory length: 147212   epsilon: 0.9065182600020294    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 805   score: 2.0   memory length: 147409   epsilon: 0.9061282000020379    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 806   score: 3.0   memory length: 147635   epsilon: 0.9056807200020476    steps: 226    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 807   score: 1.0   memory length: 147803   epsilon: 0.9053480800020548    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 808   score: 2.0   memory length: 148022   epsilon: 0.9049144600020642    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 809   score: 2.0   memory length: 148219   epsilon: 0.9045244000020727    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 810   score: 0.0   memory length: 148342   epsilon: 0.904280860002078    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 811   score: 2.0   memory length: 148524   epsilon: 0.9039205000020858    steps: 182    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 812   score: 4.0   memory length: 148817   epsilon: 0.9033403600020984    steps: 293    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 813   score: 3.0   memory length: 149089   epsilon: 0.9028018000021101    steps: 272    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 814   score: 3.0   memory length: 149314   epsilon: 0.9023563000021197    steps: 225    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 815   score: 4.0   memory length: 149591   epsilon: 0.9018078400021317    steps: 277    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 816   score: 0.0   memory length: 149714   epsilon: 0.9015643000021369    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 817   score: 3.0   memory length: 149982   epsilon: 0.9010336600021485    steps: 268    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 818   score: 3.0   memory length: 150246   epsilon: 0.9005109400021598    steps: 264    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 819   score: 3.0   memory length: 150495   epsilon: 0.9000179200021705    steps: 249    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 820   score: 5.0   memory length: 150864   epsilon: 0.8992873000021864    steps: 369    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 821   score: 0.0   memory length: 150987   epsilon: 0.8990437600021917    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 822   score: 1.0   memory length: 151156   epsilon: 0.8987091400021989    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 823   score: 1.0   memory length: 151324   epsilon: 0.8983765000022061    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 824   score: 0.0   memory length: 151447   epsilon: 0.8981329600022114    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 825   score: 3.0   memory length: 151694   epsilon: 0.897643900002222    steps: 247    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 826   score: 0.0   memory length: 151817   epsilon: 0.8974003600022273    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 827   score: 1.0   memory length: 151986   epsilon: 0.8970657400022346    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 828   score: 1.0   memory length: 152155   epsilon: 0.8967311200022419    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 829   score: 6.0   memory length: 152552   epsilon: 0.8959450600022589    steps: 397    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 830   score: 2.0   memory length: 152750   epsilon: 0.8955530200022674    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 831   score: 4.0   memory length: 153065   epsilon: 0.894929320002281    steps: 315    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 832   score: 2.0   memory length: 153262   epsilon: 0.8945392600022894    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 833   score: 4.0   memory length: 153536   epsilon: 0.8939967400023012    steps: 274    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 834   score: 2.0   memory length: 153716   epsilon: 0.893640340002309    steps: 180    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 835   score: 1.0   memory length: 153886   epsilon: 0.8933037400023163    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 836   score: 3.0   memory length: 154154   epsilon: 0.8927731000023278    steps: 268    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 837   score: 0.0   memory length: 154277   epsilon: 0.8925295600023331    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 838   score: 3.0   memory length: 154520   epsilon: 0.8920484200023435    steps: 243    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 839   score: 4.0   memory length: 154817   epsilon: 0.8914603600023563    steps: 297    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 840   score: 0.0   memory length: 154940   epsilon: 0.8912168200023616    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 841   score: 0.0   memory length: 155063   epsilon: 0.8909732800023669    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 842   score: 1.0   memory length: 155232   epsilon: 0.8906386600023741    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 843   score: 2.0   memory length: 155430   epsilon: 0.8902466200023826    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 844   score: 2.0   memory length: 155629   epsilon: 0.8898526000023912    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 845   score: 3.0   memory length: 155876   epsilon: 0.8893635400024018    steps: 247    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 846   score: 4.0   memory length: 156150   epsilon: 0.8888210200024136    steps: 274    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 847   score: 0.0   memory length: 156272   epsilon: 0.8885794600024188    steps: 122    lr: 0.0001     evaluation reward: 1.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 848   score: 2.0   memory length: 156470   epsilon: 0.8881874200024273    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 849   score: 2.0   memory length: 156667   epsilon: 0.8877973600024358    steps: 197    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 850   score: 1.0   memory length: 156838   epsilon: 0.8874587800024432    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 851   score: 1.0   memory length: 157010   epsilon: 0.8871182200024506    steps: 172    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 852   score: 2.0   memory length: 157208   epsilon: 0.8867261800024591    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 853   score: 2.0   memory length: 157406   epsilon: 0.8863341400024676    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 854   score: 1.0   memory length: 157557   epsilon: 0.8860351600024741    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 855   score: 0.0   memory length: 157680   epsilon: 0.8857916200024794    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 856   score: 4.0   memory length: 157976   epsilon: 0.8852055400024921    steps: 296    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 857   score: 3.0   memory length: 158204   epsilon: 0.8847541000025019    steps: 228    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 858   score: 1.0   memory length: 158355   epsilon: 0.8844551200025084    steps: 151    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 859   score: 1.0   memory length: 158507   epsilon: 0.8841541600025149    steps: 152    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 860   score: 2.0   memory length: 158705   epsilon: 0.8837621200025234    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 861   score: 1.0   memory length: 158875   epsilon: 0.8834255200025307    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 862   score: 2.0   memory length: 159093   epsilon: 0.8829938800025401    steps: 218    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 863   score: 0.0   memory length: 159216   epsilon: 0.8827503400025454    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 864   score: 3.0   memory length: 159463   epsilon: 0.882261280002556    steps: 247    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 865   score: 7.0   memory length: 159834   epsilon: 0.8815267000025719    steps: 371    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 866   score: 1.0   memory length: 160003   epsilon: 0.8811920800025792    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 867   score: 0.0   memory length: 160125   epsilon: 0.8809505200025844    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 868   score: 0.0   memory length: 160248   epsilon: 0.8807069800025897    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 869   score: 0.0   memory length: 160370   epsilon: 0.880465420002595    steps: 122    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 870   score: 2.0   memory length: 160568   epsilon: 0.8800733800026035    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 871   score: 4.0   memory length: 160861   epsilon: 0.8794932400026161    steps: 293    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 872   score: 2.0   memory length: 161079   epsilon: 0.8790616000026255    steps: 218    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 873   score: 2.0   memory length: 161277   epsilon: 0.878669560002634    steps: 198    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 874   score: 1.0   memory length: 161445   epsilon: 0.8783369200026412    steps: 168    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 875   score: 2.0   memory length: 161643   epsilon: 0.8779448800026497    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 876   score: 1.0   memory length: 161794   epsilon: 0.8776459000026562    steps: 151    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 877   score: 1.0   memory length: 161944   epsilon: 0.8773489000026626    steps: 150    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 878   score: 1.0   memory length: 162113   epsilon: 0.8770142800026699    steps: 169    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 879   score: 0.0   memory length: 162235   epsilon: 0.8767727200026751    steps: 122    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 880   score: 3.0   memory length: 162499   epsilon: 0.8762500000026865    steps: 264    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 881   score: 4.0   memory length: 162776   epsilon: 0.8757015400026984    steps: 277    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 882   score: 1.0   memory length: 162927   epsilon: 0.8754025600027049    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 883   score: 4.0   memory length: 163238   epsilon: 0.8747867800027183    steps: 311    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 884   score: 1.0   memory length: 163390   epsilon: 0.8744858200027248    steps: 152    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 885   score: 0.0   memory length: 163513   epsilon: 0.8742422800027301    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 886   score: 3.0   memory length: 163758   epsilon: 0.8737571800027406    steps: 245    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 887   score: 4.0   memory length: 164034   epsilon: 0.8732107000027525    steps: 276    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 888   score: 1.0   memory length: 164204   epsilon: 0.8728741000027598    steps: 170    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 889   score: 0.0   memory length: 164327   epsilon: 0.8726305600027651    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 890   score: 2.0   memory length: 164527   epsilon: 0.8722345600027737    steps: 200    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 891   score: 2.0   memory length: 164725   epsilon: 0.8718425200027822    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 892   score: 2.0   memory length: 164943   epsilon: 0.8714108800027915    steps: 218    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 893   score: 4.0   memory length: 165215   epsilon: 0.8708723200028032    steps: 272    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 894   score: 1.0   memory length: 165365   epsilon: 0.8705753200028097    steps: 150    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 895   score: 2.0   memory length: 165563   epsilon: 0.8701832800028182    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 896   score: 6.0   memory length: 165898   epsilon: 0.8695199800028326    steps: 335    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 897   score: 2.0   memory length: 166116   epsilon: 0.869088340002842    steps: 218    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 898   score: 4.0   memory length: 166393   epsilon: 0.8685398800028539    steps: 277    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 899   score: 3.0   memory length: 166640   epsilon: 0.8680508200028645    steps: 247    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 900   score: 5.0   memory length: 166984   epsilon: 0.8673697000028793    steps: 344    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 901   score: 2.0   memory length: 167182   epsilon: 0.8669776600028878    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 902   score: 2.0   memory length: 167381   epsilon: 0.8665836400028963    steps: 199    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 903   score: 3.0   memory length: 167592   epsilon: 0.8661658600029054    steps: 211    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 904   score: 3.0   memory length: 167818   epsilon: 0.8657183800029151    steps: 226    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 905   score: 3.0   memory length: 168046   epsilon: 0.8652669400029249    steps: 228    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 906   score: 3.0   memory length: 168289   epsilon: 0.8647858000029354    steps: 243    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 907   score: 6.0   memory length: 168664   epsilon: 0.8640433000029515    steps: 375    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 908   score: 0.0   memory length: 168787   epsilon: 0.8637997600029568    steps: 123    lr: 0.0001     evaluation reward: 2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 909   score: 1.0   memory length: 168956   epsilon: 0.863465140002964    steps: 169    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 910   score: 3.0   memory length: 169208   epsilon: 0.8629661800029749    steps: 252    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 911   score: 4.0   memory length: 169465   epsilon: 0.8624573200029859    steps: 257    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 912   score: 3.0   memory length: 169731   epsilon: 0.8619306400029974    steps: 266    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 913   score: 5.0   memory length: 170056   epsilon: 0.8612871400030113    steps: 325    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 914   score: 3.0   memory length: 170284   epsilon: 0.8608357000030211    steps: 228    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 915   score: 2.0   memory length: 170502   epsilon: 0.8604040600030305    steps: 218    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 916   score: 1.0   memory length: 170653   epsilon: 0.860105080003037    steps: 151    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 917   score: 4.0   memory length: 170926   epsilon: 0.8595645400030487    steps: 273    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 918   score: 3.0   memory length: 171153   epsilon: 0.8591150800030585    steps: 227    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 919   score: 2.0   memory length: 171351   epsilon: 0.858723040003067    steps: 198    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 920   score: 2.0   memory length: 171548   epsilon: 0.8583329800030755    steps: 197    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 921   score: 3.0   memory length: 171774   epsilon: 0.8578855000030852    steps: 226    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 922   score: 2.0   memory length: 171974   epsilon: 0.8574895000030938    steps: 200    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 923   score: 1.0   memory length: 172124   epsilon: 0.8571925000031002    steps: 150    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 924   score: 2.0   memory length: 172322   epsilon: 0.8568004600031087    steps: 198    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 925   score: 2.0   memory length: 172520   epsilon: 0.8564084200031172    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 926   score: 0.0   memory length: 172642   epsilon: 0.8561668600031225    steps: 122    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 927   score: 6.0   memory length: 172991   epsilon: 0.8554758400031375    steps: 349    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 928   score: 2.0   memory length: 173190   epsilon: 0.855081820003146    steps: 199    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 929   score: 5.0   memory length: 173515   epsilon: 0.85443832000316    steps: 325    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 930   score: 0.0   memory length: 173638   epsilon: 0.8541947800031653    steps: 123    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 931   score: 4.0   memory length: 173917   epsilon: 0.8536423600031773    steps: 279    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 932   score: 1.0   memory length: 174069   epsilon: 0.8533414000031838    steps: 152    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 933   score: 2.0   memory length: 174269   epsilon: 0.8529454000031924    steps: 200    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 934   score: 3.0   memory length: 174497   epsilon: 0.8524939600032022    steps: 228    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 935   score: 3.0   memory length: 174741   epsilon: 0.8520108400032127    steps: 244    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 936   score: 3.0   memory length: 175011   epsilon: 0.8514762400032243    steps: 270    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 937   score: 3.0   memory length: 175239   epsilon: 0.8510248000032341    steps: 228    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 938   score: 1.0   memory length: 175408   epsilon: 0.8506901800032414    steps: 169    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 939   score: 5.0   memory length: 175753   epsilon: 0.8500070800032562    steps: 345    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 940   score: 2.0   memory length: 175934   epsilon: 0.849648700003264    steps: 181    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 941   score: 3.0   memory length: 176199   epsilon: 0.8491240000032754    steps: 265    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 942   score: 2.0   memory length: 176396   epsilon: 0.8487339400032838    steps: 197    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 943   score: 5.0   memory length: 176722   epsilon: 0.8480884600032979    steps: 326    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 944   score: 2.0   memory length: 176920   epsilon: 0.8476964200033064    steps: 198    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 945   score: 4.0   memory length: 177196   epsilon: 0.8471499400033182    steps: 276    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 946   score: 4.0   memory length: 177477   epsilon: 0.8465935600033303    steps: 281    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 947   score: 2.0   memory length: 177695   epsilon: 0.8461619200033397    steps: 218    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 948   score: 2.0   memory length: 177911   epsilon: 0.845734240003349    steps: 216    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 949   score: 3.0   memory length: 178158   epsilon: 0.8452451800033596    steps: 247    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 950   score: 5.0   memory length: 178519   epsilon: 0.8445304000033751    steps: 361    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 951   score: 4.0   memory length: 178817   epsilon: 0.8439403600033879    steps: 298    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 952   score: 1.0   memory length: 178986   epsilon: 0.8436057400033952    steps: 169    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 953   score: 3.0   memory length: 179233   epsilon: 0.8431166800034058    steps: 247    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 954   score: 3.0   memory length: 179460   epsilon: 0.8426672200034155    steps: 227    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 955   score: 1.0   memory length: 179611   epsilon: 0.842368240003422    steps: 151    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 956   score: 0.0   memory length: 179734   epsilon: 0.8421247000034273    steps: 123    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 957   score: 1.0   memory length: 179885   epsilon: 0.8418257200034338    steps: 151    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 958   score: 1.0   memory length: 180055   epsilon: 0.8414891200034411    steps: 170    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 959   score: 1.0   memory length: 180207   epsilon: 0.8411881600034476    steps: 152    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 960   score: 3.0   memory length: 180452   epsilon: 0.8407030600034582    steps: 245    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 961   score: 5.0   memory length: 180818   epsilon: 0.8399783800034739    steps: 366    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 962   score: 1.0   memory length: 180989   epsilon: 0.8396398000034813    steps: 171    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 963   score: 1.0   memory length: 181159   epsilon: 0.8393032000034886    steps: 170    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 964   score: 0.0   memory length: 181281   epsilon: 0.8390616400034938    steps: 122    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 965   score: 1.0   memory length: 181450   epsilon: 0.8387270200035011    steps: 169    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 966   score: 1.0   memory length: 181601   epsilon: 0.8384280400035076    steps: 151    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 967   score: 1.0   memory length: 181770   epsilon: 0.8380934200035148    steps: 169    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 968   score: 2.0   memory length: 181989   epsilon: 0.8376598000035242    steps: 219    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 969   score: 3.0   memory length: 182215   epsilon: 0.837212320003534    steps: 226    lr: 0.0001     evaluation reward: 2.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 970   score: 2.0   memory length: 182413   epsilon: 0.8368202800035425    steps: 198    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 971   score: 2.0   memory length: 182630   epsilon: 0.8363906200035518    steps: 217    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 972   score: 1.0   memory length: 182801   epsilon: 0.8360520400035591    steps: 171    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 973   score: 3.0   memory length: 183069   epsilon: 0.8355214000035707    steps: 268    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 974   score: 1.0   memory length: 183220   epsilon: 0.8352224200035772    steps: 151    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 975   score: 0.0   memory length: 183343   epsilon: 0.8349788800035824    steps: 123    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 976   score: 2.0   memory length: 183541   epsilon: 0.834586840003591    steps: 198    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 977   score: 6.0   memory length: 183895   epsilon: 0.8338859200036062    steps: 354    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 978   score: 1.0   memory length: 184046   epsilon: 0.8335869400036127    steps: 151    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 979   score: 2.0   memory length: 184246   epsilon: 0.8331909400036213    steps: 200    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 980   score: 5.0   memory length: 184574   epsilon: 0.8325415000036354    steps: 328    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 981   score: 4.0   memory length: 184846   epsilon: 0.832002940003647    steps: 272    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 982   score: 5.0   memory length: 185170   epsilon: 0.831361420003661    steps: 324    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 983   score: 3.0   memory length: 185397   epsilon: 0.8309119600036707    steps: 227    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 984   score: 4.0   memory length: 185676   epsilon: 0.8303595400036827    steps: 279    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 985   score: 3.0   memory length: 185923   epsilon: 0.8298704800036933    steps: 247    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 986   score: 5.0   memory length: 186228   epsilon: 0.8292665800037065    steps: 305    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 987   score: 1.0   memory length: 186399   epsilon: 0.8289280000037138    steps: 171    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 988   score: 4.0   memory length: 186660   epsilon: 0.828411220003725    steps: 261    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 989   score: 1.0   memory length: 186828   epsilon: 0.8280785800037322    steps: 168    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 990   score: 2.0   memory length: 187044   epsilon: 0.8276509000037415    steps: 216    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 991   score: 3.0   memory length: 187313   epsilon: 0.8271182800037531    steps: 269    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 992   score: 4.0   memory length: 187590   epsilon: 0.826569820003765    steps: 277    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 993   score: 3.0   memory length: 187858   epsilon: 0.8260391800037765    steps: 268    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 994   score: 1.0   memory length: 188009   epsilon: 0.825740200003783    steps: 151    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 995   score: 2.0   memory length: 188231   epsilon: 0.8253006400037926    steps: 222    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 996   score: 2.0   memory length: 188428   epsilon: 0.824910580003801    steps: 197    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 997   score: 1.0   memory length: 188597   epsilon: 0.8245759600038083    steps: 169    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 998   score: 3.0   memory length: 188842   epsilon: 0.8240908600038188    steps: 245    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 999   score: 2.0   memory length: 189060   epsilon: 0.8236592200038282    steps: 218    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1000   score: 0.0   memory length: 189183   epsilon: 0.8234156800038335    steps: 123    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1001   score: 2.0   memory length: 189380   epsilon: 0.8230256200038419    steps: 197    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1002   score: 2.0   memory length: 189580   epsilon: 0.8226296200038505    steps: 200    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1003   score: 3.0   memory length: 189827   epsilon: 0.8221405600038612    steps: 247    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1004   score: 0.0   memory length: 189949   epsilon: 0.8218990000038664    steps: 122    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1005   score: 3.0   memory length: 190200   epsilon: 0.8214020200038772    steps: 251    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1006   score: 2.0   memory length: 190398   epsilon: 0.8210099800038857    steps: 198    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1007   score: 2.0   memory length: 190598   epsilon: 0.8206139800038943    steps: 200    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1008   score: 1.0   memory length: 190749   epsilon: 0.8203150000039008    steps: 151    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1009   score: 1.0   memory length: 190900   epsilon: 0.8200160200039073    steps: 151    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1010   score: 0.0   memory length: 191023   epsilon: 0.8197724800039126    steps: 123    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1011   score: 3.0   memory length: 191270   epsilon: 0.8192834200039232    steps: 247    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1012   score: 0.0   memory length: 191393   epsilon: 0.8190398800039285    steps: 123    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1013   score: 2.0   memory length: 191591   epsilon: 0.818647840003937    steps: 198    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1014   score: 5.0   memory length: 191890   epsilon: 0.8180558200039498    steps: 299    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1015   score: 2.0   memory length: 192108   epsilon: 0.8176241800039592    steps: 218    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1016   score: 1.0   memory length: 192278   epsilon: 0.8172875800039665    steps: 170    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1017   score: 1.0   memory length: 192448   epsilon: 0.8169509800039738    steps: 170    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1018   score: 1.0   memory length: 192599   epsilon: 0.8166520000039803    steps: 151    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1019   score: 6.0   memory length: 192937   epsilon: 0.8159827600039948    steps: 338    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1020   score: 3.0   memory length: 193166   epsilon: 0.8155293400040047    steps: 229    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1021   score: 2.0   memory length: 193388   epsilon: 0.8150897800040142    steps: 222    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1022   score: 0.0   memory length: 193511   epsilon: 0.8148462400040195    steps: 123    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1023   score: 1.0   memory length: 193683   epsilon: 0.8145056800040269    steps: 172    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1024   score: 0.0   memory length: 193806   epsilon: 0.8142621400040322    steps: 123    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1025   score: 2.0   memory length: 194003   epsilon: 0.8138720800040407    steps: 197    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1026   score: 3.0   memory length: 194249   epsilon: 0.8133850000040512    steps: 246    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1027   score: 3.0   memory length: 194476   epsilon: 0.812935540004061    steps: 227    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1028   score: 3.0   memory length: 194703   epsilon: 0.8124860800040707    steps: 227    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1029   score: 3.0   memory length: 194950   epsilon: 0.8119970200040814    steps: 247    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 1030   score: 4.0   memory length: 195206   epsilon: 0.8114901400040924    steps: 256    lr: 0.0001     evaluation reward: 2.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1031   score: 4.0   memory length: 195483   epsilon: 0.8109416800041043    steps: 277    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1032   score: 5.0   memory length: 195828   epsilon: 0.8102585800041191    steps: 345    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1033   score: 2.0   memory length: 196026   epsilon: 0.8098665400041276    steps: 198    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1034   score: 4.0   memory length: 196322   epsilon: 0.8092804600041403    steps: 296    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1035   score: 3.0   memory length: 196571   epsilon: 0.808787440004151    steps: 249    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1036   score: 4.0   memory length: 196866   epsilon: 0.8082033400041637    steps: 295    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1037   score: 3.0   memory length: 197076   epsilon: 0.8077875400041727    steps: 210    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1038   score: 3.0   memory length: 197321   epsilon: 0.8073024400041833    steps: 245    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1039   score: 4.0   memory length: 197581   epsilon: 0.8067876400041945    steps: 260    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1040   score: 3.0   memory length: 197807   epsilon: 0.8063401600042042    steps: 226    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1041   score: 4.0   memory length: 198047   epsilon: 0.8058649600042145    steps: 240    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1042   score: 2.0   memory length: 198245   epsilon: 0.805472920004223    steps: 198    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1043   score: 5.0   memory length: 198553   epsilon: 0.8048630800042362    steps: 308    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1044   score: 6.0   memory length: 198875   epsilon: 0.8042255200042501    steps: 322    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1045   score: 3.0   memory length: 199121   epsilon: 0.8037384400042606    steps: 246    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1046   score: 3.0   memory length: 199346   epsilon: 0.8032929400042703    steps: 225    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 1047   score: 0.0   memory length: 199469   epsilon: 0.8030494000042756    steps: 123    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1048   score: 4.0   memory length: 199764   epsilon: 0.8024653000042883    steps: 295    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 1049   score: 2.0   memory length: 199962   epsilon: 0.8020732600042968    steps: 198    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1050   score: 3.0   memory length: 200208   epsilon: 0.8015861800043074    steps: 246    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1051   score: 4.0   memory length: 200506   epsilon: 0.8009961400043202    steps: 298    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1052   score: 3.0   memory length: 200738   epsilon: 0.8005367800043302    steps: 232    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1053   score: 0.0   memory length: 200861   epsilon: 0.8002932400043354    steps: 123    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1054   score: 5.0   memory length: 201185   epsilon: 0.7996517200043494    steps: 324    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1055   score: 1.0   memory length: 201336   epsilon: 0.7993527400043559    steps: 151    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1056   score: 2.0   memory length: 201534   epsilon: 0.7989607000043644    steps: 198    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 1057   score: 4.0   memory length: 201794   epsilon: 0.7984459000043755    steps: 260    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1058   score: 2.0   memory length: 201974   epsilon: 0.7980895000043833    steps: 180    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 1059   score: 2.0   memory length: 202172   epsilon: 0.7976974600043918    steps: 198    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1060   score: 1.0   memory length: 202323   epsilon: 0.7973984800043983    steps: 151    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1061   score: 3.0   memory length: 202573   epsilon: 0.796903480004409    steps: 250    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1062   score: 6.0   memory length: 202930   epsilon: 0.7961966200044244    steps: 357    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1063   score: 4.0   memory length: 203205   epsilon: 0.7956521200044362    steps: 275    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1064   score: 1.0   memory length: 203376   epsilon: 0.7953135400044435    steps: 171    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1065   score: 2.0   memory length: 203594   epsilon: 0.7948819000044529    steps: 218    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1066   score: 2.0   memory length: 203776   epsilon: 0.7945215400044607    steps: 182    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1067   score: 4.0   memory length: 204074   epsilon: 0.7939315000044735    steps: 298    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1068   score: 1.0   memory length: 204243   epsilon: 0.7935968800044808    steps: 169    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1069   score: 3.0   memory length: 204489   epsilon: 0.7931098000044914    steps: 246    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1070   score: 3.0   memory length: 204738   epsilon: 0.7926167800045021    steps: 249    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1071   score: 1.0   memory length: 204889   epsilon: 0.7923178000045086    steps: 151    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1072   score: 1.0   memory length: 205040   epsilon: 0.7920188200045151    steps: 151    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1073   score: 4.0   memory length: 205319   epsilon: 0.7914664000045271    steps: 279    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1074   score: 3.0   memory length: 205567   epsilon: 0.7909753600045377    steps: 248    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1075   score: 4.0   memory length: 205844   epsilon: 0.7904269000045496    steps: 277    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1076   score: 5.0   memory length: 206173   epsilon: 0.7897754800045638    steps: 329    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1077   score: 4.0   memory length: 206447   epsilon: 0.7892329600045755    steps: 274    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1078   score: 3.0   memory length: 206677   epsilon: 0.7887775600045854    steps: 230    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1079   score: 2.0   memory length: 206858   epsilon: 0.7884191800045932    steps: 181    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1080   score: 0.0   memory length: 206981   epsilon: 0.7881756400045985    steps: 123    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1081   score: 3.0   memory length: 207227   epsilon: 0.7876885600046091    steps: 246    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1082   score: 2.0   memory length: 207445   epsilon: 0.7872569200046184    steps: 218    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1083   score: 3.0   memory length: 207670   epsilon: 0.7868114200046281    steps: 225    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1084   score: 7.0   memory length: 208042   epsilon: 0.7860748600046441    steps: 372    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1085   score: 3.0   memory length: 208269   epsilon: 0.7856254000046539    steps: 227    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1086   score: 7.0   memory length: 208612   epsilon: 0.7849462600046686    steps: 343    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1087   score: 2.0   memory length: 208809   epsilon: 0.7845562000046771    steps: 197    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1088   score: 2.0   memory length: 209006   epsilon: 0.7841661400046855    steps: 197    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1089   score: 3.0   memory length: 209236   epsilon: 0.7837107400046954    steps: 230    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1090   score: 4.0   memory length: 209511   epsilon: 0.7831662400047072    steps: 275    lr: 0.0001     evaluation reward: 2.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1091   score: 3.0   memory length: 209744   epsilon: 0.7827049000047173    steps: 233    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 1092   score: 3.0   memory length: 209970   epsilon: 0.782257420004727    steps: 226    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1093   score: 2.0   memory length: 210169   epsilon: 0.7818634000047355    steps: 199    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1094   score: 1.0   memory length: 210338   epsilon: 0.7815287800047428    steps: 169    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1095   score: 3.0   memory length: 210573   epsilon: 0.7810634800047529    steps: 235    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1096   score: 7.0   memory length: 210960   epsilon: 0.7802972200047695    steps: 387    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1097   score: 2.0   memory length: 211176   epsilon: 0.7798695400047788    steps: 216    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1098   score: 1.0   memory length: 211327   epsilon: 0.7795705600047853    steps: 151    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1099   score: 3.0   memory length: 211560   epsilon: 0.7791092200047953    steps: 233    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1100   score: 3.0   memory length: 211809   epsilon: 0.778616200004806    steps: 249    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1101   score: 2.0   memory length: 212009   epsilon: 0.7782202000048146    steps: 200    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1102   score: 8.0   memory length: 212426   epsilon: 0.7773945400048325    steps: 417    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 1103   score: 1.0   memory length: 212594   epsilon: 0.7770619000048398    steps: 168    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 1104   score: 6.0   memory length: 212964   epsilon: 0.7763293000048557    steps: 370    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1105   score: 4.0   memory length: 213222   epsilon: 0.7758184600048668    steps: 258    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1106   score: 5.0   memory length: 213544   epsilon: 0.7751809000048806    steps: 322    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1107   score: 3.0   memory length: 213792   epsilon: 0.7746898600048913    steps: 248    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1108   score: 3.0   memory length: 214042   epsilon: 0.774194860004902    steps: 250    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 1109   score: 4.0   memory length: 214356   epsilon: 0.7735731400049155    steps: 314    lr: 0.0001     evaluation reward: 2.92\n",
      "episode: 1110   score: 4.0   memory length: 214615   epsilon: 0.7730603200049266    steps: 259    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1111   score: 2.0   memory length: 214797   epsilon: 0.7726999600049345    steps: 182    lr: 0.0001     evaluation reward: 2.95\n",
      "episode: 1112   score: 4.0   memory length: 215072   epsilon: 0.7721554600049463    steps: 275    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1113   score: 1.0   memory length: 215223   epsilon: 0.7718564800049528    steps: 151    lr: 0.0001     evaluation reward: 2.98\n",
      "episode: 1114   score: 3.0   memory length: 215471   epsilon: 0.7713654400049634    steps: 248    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1115   score: 5.0   memory length: 215796   epsilon: 0.7707219400049774    steps: 325    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1116   score: 1.0   memory length: 215947   epsilon: 0.7704229600049839    steps: 151    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1117   score: 7.0   memory length: 216364   epsilon: 0.7695973000050018    steps: 417    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1118   score: 3.0   memory length: 216610   epsilon: 0.7691102200050124    steps: 246    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1119   score: 13.0   memory length: 217126   epsilon: 0.7680885400050346    steps: 516    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1120   score: 1.0   memory length: 217276   epsilon: 0.767791540005041    steps: 150    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1121   score: 0.0   memory length: 217398   epsilon: 0.7675499800050463    steps: 122    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1122   score: 0.0   memory length: 217520   epsilon: 0.7673084200050515    steps: 122    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1123   score: 3.0   memory length: 217745   epsilon: 0.7668629200050612    steps: 225    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1124   score: 3.0   memory length: 217990   epsilon: 0.7663778200050717    steps: 245    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1125   score: 4.0   memory length: 218246   epsilon: 0.7658709400050827    steps: 256    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1126   score: 0.0   memory length: 218369   epsilon: 0.765627400005088    steps: 123    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1127   score: 3.0   memory length: 218619   epsilon: 0.7651324000050987    steps: 250    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1128   score: 2.0   memory length: 218800   epsilon: 0.7647740200051065    steps: 181    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1129   score: 6.0   memory length: 219168   epsilon: 0.7640453800051223    steps: 368    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1130   score: 6.0   memory length: 219502   epsilon: 0.7633840600051367    steps: 334    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1131   score: 4.0   memory length: 219776   epsilon: 0.7628415400051485    steps: 274    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1132   score: 5.0   memory length: 220081   epsilon: 0.7622376400051616    steps: 305    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1133   score: 4.0   memory length: 220377   epsilon: 0.7616515600051743    steps: 296    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1134   score: 4.0   memory length: 220654   epsilon: 0.7611031000051862    steps: 277    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1135   score: 3.0   memory length: 220883   epsilon: 0.7606496800051961    steps: 229    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1136   score: 5.0   memory length: 221206   epsilon: 0.7600101400052099    steps: 323    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1137   score: 4.0   memory length: 221501   epsilon: 0.7594260400052226    steps: 295    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1138   score: 2.0   memory length: 221718   epsilon: 0.758996380005232    steps: 217    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1139   score: 1.0   memory length: 221868   epsilon: 0.7586993800052384    steps: 150    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1140   score: 5.0   memory length: 222195   epsilon: 0.7580519200052525    steps: 327    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1141   score: 9.0   memory length: 222653   epsilon: 0.7571450800052721    steps: 458    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1142   score: 2.0   memory length: 222851   epsilon: 0.7567530400052807    steps: 198    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1143   score: 3.0   memory length: 223098   epsilon: 0.7562639800052913    steps: 247    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1144   score: 3.0   memory length: 223368   epsilon: 0.7557293800053029    steps: 270    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1145   score: 4.0   memory length: 223684   epsilon: 0.7551037000053165    steps: 316    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1146   score: 2.0   memory length: 223902   epsilon: 0.7546720600053258    steps: 218    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1147   score: 3.0   memory length: 224151   epsilon: 0.7541790400053365    steps: 249    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1148   score: 2.0   memory length: 224351   epsilon: 0.7537830400053451    steps: 200    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1149   score: 4.0   memory length: 224612   epsilon: 0.7532662600053563    steps: 261    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1150   score: 6.0   memory length: 225005   epsilon: 0.7524881200053732    steps: 393    lr: 0.0001     evaluation reward: 3.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1151   score: 2.0   memory length: 225203   epsilon: 0.7520960800053818    steps: 198    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1152   score: 2.0   memory length: 225400   epsilon: 0.7517060200053902    steps: 197    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1153   score: 0.0   memory length: 225523   epsilon: 0.7514624800053955    steps: 123    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1154   score: 5.0   memory length: 225846   epsilon: 0.7508229400054094    steps: 323    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1155   score: 6.0   memory length: 226199   epsilon: 0.7501240000054246    steps: 353    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1156   score: 3.0   memory length: 226413   epsilon: 0.7497002800054338    steps: 214    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1157   score: 1.0   memory length: 226582   epsilon: 0.749365660005441    steps: 169    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1158   score: 3.0   memory length: 226831   epsilon: 0.7488726400054517    steps: 249    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1159   score: 7.0   memory length: 227201   epsilon: 0.7481400400054676    steps: 370    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1160   score: 6.0   memory length: 227532   epsilon: 0.7474846600054819    steps: 331    lr: 0.0001     evaluation reward: 3.37\n",
      "episode: 1161   score: 8.0   memory length: 227929   epsilon: 0.7466986000054989    steps: 397    lr: 0.0001     evaluation reward: 3.42\n",
      "episode: 1162   score: 3.0   memory length: 228175   epsilon: 0.7462115200055095    steps: 246    lr: 0.0001     evaluation reward: 3.39\n",
      "episode: 1163   score: 1.0   memory length: 228344   epsilon: 0.7458769000055168    steps: 169    lr: 0.0001     evaluation reward: 3.36\n",
      "episode: 1164   score: 5.0   memory length: 228673   epsilon: 0.7452254800055309    steps: 329    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1165   score: 3.0   memory length: 228902   epsilon: 0.7447720600055407    steps: 229    lr: 0.0001     evaluation reward: 3.41\n",
      "episode: 1166   score: 2.0   memory length: 229100   epsilon: 0.7443800200055493    steps: 198    lr: 0.0001     evaluation reward: 3.41\n",
      "episode: 1167   score: 5.0   memory length: 229425   epsilon: 0.7437365200055632    steps: 325    lr: 0.0001     evaluation reward: 3.42\n",
      "episode: 1168   score: 2.0   memory length: 229647   epsilon: 0.7432969600055728    steps: 222    lr: 0.0001     evaluation reward: 3.43\n",
      "episode: 1169   score: 4.0   memory length: 229926   epsilon: 0.7427445400055848    steps: 279    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1170   score: 4.0   memory length: 230203   epsilon: 0.7421960800055967    steps: 277    lr: 0.0001     evaluation reward: 3.45\n",
      "episode: 1171   score: 3.0   memory length: 230448   epsilon: 0.7417109800056072    steps: 245    lr: 0.0001     evaluation reward: 3.47\n",
      "episode: 1172   score: 4.0   memory length: 230723   epsilon: 0.741166480005619    steps: 275    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1173   score: 2.0   memory length: 230923   epsilon: 0.7407704800056276    steps: 200    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1174   score: 5.0   memory length: 231212   epsilon: 0.74019826000564    steps: 289    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1175   score: 2.0   memory length: 231410   epsilon: 0.7398062200056486    steps: 198    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1176   score: 3.0   memory length: 231620   epsilon: 0.7393904200056576    steps: 210    lr: 0.0001     evaluation reward: 3.46\n",
      "episode: 1177   score: 1.0   memory length: 231771   epsilon: 0.7390914400056641    steps: 151    lr: 0.0001     evaluation reward: 3.43\n",
      "episode: 1178   score: 4.0   memory length: 232071   epsilon: 0.738497440005677    steps: 300    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1179   score: 3.0   memory length: 232316   epsilon: 0.7380123400056875    steps: 245    lr: 0.0001     evaluation reward: 3.45\n",
      "episode: 1180   score: 7.0   memory length: 232712   epsilon: 0.7372282600057045    steps: 396    lr: 0.0001     evaluation reward: 3.52\n",
      "episode: 1181   score: 5.0   memory length: 233000   epsilon: 0.7366580200057169    steps: 288    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1182   score: 4.0   memory length: 233294   epsilon: 0.7360759000057295    steps: 294    lr: 0.0001     evaluation reward: 3.56\n",
      "episode: 1183   score: 2.0   memory length: 233512   epsilon: 0.7356442600057389    steps: 218    lr: 0.0001     evaluation reward: 3.55\n",
      "episode: 1184   score: 5.0   memory length: 233819   epsilon: 0.7350364000057521    steps: 307    lr: 0.0001     evaluation reward: 3.53\n",
      "episode: 1185   score: 1.0   memory length: 233970   epsilon: 0.7347374200057586    steps: 151    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1186   score: 4.0   memory length: 234263   epsilon: 0.7341572800057712    steps: 293    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1187   score: 1.0   memory length: 234413   epsilon: 0.7338602800057776    steps: 150    lr: 0.0001     evaluation reward: 3.47\n",
      "episode: 1188   score: 3.0   memory length: 234657   epsilon: 0.7333771600057881    steps: 244    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1189   score: 1.0   memory length: 234827   epsilon: 0.7330405600057954    steps: 170    lr: 0.0001     evaluation reward: 3.46\n",
      "episode: 1190   score: 2.0   memory length: 235024   epsilon: 0.7326505000058039    steps: 197    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1191   score: 3.0   memory length: 235250   epsilon: 0.7322030200058136    steps: 226    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1192   score: 2.0   memory length: 235467   epsilon: 0.7317733600058229    steps: 217    lr: 0.0001     evaluation reward: 3.43\n",
      "episode: 1193   score: 5.0   memory length: 235807   epsilon: 0.7311001600058376    steps: 340    lr: 0.0001     evaluation reward: 3.46\n",
      "episode: 1194   score: 3.0   memory length: 236055   epsilon: 0.7306091200058482    steps: 248    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1195   score: 7.0   memory length: 236442   epsilon: 0.7298428600058648    steps: 387    lr: 0.0001     evaluation reward: 3.52\n",
      "episode: 1196   score: 4.0   memory length: 236760   epsilon: 0.7292132200058785    steps: 318    lr: 0.0001     evaluation reward: 3.49\n",
      "episode: 1197   score: 5.0   memory length: 237085   epsilon: 0.7285697200058925    steps: 325    lr: 0.0001     evaluation reward: 3.52\n",
      "episode: 1198   score: 3.0   memory length: 237331   epsilon: 0.7280826400059031    steps: 246    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1199   score: 3.0   memory length: 237558   epsilon: 0.7276331800059128    steps: 227    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1200   score: 5.0   memory length: 237869   epsilon: 0.7270174000059262    steps: 311    lr: 0.0001     evaluation reward: 3.56\n",
      "episode: 1201   score: 3.0   memory length: 238114   epsilon: 0.7265323000059367    steps: 245    lr: 0.0001     evaluation reward: 3.57\n",
      "episode: 1202   score: 0.0   memory length: 238237   epsilon: 0.726288760005942    steps: 123    lr: 0.0001     evaluation reward: 3.49\n",
      "episode: 1203   score: 3.0   memory length: 238468   epsilon: 0.7258313800059519    steps: 231    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1204   score: 5.0   memory length: 238809   epsilon: 0.7251562000059666    steps: 341    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1205   score: 6.0   memory length: 239184   epsilon: 0.7244137000059827    steps: 375    lr: 0.0001     evaluation reward: 3.52\n",
      "episode: 1206   score: 4.0   memory length: 239480   epsilon: 0.7238276200059954    steps: 296    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1207   score: 0.0   memory length: 239603   epsilon: 0.7235840800060007    steps: 123    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1208   score: 3.0   memory length: 239829   epsilon: 0.7231366000060104    steps: 226    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1209   score: 5.0   memory length: 240133   epsilon: 0.7225346800060235    steps: 304    lr: 0.0001     evaluation reward: 3.49\n",
      "episode: 1210   score: 3.0   memory length: 240376   epsilon: 0.722053540006034    steps: 243    lr: 0.0001     evaluation reward: 3.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1211   score: 3.0   memory length: 240642   epsilon: 0.7215268600060454    steps: 266    lr: 0.0001     evaluation reward: 3.49\n",
      "episode: 1212   score: 2.0   memory length: 240840   epsilon: 0.7211348200060539    steps: 198    lr: 0.0001     evaluation reward: 3.47\n",
      "episode: 1213   score: 7.0   memory length: 241232   epsilon: 0.7203586600060707    steps: 392    lr: 0.0001     evaluation reward: 3.53\n",
      "episode: 1214   score: 1.0   memory length: 241382   epsilon: 0.7200616600060772    steps: 150    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1215   score: 1.0   memory length: 241550   epsilon: 0.7197290200060844    steps: 168    lr: 0.0001     evaluation reward: 3.47\n",
      "episode: 1216   score: 4.0   memory length: 241825   epsilon: 0.7191845200060962    steps: 275    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1217   score: 8.0   memory length: 242307   epsilon: 0.718230160006117    steps: 482    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1218   score: 5.0   memory length: 242633   epsilon: 0.717584680006131    steps: 326    lr: 0.0001     evaluation reward: 3.53\n",
      "episode: 1219   score: 4.0   memory length: 242911   epsilon: 0.7170342400061429    steps: 278    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1220   score: 1.0   memory length: 243063   epsilon: 0.7167332800061494    steps: 152    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1221   score: 2.0   memory length: 243281   epsilon: 0.7163016400061588    steps: 218    lr: 0.0001     evaluation reward: 3.46\n",
      "episode: 1222   score: 1.0   memory length: 243451   epsilon: 0.7159650400061661    steps: 170    lr: 0.0001     evaluation reward: 3.47\n",
      "episode: 1223   score: 5.0   memory length: 243793   epsilon: 0.7152878800061808    steps: 342    lr: 0.0001     evaluation reward: 3.49\n",
      "episode: 1224   score: 5.0   memory length: 244084   epsilon: 0.7147117000061933    steps: 291    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1225   score: 1.0   memory length: 244253   epsilon: 0.7143770800062006    steps: 169    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1226   score: 6.0   memory length: 244596   epsilon: 0.7136979400062153    steps: 343    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1227   score: 5.0   memory length: 244900   epsilon: 0.7130960200062284    steps: 304    lr: 0.0001     evaluation reward: 3.56\n",
      "episode: 1228   score: 3.0   memory length: 245148   epsilon: 0.7126049800062391    steps: 248    lr: 0.0001     evaluation reward: 3.57\n",
      "episode: 1229   score: 7.0   memory length: 245396   epsilon: 0.7121139400062497    steps: 248    lr: 0.0001     evaluation reward: 3.58\n",
      "episode: 1230   score: 2.0   memory length: 245615   epsilon: 0.7116803200062591    steps: 219    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1231   score: 6.0   memory length: 245977   epsilon: 0.7109635600062747    steps: 362    lr: 0.0001     evaluation reward: 3.56\n",
      "episode: 1232   score: 4.0   memory length: 246272   epsilon: 0.7103794600062874    steps: 295    lr: 0.0001     evaluation reward: 3.55\n",
      "episode: 1233   score: 2.0   memory length: 246469   epsilon: 0.7099894000062958    steps: 197    lr: 0.0001     evaluation reward: 3.53\n",
      "episode: 1234   score: 9.0   memory length: 246847   epsilon: 0.7092409600063121    steps: 378    lr: 0.0001     evaluation reward: 3.58\n",
      "episode: 1235   score: 2.0   memory length: 247050   epsilon: 0.7088390200063208    steps: 203    lr: 0.0001     evaluation reward: 3.57\n",
      "episode: 1236   score: 5.0   memory length: 247376   epsilon: 0.7081935400063348    steps: 326    lr: 0.0001     evaluation reward: 3.57\n",
      "episode: 1237   score: 4.0   memory length: 247651   epsilon: 0.7076490400063467    steps: 275    lr: 0.0001     evaluation reward: 3.57\n",
      "episode: 1238   score: 5.0   memory length: 247960   epsilon: 0.7070372200063599    steps: 309    lr: 0.0001     evaluation reward: 3.6\n",
      "episode: 1239   score: 4.0   memory length: 248257   epsilon: 0.7064491600063727    steps: 297    lr: 0.0001     evaluation reward: 3.63\n",
      "episode: 1240   score: 3.0   memory length: 248483   epsilon: 0.7060016800063824    steps: 226    lr: 0.0001     evaluation reward: 3.61\n",
      "episode: 1241   score: 3.0   memory length: 248729   epsilon: 0.705514600006393    steps: 246    lr: 0.0001     evaluation reward: 3.55\n",
      "episode: 1242   score: 7.0   memory length: 249109   epsilon: 0.7047622000064093    steps: 380    lr: 0.0001     evaluation reward: 3.6\n",
      "episode: 1243   score: 3.0   memory length: 249338   epsilon: 0.7043087800064192    steps: 229    lr: 0.0001     evaluation reward: 3.6\n",
      "episode: 1244   score: 6.0   memory length: 249710   epsilon: 0.7035722200064352    steps: 372    lr: 0.0001     evaluation reward: 3.63\n",
      "episode: 1245   score: 3.0   memory length: 249939   epsilon: 0.703118800006445    steps: 229    lr: 0.0001     evaluation reward: 3.62\n",
      "episode: 1246   score: 3.0   memory length: 250184   epsilon: 0.7026337000064555    steps: 245    lr: 0.0001     evaluation reward: 3.63\n",
      "episode: 1247   score: 2.0   memory length: 250386   epsilon: 0.7022337400064642    steps: 202    lr: 0.0001     evaluation reward: 3.62\n",
      "episode: 1248   score: 5.0   memory length: 250701   epsilon: 0.7016100400064778    steps: 315    lr: 0.0001     evaluation reward: 3.65\n",
      "episode: 1249   score: 3.0   memory length: 250926   epsilon: 0.7011645400064874    steps: 225    lr: 0.0001     evaluation reward: 3.64\n",
      "episode: 1250   score: 12.0   memory length: 251456   epsilon: 0.7001151400065102    steps: 530    lr: 0.0001     evaluation reward: 3.7\n",
      "episode: 1251   score: 3.0   memory length: 251670   epsilon: 0.6996914200065194    steps: 214    lr: 0.0001     evaluation reward: 3.71\n",
      "episode: 1252   score: 2.0   memory length: 251852   epsilon: 0.6993310600065272    steps: 182    lr: 0.0001     evaluation reward: 3.71\n",
      "episode: 1253   score: 3.0   memory length: 252082   epsilon: 0.6988756600065371    steps: 230    lr: 0.0001     evaluation reward: 3.74\n",
      "episode: 1254   score: 5.0   memory length: 252407   epsilon: 0.6982321600065511    steps: 325    lr: 0.0001     evaluation reward: 3.74\n",
      "episode: 1255   score: 2.0   memory length: 252604   epsilon: 0.6978421000065596    steps: 197    lr: 0.0001     evaluation reward: 3.7\n",
      "episode: 1256   score: 4.0   memory length: 252864   epsilon: 0.6973273000065707    steps: 260    lr: 0.0001     evaluation reward: 3.71\n",
      "episode: 1257   score: 4.0   memory length: 253123   epsilon: 0.6968144800065819    steps: 259    lr: 0.0001     evaluation reward: 3.74\n",
      "episode: 1258   score: 6.0   memory length: 253499   epsilon: 0.696070000006598    steps: 376    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1259   score: 5.0   memory length: 253825   epsilon: 0.695424520006612    steps: 326    lr: 0.0001     evaluation reward: 3.75\n",
      "episode: 1260   score: 4.0   memory length: 254096   epsilon: 0.6948879400066237    steps: 271    lr: 0.0001     evaluation reward: 3.73\n",
      "episode: 1261   score: 7.0   memory length: 254463   epsilon: 0.6941612800066395    steps: 367    lr: 0.0001     evaluation reward: 3.72\n",
      "episode: 1262   score: 8.0   memory length: 254892   epsilon: 0.6933118600066579    steps: 429    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1263   score: 2.0   memory length: 255073   epsilon: 0.6929534800066657    steps: 181    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1264   score: 6.0   memory length: 255428   epsilon: 0.6922505800066809    steps: 355    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1265   score: 6.0   memory length: 255803   epsilon: 0.6915080800066971    steps: 375    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1266   score: 4.0   memory length: 256097   epsilon: 0.6909259600067097    steps: 294    lr: 0.0001     evaluation reward: 3.84\n",
      "episode: 1267   score: 5.0   memory length: 256460   epsilon: 0.6902072200067253    steps: 363    lr: 0.0001     evaluation reward: 3.84\n",
      "episode: 1268   score: 5.0   memory length: 256752   epsilon: 0.6896290600067378    steps: 292    lr: 0.0001     evaluation reward: 3.87\n",
      "episode: 1269   score: 4.0   memory length: 257027   epsilon: 0.6890845600067497    steps: 275    lr: 0.0001     evaluation reward: 3.87\n",
      "episode: 1270   score: 3.0   memory length: 257272   epsilon: 0.6885994600067602    steps: 245    lr: 0.0001     evaluation reward: 3.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1271   score: 3.0   memory length: 257501   epsilon: 0.68814604000677    steps: 229    lr: 0.0001     evaluation reward: 3.86\n",
      "episode: 1272   score: 7.0   memory length: 257870   epsilon: 0.6874154200067859    steps: 369    lr: 0.0001     evaluation reward: 3.89\n",
      "episode: 1273   score: 6.0   memory length: 258212   epsilon: 0.6867382600068006    steps: 342    lr: 0.0001     evaluation reward: 3.93\n",
      "episode: 1274   score: 7.0   memory length: 258591   epsilon: 0.6859878400068169    steps: 379    lr: 0.0001     evaluation reward: 3.95\n",
      "episode: 1275   score: 2.0   memory length: 258808   epsilon: 0.6855581800068262    steps: 217    lr: 0.0001     evaluation reward: 3.95\n",
      "episode: 1276   score: 2.0   memory length: 259008   epsilon: 0.6851621800068348    steps: 200    lr: 0.0001     evaluation reward: 3.94\n",
      "episode: 1277   score: 7.0   memory length: 259431   epsilon: 0.684324640006853    steps: 423    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 1278   score: 5.0   memory length: 259732   epsilon: 0.6837286600068659    steps: 301    lr: 0.0001     evaluation reward: 4.01\n",
      "episode: 1279   score: 5.0   memory length: 260027   epsilon: 0.6831445600068786    steps: 295    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 1280   score: 2.0   memory length: 260225   epsilon: 0.6827525200068871    steps: 198    lr: 0.0001     evaluation reward: 3.98\n",
      "episode: 1281   score: 2.0   memory length: 260406   epsilon: 0.6823941400068949    steps: 181    lr: 0.0001     evaluation reward: 3.95\n",
      "episode: 1282   score: 11.0   memory length: 260838   epsilon: 0.6815387800069135    steps: 432    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 1283   score: 4.0   memory length: 261150   epsilon: 0.6809210200069269    steps: 312    lr: 0.0001     evaluation reward: 4.04\n",
      "episode: 1284   score: 5.0   memory length: 261494   epsilon: 0.6802399000069417    steps: 344    lr: 0.0001     evaluation reward: 4.04\n",
      "episode: 1285   score: 9.0   memory length: 261860   epsilon: 0.6795152200069574    steps: 366    lr: 0.0001     evaluation reward: 4.12\n",
      "episode: 1286   score: 4.0   memory length: 262140   epsilon: 0.6789608200069694    steps: 280    lr: 0.0001     evaluation reward: 4.12\n",
      "episode: 1287   score: 1.0   memory length: 262291   epsilon: 0.6786618400069759    steps: 151    lr: 0.0001     evaluation reward: 4.12\n",
      "episode: 1288   score: 5.0   memory length: 262574   epsilon: 0.6781015000069881    steps: 283    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 1289   score: 3.0   memory length: 262838   epsilon: 0.6775787800069994    steps: 264    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 1290   score: 3.0   memory length: 263083   epsilon: 0.67709368000701    steps: 245    lr: 0.0001     evaluation reward: 4.17\n",
      "episode: 1291   score: 2.0   memory length: 263301   epsilon: 0.6766620400070193    steps: 218    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 1292   score: 6.0   memory length: 263691   epsilon: 0.6758898400070361    steps: 390    lr: 0.0001     evaluation reward: 4.2\n",
      "episode: 1293   score: 7.0   memory length: 264113   epsilon: 0.6750542800070543    steps: 422    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 1294   score: 5.0   memory length: 264393   epsilon: 0.6744998800070663    steps: 280    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1295   score: 8.0   memory length: 264722   epsilon: 0.6738484600070804    steps: 329    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1296   score: 4.0   memory length: 264962   epsilon: 0.6733732600070907    steps: 240    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1297   score: 5.0   memory length: 265252   epsilon: 0.6727990600071032    steps: 290    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1298   score: 5.0   memory length: 265543   epsilon: 0.6722228800071157    steps: 291    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 1299   score: 2.0   memory length: 265723   epsilon: 0.6718664800071235    steps: 180    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1300   score: 2.0   memory length: 265940   epsilon: 0.6714368200071328    steps: 217    lr: 0.0001     evaluation reward: 4.23\n",
      "episode: 1301   score: 3.0   memory length: 266206   epsilon: 0.6709101400071442    steps: 266    lr: 0.0001     evaluation reward: 4.23\n",
      "episode: 1302   score: 3.0   memory length: 266431   epsilon: 0.6704646400071539    steps: 225    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1303   score: 5.0   memory length: 266800   epsilon: 0.6697340200071698    steps: 369    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 1304   score: 3.0   memory length: 267030   epsilon: 0.6692786200071796    steps: 230    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1305   score: 6.0   memory length: 267377   epsilon: 0.6685915600071946    steps: 347    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1306   score: 0.0   memory length: 267500   epsilon: 0.6683480200071998    steps: 123    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 1307   score: 3.0   memory length: 267728   epsilon: 0.6678965800072096    steps: 228    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1308   score: 3.0   memory length: 267980   epsilon: 0.6673976200072205    steps: 252    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1309   score: 0.0   memory length: 268103   epsilon: 0.6671540800072258    steps: 123    lr: 0.0001     evaluation reward: 4.2\n",
      "episode: 1310   score: 2.0   memory length: 268283   epsilon: 0.6667976800072335    steps: 180    lr: 0.0001     evaluation reward: 4.19\n",
      "episode: 1311   score: 9.0   memory length: 268784   epsilon: 0.665805700007255    steps: 501    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1312   score: 8.0   memory length: 269207   epsilon: 0.6649681600072732    steps: 423    lr: 0.0001     evaluation reward: 4.31\n",
      "episode: 1313   score: 3.0   memory length: 269453   epsilon: 0.6644810800072838    steps: 246    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 1314   score: 3.0   memory length: 269681   epsilon: 0.6640296400072936    steps: 228    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1315   score: 2.0   memory length: 269897   epsilon: 0.6636019600073029    steps: 216    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1316   score: 3.0   memory length: 270127   epsilon: 0.6631465600073128    steps: 230    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1317   score: 3.0   memory length: 270352   epsilon: 0.6627010600073224    steps: 225    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1318   score: 3.0   memory length: 270583   epsilon: 0.6622436800073324    steps: 231    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 1319   score: 4.0   memory length: 270880   epsilon: 0.6616556200073451    steps: 297    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 1320   score: 5.0   memory length: 271222   epsilon: 0.6609784600073598    steps: 342    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1321   score: 11.0   memory length: 271649   epsilon: 0.6601330000073782    steps: 427    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 1322   score: 3.0   memory length: 271894   epsilon: 0.6596479000073887    steps: 245    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1323   score: 3.0   memory length: 272140   epsilon: 0.6591608200073993    steps: 246    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 1324   score: 9.0   memory length: 272612   epsilon: 0.6582262600074196    steps: 472    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1325   score: 4.0   memory length: 272870   epsilon: 0.6577154200074307    steps: 258    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1326   score: 5.0   memory length: 273214   epsilon: 0.6570343000074454    steps: 344    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1327   score: 11.0   memory length: 273666   epsilon: 0.6561393400074649    steps: 452    lr: 0.0001     evaluation reward: 4.47\n",
      "episode: 1328   score: 4.0   memory length: 273960   epsilon: 0.6555572200074775    steps: 294    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1329   score: 2.0   memory length: 274160   epsilon: 0.6551612200074861    steps: 200    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1330   score: 5.0   memory length: 274486   epsilon: 0.6545157400075001    steps: 326    lr: 0.0001     evaluation reward: 4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1331   score: 2.0   memory length: 274667   epsilon: 0.6541573600075079    steps: 181    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1332   score: 7.0   memory length: 275055   epsilon: 0.6533891200075246    steps: 388    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1333   score: 6.0   memory length: 275398   epsilon: 0.6527099800075393    steps: 343    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 1334   score: 3.0   memory length: 275663   epsilon: 0.6521852800075507    steps: 265    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1335   score: 2.0   memory length: 275864   epsilon: 0.6517873000075594    steps: 201    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1336   score: 11.0   memory length: 276427   epsilon: 0.6506725600075836    steps: 563    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 1337   score: 5.0   memory length: 276753   epsilon: 0.6500270800075976    steps: 326    lr: 0.0001     evaluation reward: 4.5\n",
      "episode: 1338   score: 0.0   memory length: 276876   epsilon: 0.6497835400076029    steps: 123    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1339   score: 6.0   memory length: 277205   epsilon: 0.649132120007617    steps: 329    lr: 0.0001     evaluation reward: 4.47\n",
      "episode: 1340   score: 1.0   memory length: 277356   epsilon: 0.6488331400076235    steps: 151    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1341   score: 3.0   memory length: 277603   epsilon: 0.6483440800076341    steps: 247    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1342   score: 1.0   memory length: 277753   epsilon: 0.6480470800076406    steps: 150    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1343   score: 7.0   memory length: 278142   epsilon: 0.6472768600076573    steps: 389    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1344   score: 8.0   memory length: 278620   epsilon: 0.6463304200076778    steps: 478    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1345   score: 6.0   memory length: 278942   epsilon: 0.6456928600076917    steps: 322    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1346   score: 3.0   memory length: 279210   epsilon: 0.6451622200077032    steps: 268    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1347   score: 2.0   memory length: 279429   epsilon: 0.6447286000077126    steps: 219    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1348   score: 5.0   memory length: 279752   epsilon: 0.6440890600077265    steps: 323    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1349   score: 4.0   memory length: 279995   epsilon: 0.6436079200077369    steps: 243    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 1350   score: 5.0   memory length: 280339   epsilon: 0.6429268000077517    steps: 344    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1351   score: 2.0   memory length: 280519   epsilon: 0.6425704000077594    steps: 180    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1352   score: 7.0   memory length: 280944   epsilon: 0.6417289000077777    steps: 425    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 1353   score: 1.0   memory length: 281112   epsilon: 0.6413962600077849    steps: 168    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 1354   score: 7.0   memory length: 281528   epsilon: 0.6405725800078028    steps: 416    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 1355   score: 1.0   memory length: 281679   epsilon: 0.6402736000078093    steps: 151    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1356   score: 3.0   memory length: 281889   epsilon: 0.6398578000078183    steps: 210    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 1357   score: 1.0   memory length: 282061   epsilon: 0.6395172400078257    steps: 172    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1358   score: 4.0   memory length: 282336   epsilon: 0.6389727400078375    steps: 275    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1359   score: 6.0   memory length: 282714   epsilon: 0.6382243000078538    steps: 378    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1360   score: 5.0   memory length: 283031   epsilon: 0.6375966400078674    steps: 317    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1361   score: 6.0   memory length: 283389   epsilon: 0.6368878000078828    steps: 358    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1362   score: 6.0   memory length: 283741   epsilon: 0.6361908400078979    steps: 352    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 1363   score: 1.0   memory length: 283891   epsilon: 0.6358938400079044    steps: 150    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1364   score: 11.0   memory length: 284465   epsilon: 0.6347573200079291    steps: 574    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1365   score: 5.0   memory length: 284812   epsilon: 0.634070260007944    steps: 347    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1366   score: 3.0   memory length: 285057   epsilon: 0.6335851600079545    steps: 245    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1367   score: 8.0   memory length: 285499   epsilon: 0.6327100000079735    steps: 442    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1368   score: 3.0   memory length: 285726   epsilon: 0.6322605400079833    steps: 227    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1369   score: 4.0   memory length: 285985   epsilon: 0.6317477200079944    steps: 259    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1370   score: 3.0   memory length: 286194   epsilon: 0.6313339000080034    steps: 209    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1371   score: 6.0   memory length: 286553   epsilon: 0.6306230800080188    steps: 359    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 1372   score: 3.0   memory length: 286783   epsilon: 0.6301676800080287    steps: 230    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1373   score: 8.0   memory length: 287220   epsilon: 0.6293024200080475    steps: 437    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1374   score: 5.0   memory length: 287527   epsilon: 0.6286945600080607    steps: 307    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1375   score: 5.0   memory length: 287857   epsilon: 0.6280411600080749    steps: 330    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1376   score: 3.0   memory length: 288086   epsilon: 0.6275877400080847    steps: 229    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 1377   score: 5.0   memory length: 288392   epsilon: 0.6269818600080979    steps: 306    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1378   score: 3.0   memory length: 288627   epsilon: 0.626516560008108    steps: 235    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1379   score: 6.0   memory length: 288999   epsilon: 0.625780000008124    steps: 372    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 1380   score: 3.0   memory length: 289246   epsilon: 0.6252909400081346    steps: 247    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1381   score: 5.0   memory length: 289550   epsilon: 0.6246890200081476    steps: 304    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1382   score: 5.0   memory length: 289873   epsilon: 0.6240494800081615    steps: 323    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1383   score: 4.0   memory length: 290168   epsilon: 0.6234653800081742    steps: 295    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1384   score: 6.0   memory length: 290498   epsilon: 0.6228119800081884    steps: 330    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1385   score: 6.0   memory length: 290872   epsilon: 0.6220714600082045    steps: 374    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1386   score: 1.0   memory length: 291023   epsilon: 0.621772480008211    steps: 151    lr: 0.0001     evaluation reward: 4.34\n",
      "episode: 1387   score: 7.0   memory length: 291419   epsilon: 0.620988400008228    steps: 396    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1388   score: 8.0   memory length: 291867   epsilon: 0.6201013600082472    steps: 448    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 1389   score: 10.0   memory length: 292360   epsilon: 0.6191252200082684    steps: 493    lr: 0.0001     evaluation reward: 4.5\n",
      "episode: 1390   score: 6.0   memory length: 292772   epsilon: 0.6183094600082861    steps: 412    lr: 0.0001     evaluation reward: 4.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1391   score: 9.0   memory length: 293237   epsilon: 0.6173887600083061    steps: 465    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 1392   score: 5.0   memory length: 293580   epsilon: 0.6167096200083209    steps: 343    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 1393   score: 10.0   memory length: 294004   epsilon: 0.6158701000083391    steps: 424    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 1394   score: 7.0   memory length: 294376   epsilon: 0.6151335400083551    steps: 372    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 1395   score: 6.0   memory length: 294768   epsilon: 0.6143573800083719    steps: 392    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 1396   score: 4.0   memory length: 295050   epsilon: 0.613799020008384    steps: 282    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 1397   score: 6.0   memory length: 295424   epsilon: 0.6130585000084001    steps: 374    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 1398   score: 6.0   memory length: 295765   epsilon: 0.6123833200084148    steps: 341    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 1399   score: 6.0   memory length: 296103   epsilon: 0.6117140800084293    steps: 338    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 1400   score: 4.0   memory length: 296380   epsilon: 0.6111656200084412    steps: 277    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 1401   score: 4.0   memory length: 296676   epsilon: 0.6105795400084539    steps: 296    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 1402   score: 4.0   memory length: 296951   epsilon: 0.6100350400084658    steps: 275    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 1403   score: 1.0   memory length: 297120   epsilon: 0.609700420008473    steps: 169    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 1404   score: 3.0   memory length: 297366   epsilon: 0.6092133400084836    steps: 246    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 1405   score: 8.0   memory length: 297813   epsilon: 0.6083282800085028    steps: 447    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 1406   score: 3.0   memory length: 298059   epsilon: 0.6078412000085134    steps: 246    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 1407   score: 3.0   memory length: 298285   epsilon: 0.6073937200085231    steps: 226    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 1408   score: 7.0   memory length: 298711   epsilon: 0.6065502400085414    steps: 426    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 1409   score: 7.0   memory length: 299097   epsilon: 0.605785960008558    steps: 386    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 1410   score: 5.0   memory length: 299441   epsilon: 0.6051048400085728    steps: 344    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 1411   score: 4.0   memory length: 299701   epsilon: 0.604590040008584    steps: 260    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1412   score: 5.0   memory length: 300012   epsilon: 0.6039742600085973    steps: 311    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 1413   score: 7.0   memory length: 300364   epsilon: 0.6032773000086125    steps: 352    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 1414   score: 4.0   memory length: 300639   epsilon: 0.6027328000086243    steps: 275    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 1415   score: 4.0   memory length: 300914   epsilon: 0.6021883000086361    steps: 275    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 1416   score: 6.0   memory length: 301289   epsilon: 0.6014458000086522    steps: 375    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 1417   score: 6.0   memory length: 301646   epsilon: 0.6007389400086676    steps: 357    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 1418   score: 2.0   memory length: 301844   epsilon: 0.6003469000086761    steps: 198    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 1419   score: 4.0   memory length: 302121   epsilon: 0.599798440008688    steps: 277    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 1420   score: 3.0   memory length: 302351   epsilon: 0.5993430400086979    steps: 230    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 1421   score: 5.0   memory length: 302677   epsilon: 0.5986975600087119    steps: 326    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 1422   score: 1.0   memory length: 302828   epsilon: 0.5983985800087184    steps: 151    lr: 0.0001     evaluation reward: 4.81\n",
      "episode: 1423   score: 1.0   memory length: 302978   epsilon: 0.5981015800087248    steps: 150    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 1424   score: 6.0   memory length: 303316   epsilon: 0.5974323400087393    steps: 338    lr: 0.0001     evaluation reward: 4.76\n",
      "episode: 1425   score: 6.0   memory length: 303671   epsilon: 0.5967294400087546    steps: 355    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 1426   score: 7.0   memory length: 304057   epsilon: 0.5959651600087712    steps: 386    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 1427   score: 6.0   memory length: 304429   epsilon: 0.5952286000087872    steps: 372    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 1428   score: 4.0   memory length: 304689   epsilon: 0.5947138000087984    steps: 260    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 1429   score: 4.0   memory length: 304963   epsilon: 0.5941712800088101    steps: 274    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 1430   score: 6.0   memory length: 305353   epsilon: 0.5933990800088269    steps: 390    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 1431   score: 7.0   memory length: 305800   epsilon: 0.5925140200088461    steps: 447    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 1432   score: 3.0   memory length: 306031   epsilon: 0.592056640008856    steps: 231    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 1433   score: 6.0   memory length: 306367   epsilon: 0.5913913600088705    steps: 336    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 1434   score: 7.0   memory length: 306726   epsilon: 0.5906805400088859    steps: 359    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 1435   score: 3.0   memory length: 306953   epsilon: 0.5902310800088957    steps: 227    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 1436   score: 5.0   memory length: 307260   epsilon: 0.5896232200089089    steps: 307    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 1437   score: 3.0   memory length: 307488   epsilon: 0.5891717800089187    steps: 228    lr: 0.0001     evaluation reward: 4.76\n",
      "episode: 1438   score: 6.0   memory length: 307826   epsilon: 0.5885025400089332    steps: 338    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1439   score: 5.0   memory length: 308152   epsilon: 0.5878570600089472    steps: 326    lr: 0.0001     evaluation reward: 4.81\n",
      "episode: 1440   score: 6.0   memory length: 308494   epsilon: 0.5871799000089619    steps: 342    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 1441   score: 4.0   memory length: 308744   epsilon: 0.5866849000089727    steps: 250    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 1442   score: 4.0   memory length: 308983   epsilon: 0.5862116800089829    steps: 239    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 1443   score: 5.0   memory length: 309325   epsilon: 0.5855345200089976    steps: 342    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 1444   score: 5.0   memory length: 309651   epsilon: 0.5848890400090117    steps: 326    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 1445   score: 4.0   memory length: 309924   epsilon: 0.5843485000090234    steps: 273    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 1446   score: 4.0   memory length: 310201   epsilon: 0.5838000400090353    steps: 277    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 1447   score: 8.0   memory length: 310615   epsilon: 0.5829803200090531    steps: 414    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 1448   score: 5.0   memory length: 310938   epsilon: 0.582340780009067    steps: 323    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 1449   score: 7.0   memory length: 311312   epsilon: 0.581600260009083    steps: 374    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 1450   score: 3.0   memory length: 311543   epsilon: 0.581142880009093    steps: 231    lr: 0.0001     evaluation reward: 4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1451   score: 7.0   memory length: 311956   epsilon: 0.5803251400091107    steps: 413    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 1452   score: 4.0   memory length: 312200   epsilon: 0.5798420200091212    steps: 244    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 1453   score: 3.0   memory length: 312450   epsilon: 0.579347020009132    steps: 250    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 1454   score: 7.0   memory length: 312872   epsilon: 0.5785114600091501    steps: 422    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 1455   score: 6.0   memory length: 313248   epsilon: 0.5777669800091663    steps: 376    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 1456   score: 8.0   memory length: 313676   epsilon: 0.5769195400091847    steps: 428    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 1457   score: 12.0   memory length: 314277   epsilon: 0.5757295600092105    steps: 601    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 1458   score: 8.0   memory length: 314701   epsilon: 0.5748900400092287    steps: 424    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 1459   score: 3.0   memory length: 314929   epsilon: 0.5744386000092385    steps: 228    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 1460   score: 10.0   memory length: 315453   epsilon: 0.573401080009261    steps: 524    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 1461   score: 4.0   memory length: 315732   epsilon: 0.572848660009273    steps: 279    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 1462   score: 11.0   memory length: 316108   epsilon: 0.5721041800092892    steps: 376    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 1463   score: 7.0   memory length: 316514   epsilon: 0.5713003000093066    steps: 406    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 1464   score: 7.0   memory length: 316927   epsilon: 0.5704825600093244    steps: 413    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 1465   score: 9.0   memory length: 317407   epsilon: 0.569532160009345    steps: 480    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 1466   score: 5.0   memory length: 317713   epsilon: 0.5689262800093582    steps: 306    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 1467   score: 2.0   memory length: 317913   epsilon: 0.5685302800093668    steps: 200    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 1468   score: 9.0   memory length: 318363   epsilon: 0.5676392800093861    steps: 450    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 1469   score: 4.0   memory length: 318607   epsilon: 0.5671561600093966    steps: 244    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 1470   score: 4.0   memory length: 318868   epsilon: 0.5666393800094078    steps: 261    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 1471   score: 3.0   memory length: 319096   epsilon: 0.5661879400094176    steps: 228    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 1472   score: 7.0   memory length: 319518   epsilon: 0.5653523800094358    steps: 422    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 1473   score: 4.0   memory length: 319799   epsilon: 0.5647960000094479    steps: 281    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 1474   score: 11.0   memory length: 320230   epsilon: 0.5639426200094664    steps: 431    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 1475   score: 4.0   memory length: 320508   epsilon: 0.5633921800094783    steps: 278    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 1476   score: 3.0   memory length: 320737   epsilon: 0.5629387600094882    steps: 229    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 1477   score: 5.0   memory length: 321081   epsilon: 0.562257640009503    steps: 344    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 1478   score: 6.0   memory length: 321440   epsilon: 0.5615468200095184    steps: 359    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 1479   score: 5.0   memory length: 321787   epsilon: 0.5608597600095333    steps: 347    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 1480   score: 4.0   memory length: 322080   epsilon: 0.5602796200095459    steps: 293    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 1481   score: 5.0   memory length: 322352   epsilon: 0.5597410600095576    steps: 272    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 1482   score: 11.0   memory length: 322748   epsilon: 0.5589569800095746    steps: 396    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 1483   score: 9.0   memory length: 323187   epsilon: 0.5580877600095935    steps: 439    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1484   score: 5.0   memory length: 323477   epsilon: 0.557513560009606    steps: 290    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 1485   score: 6.0   memory length: 323846   epsilon: 0.5567829400096218    steps: 369    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 1486   score: 6.0   memory length: 324191   epsilon: 0.5560998400096366    steps: 345    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 1487   score: 4.0   memory length: 324489   epsilon: 0.5555098000096494    steps: 298    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 1488   score: 14.0   memory length: 325050   epsilon: 0.5543990200096736    steps: 561    lr: 0.0001     evaluation reward: 5.57\n",
      "episode: 1489   score: 6.0   memory length: 325389   epsilon: 0.5537278000096881    steps: 339    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 1490   score: 4.0   memory length: 325664   epsilon: 0.5531833000097    steps: 275    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 1491   score: 5.0   memory length: 325988   epsilon: 0.5525417800097139    steps: 324    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 1492   score: 8.0   memory length: 326466   epsilon: 0.5515953400097344    steps: 478    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1493   score: 4.0   memory length: 326706   epsilon: 0.5511201400097447    steps: 240    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 1494   score: 4.0   memory length: 326983   epsilon: 0.5505716800097566    steps: 277    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 1495   score: 5.0   memory length: 327308   epsilon: 0.5499281800097706    steps: 325    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 1496   score: 3.0   memory length: 327519   epsilon: 0.5495104000097797    steps: 211    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 1497   score: 4.0   memory length: 327799   epsilon: 0.5489560000097917    steps: 280    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 1498   score: 4.0   memory length: 328076   epsilon: 0.5484075400098036    steps: 277    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 1499   score: 8.0   memory length: 328493   epsilon: 0.5475818800098216    steps: 417    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 1500   score: 2.0   memory length: 328713   epsilon: 0.547146280009831    steps: 220    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 1501   score: 4.0   memory length: 328970   epsilon: 0.5466374200098421    steps: 257    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 1502   score: 6.0   memory length: 329324   epsilon: 0.5459365000098573    steps: 354    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 1503   score: 6.0   memory length: 329676   epsilon: 0.5452395400098724    steps: 352    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 1504   score: 2.0   memory length: 329873   epsilon: 0.5448494800098809    steps: 197    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 1505   score: 5.0   memory length: 330233   epsilon: 0.5441366800098963    steps: 360    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 1506   score: 4.0   memory length: 330495   epsilon: 0.5436179200099076    steps: 262    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 1507   score: 15.0   memory length: 331196   epsilon: 0.5422299400099377    steps: 701    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 1508   score: 5.0   memory length: 331504   epsilon: 0.541620100009951    steps: 308    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 1509   score: 10.0   memory length: 332059   epsilon: 0.5405212000099748    steps: 555    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 1510   score: 3.0   memory length: 332267   epsilon: 0.5401093600099838    steps: 208    lr: 0.0001     evaluation reward: 5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1511   score: 6.0   memory length: 332643   epsilon: 0.5393648800099999    steps: 376    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 1512   score: 10.0   memory length: 333140   epsilon: 0.5383808200100213    steps: 497    lr: 0.0001     evaluation reward: 5.57\n",
      "episode: 1513   score: 4.0   memory length: 333398   epsilon: 0.5378699800100324    steps: 258    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 1514   score: 15.0   memory length: 333984   epsilon: 0.5367097000100576    steps: 586    lr: 0.0001     evaluation reward: 5.65\n",
      "episode: 1515   score: 7.0   memory length: 334351   epsilon: 0.5359830400100734    steps: 367    lr: 0.0001     evaluation reward: 5.68\n",
      "episode: 1516   score: 6.0   memory length: 334710   epsilon: 0.5352722200100888    steps: 359    lr: 0.0001     evaluation reward: 5.68\n",
      "episode: 1517   score: 9.0   memory length: 335225   epsilon: 0.5342525200101109    steps: 515    lr: 0.0001     evaluation reward: 5.71\n",
      "episode: 1518   score: 2.0   memory length: 335406   epsilon: 0.5338941400101187    steps: 181    lr: 0.0001     evaluation reward: 5.71\n",
      "episode: 1519   score: 8.0   memory length: 335853   epsilon: 0.5330090800101379    steps: 447    lr: 0.0001     evaluation reward: 5.75\n",
      "episode: 1520   score: 6.0   memory length: 336224   epsilon: 0.5322745000101539    steps: 371    lr: 0.0001     evaluation reward: 5.78\n",
      "episode: 1521   score: 7.0   memory length: 336626   epsilon: 0.5314785400101711    steps: 402    lr: 0.0001     evaluation reward: 5.8\n",
      "episode: 1522   score: 5.0   memory length: 336934   epsilon: 0.5308687000101844    steps: 308    lr: 0.0001     evaluation reward: 5.84\n",
      "episode: 1523   score: 9.0   memory length: 337425   epsilon: 0.5298965200102055    steps: 491    lr: 0.0001     evaluation reward: 5.92\n",
      "episode: 1524   score: 4.0   memory length: 337722   epsilon: 0.5293084600102183    steps: 297    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1525   score: 7.0   memory length: 338102   epsilon: 0.5285560600102346    steps: 380    lr: 0.0001     evaluation reward: 5.91\n",
      "episode: 1526   score: 5.0   memory length: 338464   epsilon: 0.5278393000102501    steps: 362    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1527   score: 6.0   memory length: 338802   epsilon: 0.5271700600102647    steps: 338    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1528   score: 6.0   memory length: 339146   epsilon: 0.5264889400102795    steps: 344    lr: 0.0001     evaluation reward: 5.91\n",
      "episode: 1529   score: 3.0   memory length: 339377   epsilon: 0.5260315600102894    steps: 231    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1530   score: 5.0   memory length: 339701   epsilon: 0.5253900400103033    steps: 324    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1531   score: 6.0   memory length: 340078   epsilon: 0.5246435800103195    steps: 377    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1532   score: 4.0   memory length: 340319   epsilon: 0.5241664000103299    steps: 241    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1533   score: 3.0   memory length: 340568   epsilon: 0.5236733800103406    steps: 249    lr: 0.0001     evaluation reward: 5.86\n",
      "episode: 1534   score: 5.0   memory length: 340877   epsilon: 0.5230615600103539    steps: 309    lr: 0.0001     evaluation reward: 5.84\n",
      "episode: 1535   score: 12.0   memory length: 341281   epsilon: 0.5222616400103712    steps: 404    lr: 0.0001     evaluation reward: 5.93\n",
      "episode: 1536   score: 2.0   memory length: 341463   epsilon: 0.521901280010379    steps: 182    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1537   score: 6.0   memory length: 341843   epsilon: 0.5211488800103954    steps: 380    lr: 0.0001     evaluation reward: 5.93\n",
      "episode: 1538   score: 7.0   memory length: 342222   epsilon: 0.5203984600104117    steps: 379    lr: 0.0001     evaluation reward: 5.94\n",
      "episode: 1539   score: 4.0   memory length: 342481   epsilon: 0.5198856400104228    steps: 259    lr: 0.0001     evaluation reward: 5.93\n",
      "episode: 1540   score: 4.0   memory length: 342734   epsilon: 0.5193847000104337    steps: 253    lr: 0.0001     evaluation reward: 5.91\n",
      "episode: 1541   score: 2.0   memory length: 342933   epsilon: 0.5189906800104422    steps: 199    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1542   score: 3.0   memory length: 343163   epsilon: 0.5185352800104521    steps: 230    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1543   score: 5.0   memory length: 343473   epsilon: 0.5179214800104655    steps: 310    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1544   score: 8.0   memory length: 343913   epsilon: 0.5170502800104844    steps: 440    lr: 0.0001     evaluation reward: 5.91\n",
      "episode: 1545   score: 5.0   memory length: 344217   epsilon: 0.5164483600104974    steps: 304    lr: 0.0001     evaluation reward: 5.92\n",
      "episode: 1546   score: 10.0   memory length: 344698   epsilon: 0.5154959800105181    steps: 481    lr: 0.0001     evaluation reward: 5.98\n",
      "episode: 1547   score: 4.0   memory length: 345015   epsilon: 0.5148683200105317    steps: 317    lr: 0.0001     evaluation reward: 5.94\n",
      "episode: 1548   score: 6.0   memory length: 345369   epsilon: 0.514167400010547    steps: 354    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1549   score: 2.0   memory length: 345567   epsilon: 0.5137753600105555    steps: 198    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1550   score: 8.0   memory length: 346018   epsilon: 0.5128823800105748    steps: 451    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1551   score: 7.0   memory length: 346380   epsilon: 0.5121656200105904    steps: 362    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1552   score: 1.0   memory length: 346550   epsilon: 0.5118290200105977    steps: 170    lr: 0.0001     evaluation reward: 5.92\n",
      "episode: 1553   score: 8.0   memory length: 347041   epsilon: 0.5108568400106188    steps: 491    lr: 0.0001     evaluation reward: 5.97\n",
      "episode: 1554   score: 9.0   memory length: 347524   epsilon: 0.5099005000106396    steps: 483    lr: 0.0001     evaluation reward: 5.99\n",
      "episode: 1555   score: 7.0   memory length: 347901   epsilon: 0.5091540400106558    steps: 377    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 1556   score: 25.0   memory length: 348520   epsilon: 0.5079284200106824    steps: 619    lr: 0.0001     evaluation reward: 6.17\n",
      "episode: 1557   score: 6.0   memory length: 348895   epsilon: 0.5071859200106985    steps: 375    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1558   score: 5.0   memory length: 349225   epsilon: 0.5065325200107127    steps: 330    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1559   score: 6.0   memory length: 349579   epsilon: 0.5058316000107279    steps: 354    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1560   score: 8.0   memory length: 350001   epsilon: 0.504996040010746    steps: 422    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 1561   score: 12.0   memory length: 350559   epsilon: 0.50389120001077    steps: 558    lr: 0.0001     evaluation reward: 6.17\n",
      "episode: 1562   score: 5.0   memory length: 350905   epsilon: 0.5032061200107849    steps: 346    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1563   score: 9.0   memory length: 351383   epsilon: 0.5022596800108055    steps: 478    lr: 0.0001     evaluation reward: 6.13\n",
      "episode: 1564   score: 6.0   memory length: 351747   epsilon: 0.5015389600108211    steps: 364    lr: 0.0001     evaluation reward: 6.12\n",
      "episode: 1565   score: 10.0   memory length: 352144   epsilon: 0.5007529000108382    steps: 397    lr: 0.0001     evaluation reward: 6.13\n",
      "episode: 1566   score: 7.0   memory length: 352516   epsilon: 0.5000163400108542    steps: 372    lr: 0.0001     evaluation reward: 6.15\n",
      "episode: 1567   score: 4.0   memory length: 352813   epsilon: 0.4994282800108509    steps: 297    lr: 0.0001     evaluation reward: 6.17\n",
      "episode: 1568   score: 6.0   memory length: 353170   epsilon: 0.4987214200108464    steps: 357    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 1569   score: 6.0   memory length: 353539   epsilon: 0.4979908000108418    steps: 369    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1570   score: 10.0   memory length: 354057   epsilon: 0.4969651600108353    steps: 518    lr: 0.0001     evaluation reward: 6.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1571   score: 6.0   memory length: 354396   epsilon: 0.49629394001083105    steps: 339    lr: 0.0001     evaluation reward: 6.25\n",
      "episode: 1572   score: 6.0   memory length: 354770   epsilon: 0.49555342001082636    steps: 374    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1573   score: 4.0   memory length: 355036   epsilon: 0.49502674001082303    steps: 266    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1574   score: 8.0   memory length: 355459   epsilon: 0.49418920001081773    steps: 423    lr: 0.0001     evaluation reward: 6.21\n",
      "episode: 1575   score: 9.0   memory length: 355898   epsilon: 0.49331998001081223    steps: 439    lr: 0.0001     evaluation reward: 6.26\n",
      "episode: 1576   score: 13.0   memory length: 356531   epsilon: 0.4920666400108043    steps: 633    lr: 0.0001     evaluation reward: 6.36\n",
      "episode: 1577   score: 13.0   memory length: 357030   epsilon: 0.49107862001079805    steps: 499    lr: 0.0001     evaluation reward: 6.44\n",
      "episode: 1578   score: 7.0   memory length: 357449   epsilon: 0.4902490000107928    steps: 419    lr: 0.0001     evaluation reward: 6.45\n",
      "episode: 1579   score: 11.0   memory length: 357977   epsilon: 0.4892035600107862    steps: 528    lr: 0.0001     evaluation reward: 6.51\n",
      "episode: 1580   score: 5.0   memory length: 358251   epsilon: 0.48866104001078275    steps: 274    lr: 0.0001     evaluation reward: 6.52\n",
      "episode: 1581   score: 9.0   memory length: 358701   epsilon: 0.4877700400107771    steps: 450    lr: 0.0001     evaluation reward: 6.56\n",
      "episode: 1582   score: 4.0   memory length: 358945   epsilon: 0.48728692001077406    steps: 244    lr: 0.0001     evaluation reward: 6.49\n",
      "episode: 1583   score: 7.0   memory length: 359314   epsilon: 0.48655630001076944    steps: 369    lr: 0.0001     evaluation reward: 6.47\n",
      "episode: 1584   score: 7.0   memory length: 359741   epsilon: 0.4857108400107641    steps: 427    lr: 0.0001     evaluation reward: 6.49\n",
      "episode: 1585   score: 9.0   memory length: 360189   epsilon: 0.4848238000107585    steps: 448    lr: 0.0001     evaluation reward: 6.52\n",
      "episode: 1586   score: 5.0   memory length: 360461   epsilon: 0.48428524001075507    steps: 272    lr: 0.0001     evaluation reward: 6.51\n",
      "episode: 1587   score: 8.0   memory length: 360883   epsilon: 0.4834496800107498    steps: 422    lr: 0.0001     evaluation reward: 6.55\n",
      "episode: 1588   score: 6.0   memory length: 361217   epsilon: 0.4827883600107456    steps: 334    lr: 0.0001     evaluation reward: 6.47\n",
      "episode: 1589   score: 9.0   memory length: 361725   epsilon: 0.48178252001073923    steps: 508    lr: 0.0001     evaluation reward: 6.5\n",
      "episode: 1590   score: 4.0   memory length: 362003   epsilon: 0.48123208001073575    steps: 278    lr: 0.0001     evaluation reward: 6.5\n",
      "episode: 1591   score: 3.0   memory length: 362267   epsilon: 0.48070936001073244    steps: 264    lr: 0.0001     evaluation reward: 6.48\n",
      "episode: 1592   score: 6.0   memory length: 362596   epsilon: 0.4800579400107283    steps: 329    lr: 0.0001     evaluation reward: 6.46\n",
      "episode: 1593   score: 4.0   memory length: 362837   epsilon: 0.4795807600107253    steps: 241    lr: 0.0001     evaluation reward: 6.46\n",
      "episode: 1594   score: 7.0   memory length: 363244   epsilon: 0.4787749000107202    steps: 407    lr: 0.0001     evaluation reward: 6.49\n",
      "episode: 1595   score: 6.0   memory length: 363576   epsilon: 0.47811754001071605    steps: 332    lr: 0.0001     evaluation reward: 6.5\n",
      "episode: 1596   score: 5.0   memory length: 363885   epsilon: 0.4775057200107122    steps: 309    lr: 0.0001     evaluation reward: 6.52\n",
      "episode: 1597   score: 6.0   memory length: 364229   epsilon: 0.47682460001070787    steps: 344    lr: 0.0001     evaluation reward: 6.54\n",
      "episode: 1598   score: 3.0   memory length: 364439   epsilon: 0.47640880001070524    steps: 210    lr: 0.0001     evaluation reward: 6.53\n",
      "episode: 1599   score: 9.0   memory length: 364961   epsilon: 0.4753752400106987    steps: 522    lr: 0.0001     evaluation reward: 6.54\n",
      "episode: 1600   score: 9.0   memory length: 365439   epsilon: 0.4744288000106927    steps: 478    lr: 0.0001     evaluation reward: 6.61\n",
      "episode: 1601   score: 10.0   memory length: 365936   epsilon: 0.4734447400106865    steps: 497    lr: 0.0001     evaluation reward: 6.67\n",
      "episode: 1602   score: 9.0   memory length: 366371   epsilon: 0.47258344001068103    steps: 435    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 1603   score: 5.0   memory length: 366714   epsilon: 0.47190430001067674    steps: 343    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 1604   score: 10.0   memory length: 367171   epsilon: 0.470999440010671    steps: 457    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1605   score: 4.0   memory length: 367411   epsilon: 0.470524240010668    steps: 240    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 1606   score: 16.0   memory length: 368150   epsilon: 0.46906102001065875    steps: 739    lr: 0.0001     evaluation reward: 6.88\n",
      "episode: 1607   score: 5.0   memory length: 368459   epsilon: 0.4684492000106549    steps: 309    lr: 0.0001     evaluation reward: 6.78\n",
      "episode: 1608   score: 4.0   memory length: 368717   epsilon: 0.46793836001065164    steps: 258    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1609   score: 6.0   memory length: 369092   epsilon: 0.46719586001064695    steps: 375    lr: 0.0001     evaluation reward: 6.73\n",
      "episode: 1610   score: 7.0   memory length: 369473   epsilon: 0.4664414800106422    steps: 381    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1611   score: 9.0   memory length: 369980   epsilon: 0.4654376200106358    steps: 507    lr: 0.0001     evaluation reward: 6.8\n",
      "episode: 1612   score: 4.0   memory length: 370257   epsilon: 0.46488916001063235    steps: 277    lr: 0.0001     evaluation reward: 6.74\n",
      "episode: 1613   score: 5.0   memory length: 370601   epsilon: 0.46420804001062804    steps: 344    lr: 0.0001     evaluation reward: 6.75\n",
      "episode: 1614   score: 4.0   memory length: 370859   epsilon: 0.4636972000106248    steps: 258    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 1615   score: 7.0   memory length: 371264   epsilon: 0.46289530001061974    steps: 405    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 1616   score: 4.0   memory length: 371542   epsilon: 0.46234486001061625    steps: 278    lr: 0.0001     evaluation reward: 6.62\n",
      "episode: 1617   score: 9.0   memory length: 371982   epsilon: 0.46147366001061074    steps: 440    lr: 0.0001     evaluation reward: 6.62\n",
      "episode: 1618   score: 6.0   memory length: 372328   epsilon: 0.4607885800106064    steps: 346    lr: 0.0001     evaluation reward: 6.66\n",
      "episode: 1619   score: 7.0   memory length: 372717   epsilon: 0.46001836001060153    steps: 389    lr: 0.0001     evaluation reward: 6.65\n",
      "episode: 1620   score: 6.0   memory length: 373040   epsilon: 0.4593788200105975    steps: 323    lr: 0.0001     evaluation reward: 6.65\n",
      "episode: 1621   score: 14.0   memory length: 373681   epsilon: 0.45810964001058946    steps: 641    lr: 0.0001     evaluation reward: 6.72\n",
      "episode: 1622   score: 8.0   memory length: 374057   epsilon: 0.45736516001058475    steps: 376    lr: 0.0001     evaluation reward: 6.75\n",
      "episode: 1623   score: 5.0   memory length: 374345   epsilon: 0.45679492001058114    steps: 288    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 1624   score: 9.0   memory length: 374845   epsilon: 0.4558049200105749    steps: 500    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 1625   score: 7.0   memory length: 375252   epsilon: 0.4549990600105698    steps: 407    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 1626   score: 5.0   memory length: 375562   epsilon: 0.4543852600105659    steps: 310    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 1627   score: 12.0   memory length: 376123   epsilon: 0.45327448001055887    steps: 561    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 1628   score: 3.0   memory length: 376352   epsilon: 0.452821060010556    steps: 229    lr: 0.0001     evaluation reward: 6.79\n",
      "episode: 1629   score: 5.0   memory length: 376676   epsilon: 0.45217954001055194    steps: 324    lr: 0.0001     evaluation reward: 6.81\n",
      "episode: 1630   score: 9.0   memory length: 377123   epsilon: 0.45129448001054634    steps: 447    lr: 0.0001     evaluation reward: 6.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1631   score: 9.0   memory length: 377451   epsilon: 0.45064504001054223    steps: 328    lr: 0.0001     evaluation reward: 6.88\n",
      "episode: 1632   score: 5.0   memory length: 377793   epsilon: 0.44996788001053795    steps: 342    lr: 0.0001     evaluation reward: 6.89\n",
      "episode: 1633   score: 6.0   memory length: 378169   epsilon: 0.44922340001053324    steps: 376    lr: 0.0001     evaluation reward: 6.92\n",
      "episode: 1634   score: 6.0   memory length: 378525   epsilon: 0.4485185200105288    steps: 356    lr: 0.0001     evaluation reward: 6.93\n",
      "episode: 1635   score: 9.0   memory length: 379014   epsilon: 0.44755030001052265    steps: 489    lr: 0.0001     evaluation reward: 6.9\n",
      "episode: 1636   score: 6.0   memory length: 379352   epsilon: 0.4468810600105184    steps: 338    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 1637   score: 7.0   memory length: 379738   epsilon: 0.4461167800105136    steps: 386    lr: 0.0001     evaluation reward: 6.95\n",
      "episode: 1638   score: 2.0   memory length: 379936   epsilon: 0.4457247400105111    steps: 198    lr: 0.0001     evaluation reward: 6.9\n",
      "episode: 1639   score: 8.0   memory length: 380390   epsilon: 0.4448258200105054    steps: 454    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 1640   score: 9.0   memory length: 380879   epsilon: 0.4438576000104993    steps: 489    lr: 0.0001     evaluation reward: 6.99\n",
      "episode: 1641   score: 8.0   memory length: 381334   epsilon: 0.4429567000104936    steps: 455    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 1642   score: 7.0   memory length: 381719   epsilon: 0.44219440001048876    steps: 385    lr: 0.0001     evaluation reward: 7.09\n",
      "episode: 1643   score: 7.0   memory length: 382055   epsilon: 0.44152912001048455    steps: 336    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 1644   score: 8.0   memory length: 382508   epsilon: 0.4406321800104789    steps: 453    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 1645   score: 4.0   memory length: 382770   epsilon: 0.4401134200104756    steps: 262    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 1646   score: 6.0   memory length: 383137   epsilon: 0.439386760010471    steps: 367    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 1647   score: 6.0   memory length: 383479   epsilon: 0.4387096000104667    steps: 342    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 1648   score: 5.0   memory length: 383787   epsilon: 0.43809976001046286    steps: 308    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 1649   score: 15.0   memory length: 384512   epsilon: 0.4366642600104538    steps: 725    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 1650   score: 5.0   memory length: 384821   epsilon: 0.4360524400104499    steps: 309    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1651   score: 5.0   memory length: 385111   epsilon: 0.43547824001044627    steps: 290    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 1652   score: 9.0   memory length: 385453   epsilon: 0.434801080010442    steps: 342    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 1653   score: 4.0   memory length: 385698   epsilon: 0.4343159800104389    steps: 245    lr: 0.0001     evaluation reward: 7.19\n",
      "episode: 1654   score: 7.0   memory length: 386091   epsilon: 0.433537840010434    steps: 393    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1655   score: 3.0   memory length: 386316   epsilon: 0.4330923400104312    steps: 225    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 1656   score: 6.0   memory length: 386632   epsilon: 0.4324666600104272    steps: 316    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 1657   score: 6.0   memory length: 386994   epsilon: 0.4317499000104227    steps: 362    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 1658   score: 3.0   memory length: 387225   epsilon: 0.4312925200104198    steps: 231    lr: 0.0001     evaluation reward: 6.92\n",
      "episode: 1659   score: 7.0   memory length: 387597   epsilon: 0.43055596001041513    steps: 372    lr: 0.0001     evaluation reward: 6.93\n",
      "episode: 1660   score: 6.0   memory length: 387919   epsilon: 0.4299184000104111    steps: 322    lr: 0.0001     evaluation reward: 6.91\n",
      "episode: 1661   score: 7.0   memory length: 388326   epsilon: 0.429112540010406    steps: 407    lr: 0.0001     evaluation reward: 6.86\n",
      "episode: 1662   score: 7.0   memory length: 388705   epsilon: 0.42836212001040125    steps: 379    lr: 0.0001     evaluation reward: 6.88\n",
      "episode: 1663   score: 4.0   memory length: 388968   epsilon: 0.42784138001039795    steps: 263    lr: 0.0001     evaluation reward: 6.83\n",
      "episode: 1664   score: 6.0   memory length: 389360   epsilon: 0.42706522001039304    steps: 392    lr: 0.0001     evaluation reward: 6.83\n",
      "episode: 1665   score: 7.0   memory length: 389716   epsilon: 0.4263603400103886    steps: 356    lr: 0.0001     evaluation reward: 6.8\n",
      "episode: 1666   score: 4.0   memory length: 389974   epsilon: 0.42584950001038535    steps: 258    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1667   score: 9.0   memory length: 390280   epsilon: 0.4252436200103815    steps: 306    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 1668   score: 4.0   memory length: 390557   epsilon: 0.42469516001037805    steps: 277    lr: 0.0001     evaluation reward: 6.8\n",
      "episode: 1669   score: 5.0   memory length: 390845   epsilon: 0.42412492001037444    steps: 288    lr: 0.0001     evaluation reward: 6.79\n",
      "episode: 1670   score: 5.0   memory length: 391152   epsilon: 0.4235170600103706    steps: 307    lr: 0.0001     evaluation reward: 6.74\n",
      "episode: 1671   score: 9.0   memory length: 391574   epsilon: 0.4226815000103653    steps: 422    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1672   score: 6.0   memory length: 391914   epsilon: 0.42200830001036105    steps: 340    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1673   score: 8.0   memory length: 392290   epsilon: 0.42126382001035634    steps: 376    lr: 0.0001     evaluation reward: 6.81\n",
      "episode: 1674   score: 9.0   memory length: 392731   epsilon: 0.4203906400103508    steps: 441    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 1675   score: 12.0   memory length: 393308   epsilon: 0.4192481800103436    steps: 577    lr: 0.0001     evaluation reward: 6.85\n",
      "episode: 1676   score: 15.0   memory length: 393894   epsilon: 0.41808790001033624    steps: 586    lr: 0.0001     evaluation reward: 6.87\n",
      "episode: 1677   score: 9.0   memory length: 394375   epsilon: 0.4171355200103302    steps: 481    lr: 0.0001     evaluation reward: 6.83\n",
      "episode: 1678   score: 9.0   memory length: 394802   epsilon: 0.41629006001032487    steps: 427    lr: 0.0001     evaluation reward: 6.85\n",
      "episode: 1679   score: 8.0   memory length: 395255   epsilon: 0.4153931200103192    steps: 453    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 1680   score: 7.0   memory length: 395616   epsilon: 0.41467834001031467    steps: 361    lr: 0.0001     evaluation reward: 6.84\n",
      "episode: 1681   score: 21.0   memory length: 396418   epsilon: 0.4130903800103046    steps: 802    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 1682   score: 5.0   memory length: 396719   epsilon: 0.41249440001030085    steps: 301    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 1683   score: 4.0   memory length: 396963   epsilon: 0.4120112800102978    steps: 244    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 1684   score: 13.0   memory length: 397468   epsilon: 0.41101138001029147    steps: 505    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 1685   score: 11.0   memory length: 397996   epsilon: 0.40996594001028486    steps: 528    lr: 0.0001     evaluation reward: 7.02\n",
      "episode: 1686   score: 8.0   memory length: 398442   epsilon: 0.40908286001027927    steps: 446    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 1687   score: 5.0   memory length: 398749   epsilon: 0.4084750000102754    steps: 307    lr: 0.0001     evaluation reward: 7.02\n",
      "episode: 1688   score: 9.0   memory length: 399206   epsilon: 0.4075701400102697    steps: 457    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 1689   score: 6.0   memory length: 399549   epsilon: 0.4068910000102654    steps: 343    lr: 0.0001     evaluation reward: 7.02\n",
      "episode: 1690   score: 7.0   memory length: 399940   epsilon: 0.4061168200102605    steps: 391    lr: 0.0001     evaluation reward: 7.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1691   score: 4.0   memory length: 400202   epsilon: 0.4055980600102572    steps: 262    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 1692   score: 8.0   memory length: 400638   epsilon: 0.40473478001025176    steps: 436    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 1693   score: 13.0   memory length: 401236   epsilon: 0.40355074001024427    steps: 598    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1694   score: 11.0   memory length: 401838   epsilon: 0.4023587800102367    steps: 602    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 1695   score: 3.0   memory length: 402048   epsilon: 0.4019429800102341    steps: 210    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 1696   score: 4.0   memory length: 402340   epsilon: 0.40136482001023044    steps: 292    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1697   score: 5.0   memory length: 402646   epsilon: 0.4007589400102266    steps: 306    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 1698   score: 11.0   memory length: 403119   epsilon: 0.3998224000102207    steps: 473    lr: 0.0001     evaluation reward: 7.24\n",
      "episode: 1699   score: 10.0   memory length: 403615   epsilon: 0.39884032001021447    steps: 496    lr: 0.0001     evaluation reward: 7.25\n",
      "episode: 1700   score: 4.0   memory length: 403891   epsilon: 0.398293840010211    steps: 276    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 1701   score: 6.0   memory length: 404244   epsilon: 0.3975949000102066    steps: 353    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 1702   score: 10.0   memory length: 404728   epsilon: 0.3966365800102005    steps: 484    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1703   score: 9.0   memory length: 405201   epsilon: 0.3957000400101946    steps: 473    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 1704   score: 9.0   memory length: 405635   epsilon: 0.39484072001018916    steps: 434    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 1705   score: 8.0   memory length: 406061   epsilon: 0.3939972400101838    steps: 426    lr: 0.0001     evaluation reward: 7.24\n",
      "episode: 1706   score: 5.0   memory length: 406406   epsilon: 0.3933141400101795    steps: 345    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 1707   score: 17.0   memory length: 407030   epsilon: 0.3920786200101717    steps: 624    lr: 0.0001     evaluation reward: 7.25\n",
      "episode: 1708   score: 10.0   memory length: 407423   epsilon: 0.39130048001016676    steps: 393    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 1709   score: 9.0   memory length: 407946   epsilon: 0.3902649400101602    steps: 523    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 1710   score: 6.0   memory length: 408314   epsilon: 0.3895363000101556    steps: 368    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 1711   score: 3.0   memory length: 408545   epsilon: 0.3890789200101527    steps: 231    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 1712   score: 6.0   memory length: 408918   epsilon: 0.38834038001014803    steps: 373    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 1713   score: 12.0   memory length: 409518   epsilon: 0.3871523800101405    steps: 600    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 1714   score: 12.0   memory length: 409944   epsilon: 0.3863089000101352    steps: 426    lr: 0.0001     evaluation reward: 7.44\n",
      "episode: 1715   score: 6.0   memory length: 410263   epsilon: 0.3856772800101312    steps: 319    lr: 0.0001     evaluation reward: 7.43\n",
      "episode: 1716   score: 9.0   memory length: 410686   epsilon: 0.3848397400101259    steps: 423    lr: 0.0001     evaluation reward: 7.48\n",
      "episode: 1717   score: 11.0   memory length: 411089   epsilon: 0.38404180001012084    steps: 403    lr: 0.0001     evaluation reward: 7.5\n",
      "episode: 1718   score: 6.0   memory length: 411463   epsilon: 0.38330128001011615    steps: 374    lr: 0.0001     evaluation reward: 7.5\n",
      "episode: 1719   score: 8.0   memory length: 411865   epsilon: 0.3825053200101111    steps: 402    lr: 0.0001     evaluation reward: 7.51\n",
      "episode: 1720   score: 7.0   memory length: 412245   epsilon: 0.38175292001010636    steps: 380    lr: 0.0001     evaluation reward: 7.52\n",
      "episode: 1721   score: 9.0   memory length: 412709   epsilon: 0.38083420001010054    steps: 464    lr: 0.0001     evaluation reward: 7.47\n",
      "episode: 1722   score: 8.0   memory length: 413119   epsilon: 0.3800224000100954    steps: 410    lr: 0.0001     evaluation reward: 7.47\n",
      "episode: 1723   score: 5.0   memory length: 413407   epsilon: 0.3794521600100918    steps: 288    lr: 0.0001     evaluation reward: 7.47\n",
      "episode: 1724   score: 6.0   memory length: 413781   epsilon: 0.3787116400100871    steps: 374    lr: 0.0001     evaluation reward: 7.44\n",
      "episode: 1725   score: 7.0   memory length: 414184   epsilon: 0.37791370001008207    steps: 403    lr: 0.0001     evaluation reward: 7.44\n",
      "episode: 1726   score: 10.0   memory length: 414698   epsilon: 0.3768959800100756    steps: 514    lr: 0.0001     evaluation reward: 7.49\n",
      "episode: 1727   score: 4.0   memory length: 414960   epsilon: 0.37637722001007234    steps: 262    lr: 0.0001     evaluation reward: 7.41\n",
      "episode: 1728   score: 8.0   memory length: 415410   epsilon: 0.3754862200100667    steps: 450    lr: 0.0001     evaluation reward: 7.46\n",
      "episode: 1729   score: 6.0   memory length: 415798   epsilon: 0.37471798001006185    steps: 388    lr: 0.0001     evaluation reward: 7.47\n",
      "episode: 1730   score: 5.0   memory length: 416158   epsilon: 0.37400518001005734    steps: 360    lr: 0.0001     evaluation reward: 7.43\n",
      "episode: 1731   score: 4.0   memory length: 416438   epsilon: 0.37345078001005383    steps: 280    lr: 0.0001     evaluation reward: 7.38\n",
      "episode: 1732   score: 6.0   memory length: 416788   epsilon: 0.37275778001004944    steps: 350    lr: 0.0001     evaluation reward: 7.39\n",
      "episode: 1733   score: 7.0   memory length: 417196   epsilon: 0.37194994001004433    steps: 408    lr: 0.0001     evaluation reward: 7.4\n",
      "episode: 1734   score: 11.0   memory length: 417712   epsilon: 0.37092826001003787    steps: 516    lr: 0.0001     evaluation reward: 7.45\n",
      "episode: 1735   score: 10.0   memory length: 418185   epsilon: 0.36999172001003194    steps: 473    lr: 0.0001     evaluation reward: 7.46\n",
      "episode: 1736   score: 12.0   memory length: 418799   epsilon: 0.36877600001002425    steps: 614    lr: 0.0001     evaluation reward: 7.52\n",
      "episode: 1737   score: 6.0   memory length: 419172   epsilon: 0.3680374600100196    steps: 373    lr: 0.0001     evaluation reward: 7.51\n",
      "episode: 1738   score: 8.0   memory length: 419596   epsilon: 0.36719794001001427    steps: 424    lr: 0.0001     evaluation reward: 7.57\n",
      "episode: 1739   score: 12.0   memory length: 420208   epsilon: 0.3659861800100066    steps: 612    lr: 0.0001     evaluation reward: 7.61\n",
      "episode: 1740   score: 5.0   memory length: 420533   epsilon: 0.36534268001000253    steps: 325    lr: 0.0001     evaluation reward: 7.57\n",
      "episode: 1741   score: 6.0   memory length: 420886   epsilon: 0.3646437400099981    steps: 353    lr: 0.0001     evaluation reward: 7.55\n",
      "episode: 1742   score: 11.0   memory length: 421404   epsilon: 0.3636181000099916    steps: 518    lr: 0.0001     evaluation reward: 7.59\n",
      "episode: 1743   score: 8.0   memory length: 421835   epsilon: 0.3627647200099862    steps: 431    lr: 0.0001     evaluation reward: 7.6\n",
      "episode: 1744   score: 7.0   memory length: 422216   epsilon: 0.36201034000998145    steps: 381    lr: 0.0001     evaluation reward: 7.59\n",
      "episode: 1745   score: 7.0   memory length: 422577   epsilon: 0.3612955600099769    steps: 361    lr: 0.0001     evaluation reward: 7.62\n",
      "episode: 1746   score: 3.0   memory length: 422790   epsilon: 0.36087382000997426    steps: 213    lr: 0.0001     evaluation reward: 7.59\n",
      "episode: 1747   score: 5.0   memory length: 423081   epsilon: 0.3602976400099706    steps: 291    lr: 0.0001     evaluation reward: 7.58\n",
      "episode: 1748   score: 13.0   memory length: 423664   epsilon: 0.3591433000099633    steps: 583    lr: 0.0001     evaluation reward: 7.66\n",
      "episode: 1749   score: 8.0   memory length: 424125   epsilon: 0.35823052000995753    steps: 461    lr: 0.0001     evaluation reward: 7.59\n",
      "episode: 1750   score: 5.0   memory length: 424430   epsilon: 0.3576266200099537    steps: 305    lr: 0.0001     evaluation reward: 7.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1751   score: 7.0   memory length: 424816   epsilon: 0.3568623400099489    steps: 386    lr: 0.0001     evaluation reward: 7.61\n",
      "episode: 1752   score: 12.0   memory length: 425363   epsilon: 0.355779280009942    steps: 547    lr: 0.0001     evaluation reward: 7.64\n",
      "episode: 1753   score: 9.0   memory length: 425869   epsilon: 0.3547774000099357    steps: 506    lr: 0.0001     evaluation reward: 7.69\n",
      "episode: 1754   score: 9.0   memory length: 426336   epsilon: 0.35385274000992983    steps: 467    lr: 0.0001     evaluation reward: 7.71\n",
      "episode: 1755   score: 12.0   memory length: 426795   epsilon: 0.3529439200099241    steps: 459    lr: 0.0001     evaluation reward: 7.8\n",
      "episode: 1756   score: 4.0   memory length: 427035   epsilon: 0.3524687200099211    steps: 240    lr: 0.0001     evaluation reward: 7.78\n",
      "episode: 1757   score: 9.0   memory length: 427458   epsilon: 0.3516311800099158    steps: 423    lr: 0.0001     evaluation reward: 7.81\n",
      "episode: 1758   score: 16.0   memory length: 428072   epsilon: 0.3504154600099081    steps: 614    lr: 0.0001     evaluation reward: 7.94\n",
      "episode: 1759   score: 10.0   memory length: 428595   epsilon: 0.34937992000990153    steps: 523    lr: 0.0001     evaluation reward: 7.97\n",
      "episode: 1760   score: 8.0   memory length: 429031   epsilon: 0.3485166400098961    steps: 436    lr: 0.0001     evaluation reward: 7.99\n",
      "episode: 1761   score: 7.0   memory length: 429456   epsilon: 0.34767514000989075    steps: 425    lr: 0.0001     evaluation reward: 7.99\n",
      "episode: 1762   score: 7.0   memory length: 429830   epsilon: 0.34693462000988606    steps: 374    lr: 0.0001     evaluation reward: 7.99\n",
      "episode: 1763   score: 8.0   memory length: 430306   epsilon: 0.3459921400098801    steps: 476    lr: 0.0001     evaluation reward: 8.03\n",
      "episode: 1764   score: 11.0   memory length: 430821   epsilon: 0.34497244000987365    steps: 515    lr: 0.0001     evaluation reward: 8.08\n",
      "episode: 1765   score: 12.0   memory length: 431408   epsilon: 0.3438101800098663    steps: 587    lr: 0.0001     evaluation reward: 8.13\n",
      "episode: 1766   score: 12.0   memory length: 431976   epsilon: 0.3426855400098592    steps: 568    lr: 0.0001     evaluation reward: 8.21\n",
      "episode: 1767   score: 15.0   memory length: 432517   epsilon: 0.3416143600098524    steps: 541    lr: 0.0001     evaluation reward: 8.27\n",
      "episode: 1768   score: 6.0   memory length: 432893   epsilon: 0.3408698800098477    steps: 376    lr: 0.0001     evaluation reward: 8.29\n",
      "episode: 1769   score: 9.0   memory length: 433296   epsilon: 0.34007194000984264    steps: 403    lr: 0.0001     evaluation reward: 8.33\n",
      "episode: 1770   score: 6.0   memory length: 433670   epsilon: 0.33933142000983796    steps: 374    lr: 0.0001     evaluation reward: 8.34\n",
      "episode: 1771   score: 11.0   memory length: 434190   epsilon: 0.33830182000983144    steps: 520    lr: 0.0001     evaluation reward: 8.36\n",
      "episode: 1772   score: 7.0   memory length: 434582   epsilon: 0.33752566000982653    steps: 392    lr: 0.0001     evaluation reward: 8.37\n",
      "episode: 1773   score: 10.0   memory length: 435080   epsilon: 0.3365396200098203    steps: 498    lr: 0.0001     evaluation reward: 8.39\n",
      "episode: 1774   score: 10.0   memory length: 435575   epsilon: 0.3355595200098141    steps: 495    lr: 0.0001     evaluation reward: 8.4\n",
      "episode: 1775   score: 6.0   memory length: 435919   epsilon: 0.3348784000098098    steps: 344    lr: 0.0001     evaluation reward: 8.34\n",
      "episode: 1776   score: 12.0   memory length: 436456   epsilon: 0.33381514000980306    steps: 537    lr: 0.0001     evaluation reward: 8.31\n",
      "episode: 1777   score: 3.0   memory length: 436688   epsilon: 0.33335578000980015    steps: 232    lr: 0.0001     evaluation reward: 8.25\n",
      "episode: 1778   score: 8.0   memory length: 437126   epsilon: 0.33248854000979466    steps: 438    lr: 0.0001     evaluation reward: 8.24\n",
      "episode: 1779   score: 17.0   memory length: 437807   epsilon: 0.33114016000978613    steps: 681    lr: 0.0001     evaluation reward: 8.33\n",
      "episode: 1780   score: 8.0   memory length: 438246   epsilon: 0.33027094000978063    steps: 439    lr: 0.0001     evaluation reward: 8.34\n",
      "episode: 1781   score: 11.0   memory length: 438781   epsilon: 0.32921164000977393    steps: 535    lr: 0.0001     evaluation reward: 8.24\n",
      "episode: 1782   score: 6.0   memory length: 439140   epsilon: 0.32850082000976943    steps: 359    lr: 0.0001     evaluation reward: 8.25\n",
      "episode: 1783   score: 12.0   memory length: 439616   epsilon: 0.32755834000976347    steps: 476    lr: 0.0001     evaluation reward: 8.33\n",
      "episode: 1784   score: 11.0   memory length: 440121   epsilon: 0.32655844000975714    steps: 505    lr: 0.0001     evaluation reward: 8.31\n",
      "episode: 1785   score: 7.0   memory length: 440509   epsilon: 0.3257902000097523    steps: 388    lr: 0.0001     evaluation reward: 8.27\n",
      "episode: 1786   score: 3.0   memory length: 440757   epsilon: 0.3252991600097492    steps: 248    lr: 0.0001     evaluation reward: 8.22\n",
      "episode: 1787   score: 18.0   memory length: 441578   epsilon: 0.3236735800097389    steps: 821    lr: 0.0001     evaluation reward: 8.35\n",
      "episode: 1788   score: 7.0   memory length: 441978   epsilon: 0.3228815800097339    steps: 400    lr: 0.0001     evaluation reward: 8.33\n",
      "episode: 1789   score: 11.0   memory length: 442492   epsilon: 0.32186386000972744    steps: 514    lr: 0.0001     evaluation reward: 8.38\n",
      "episode: 1790   score: 8.0   memory length: 442934   epsilon: 0.3209887000097219    steps: 442    lr: 0.0001     evaluation reward: 8.39\n",
      "episode: 1791   score: 10.0   memory length: 443487   epsilon: 0.319893760009715    steps: 553    lr: 0.0001     evaluation reward: 8.45\n",
      "episode: 1792   score: 15.0   memory length: 444069   epsilon: 0.3187414000097077    steps: 582    lr: 0.0001     evaluation reward: 8.52\n",
      "episode: 1793   score: 12.0   memory length: 444641   epsilon: 0.3176088400097005    steps: 572    lr: 0.0001     evaluation reward: 8.51\n",
      "episode: 1794   score: 11.0   memory length: 445221   epsilon: 0.31646044000969326    steps: 580    lr: 0.0001     evaluation reward: 8.51\n",
      "episode: 1795   score: 12.0   memory length: 445823   epsilon: 0.3152684800096857    steps: 602    lr: 0.0001     evaluation reward: 8.6\n",
      "episode: 1796   score: 19.0   memory length: 446614   epsilon: 0.3137023000096758    steps: 791    lr: 0.0001     evaluation reward: 8.75\n",
      "episode: 1797   score: 6.0   memory length: 446958   epsilon: 0.3130211800096715    steps: 344    lr: 0.0001     evaluation reward: 8.76\n",
      "episode: 1798   score: 10.0   memory length: 447506   epsilon: 0.31193614000966463    steps: 548    lr: 0.0001     evaluation reward: 8.75\n",
      "episode: 1799   score: 10.0   memory length: 447970   epsilon: 0.3110174200096588    steps: 464    lr: 0.0001     evaluation reward: 8.75\n",
      "episode: 1800   score: 11.0   memory length: 448453   epsilon: 0.31006108000965277    steps: 483    lr: 0.0001     evaluation reward: 8.82\n",
      "episode: 1801   score: 13.0   memory length: 448956   epsilon: 0.30906514000964647    steps: 503    lr: 0.0001     evaluation reward: 8.89\n",
      "episode: 1802   score: 14.0   memory length: 449457   epsilon: 0.3080731600096402    steps: 501    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1803   score: 9.0   memory length: 449923   epsilon: 0.30715048000963435    steps: 466    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1804   score: 8.0   memory length: 450346   epsilon: 0.30631294000962905    steps: 423    lr: 0.0001     evaluation reward: 8.92\n",
      "episode: 1805   score: 14.0   memory length: 451014   epsilon: 0.3049903000096207    steps: 668    lr: 0.0001     evaluation reward: 8.98\n",
      "episode: 1806   score: 3.0   memory length: 451243   epsilon: 0.3045368800096178    steps: 229    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1807   score: 13.0   memory length: 451728   epsilon: 0.30357658000961174    steps: 485    lr: 0.0001     evaluation reward: 8.92\n",
      "episode: 1808   score: 11.0   memory length: 452212   epsilon: 0.3026182600096057    steps: 484    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1809   score: 9.0   memory length: 452673   epsilon: 0.3017054800095999    steps: 461    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1810   score: 13.0   memory length: 453176   epsilon: 0.3007095400095936    steps: 503    lr: 0.0001     evaluation reward: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1811   score: 13.0   memory length: 453670   epsilon: 0.2997314200095874    steps: 494    lr: 0.0001     evaluation reward: 9.1\n",
      "episode: 1812   score: 10.0   memory length: 454190   epsilon: 0.2987018200095809    steps: 520    lr: 0.0001     evaluation reward: 9.14\n",
      "episode: 1813   score: 11.0   memory length: 454720   epsilon: 0.29765242000957426    steps: 530    lr: 0.0001     evaluation reward: 9.13\n",
      "episode: 1814   score: 6.0   memory length: 455062   epsilon: 0.29697526000957    steps: 342    lr: 0.0001     evaluation reward: 9.07\n",
      "episode: 1815   score: 10.0   memory length: 455547   epsilon: 0.2960149600095639    steps: 485    lr: 0.0001     evaluation reward: 9.11\n",
      "episode: 1816   score: 10.0   memory length: 456096   epsilon: 0.294927940009557    steps: 549    lr: 0.0001     evaluation reward: 9.12\n",
      "episode: 1817   score: 8.0   memory length: 456484   epsilon: 0.29415970000955216    steps: 388    lr: 0.0001     evaluation reward: 9.09\n",
      "episode: 1818   score: 11.0   memory length: 457078   epsilon: 0.2929835800095447    steps: 594    lr: 0.0001     evaluation reward: 9.14\n",
      "episode: 1819   score: 6.0   memory length: 457472   epsilon: 0.2922034600095398    steps: 394    lr: 0.0001     evaluation reward: 9.12\n",
      "episode: 1820   score: 5.0   memory length: 457802   epsilon: 0.29155006000953565    steps: 330    lr: 0.0001     evaluation reward: 9.1\n",
      "episode: 1821   score: 13.0   memory length: 458437   epsilon: 0.2902927600095277    steps: 635    lr: 0.0001     evaluation reward: 9.14\n",
      "episode: 1822   score: 8.0   memory length: 458836   epsilon: 0.2895027400095227    steps: 399    lr: 0.0001     evaluation reward: 9.14\n",
      "episode: 1823   score: 9.0   memory length: 459298   epsilon: 0.2885879800095169    steps: 462    lr: 0.0001     evaluation reward: 9.18\n",
      "episode: 1824   score: 11.0   memory length: 459805   epsilon: 0.28758412000951056    steps: 507    lr: 0.0001     evaluation reward: 9.23\n",
      "episode: 1825   score: 9.0   memory length: 460114   epsilon: 0.2869723000095067    steps: 309    lr: 0.0001     evaluation reward: 9.25\n",
      "episode: 1826   score: 12.0   memory length: 460667   epsilon: 0.28587736000949976    steps: 553    lr: 0.0001     evaluation reward: 9.27\n",
      "episode: 1827   score: 9.0   memory length: 461120   epsilon: 0.2849804200094941    steps: 453    lr: 0.0001     evaluation reward: 9.32\n",
      "episode: 1828   score: 7.0   memory length: 461507   epsilon: 0.28421416000948924    steps: 387    lr: 0.0001     evaluation reward: 9.31\n",
      "episode: 1829   score: 9.0   memory length: 462013   epsilon: 0.2832122800094829    steps: 506    lr: 0.0001     evaluation reward: 9.34\n",
      "episode: 1830   score: 14.0   memory length: 462668   epsilon: 0.2819153800094747    steps: 655    lr: 0.0001     evaluation reward: 9.43\n",
      "episode: 1831   score: 7.0   memory length: 463058   epsilon: 0.2811431800094698    steps: 390    lr: 0.0001     evaluation reward: 9.46\n",
      "episode: 1832   score: 18.0   memory length: 463721   epsilon: 0.2798304400094615    steps: 663    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1833   score: 15.0   memory length: 464361   epsilon: 0.2785632400094535    steps: 640    lr: 0.0001     evaluation reward: 9.66\n",
      "episode: 1834   score: 13.0   memory length: 464963   epsilon: 0.27737128000944594    steps: 602    lr: 0.0001     evaluation reward: 9.68\n",
      "episode: 1835   score: 6.0   memory length: 465303   epsilon: 0.2766980800094417    steps: 340    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1836   score: 12.0   memory length: 465876   epsilon: 0.2755635400094345    steps: 573    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1837   score: 11.0   memory length: 466382   epsilon: 0.27456166000942817    steps: 506    lr: 0.0001     evaluation reward: 9.69\n",
      "episode: 1838   score: 12.0   memory length: 466967   epsilon: 0.27340336000942084    steps: 585    lr: 0.0001     evaluation reward: 9.73\n",
      "episode: 1839   score: 12.0   memory length: 467546   epsilon: 0.2722569400094136    steps: 579    lr: 0.0001     evaluation reward: 9.73\n",
      "episode: 1840   score: 9.0   memory length: 468038   epsilon: 0.2712827800094074    steps: 492    lr: 0.0001     evaluation reward: 9.77\n",
      "episode: 1841   score: 12.0   memory length: 468481   epsilon: 0.27040564000940187    steps: 443    lr: 0.0001     evaluation reward: 9.83\n",
      "episode: 1842   score: 13.0   memory length: 469052   epsilon: 0.2692750600093947    steps: 571    lr: 0.0001     evaluation reward: 9.85\n",
      "episode: 1843   score: 10.0   memory length: 469531   epsilon: 0.2683266400093887    steps: 479    lr: 0.0001     evaluation reward: 9.87\n",
      "episode: 1844   score: 15.0   memory length: 470223   epsilon: 0.26695648000938005    steps: 692    lr: 0.0001     evaluation reward: 9.95\n",
      "episode: 1845   score: 16.0   memory length: 470801   epsilon: 0.2658120400093728    steps: 578    lr: 0.0001     evaluation reward: 10.04\n",
      "episode: 1846   score: 6.0   memory length: 471154   epsilon: 0.2651131000093684    steps: 353    lr: 0.0001     evaluation reward: 10.07\n",
      "episode: 1847   score: 12.0   memory length: 471710   epsilon: 0.2640122200093614    steps: 556    lr: 0.0001     evaluation reward: 10.14\n",
      "episode: 1848   score: 9.0   memory length: 472161   epsilon: 0.26311924000935577    steps: 451    lr: 0.0001     evaluation reward: 10.1\n",
      "episode: 1849   score: 18.0   memory length: 472830   epsilon: 0.2617946200093474    steps: 669    lr: 0.0001     evaluation reward: 10.2\n",
      "episode: 1850   score: 8.0   memory length: 473269   epsilon: 0.2609254000093419    steps: 439    lr: 0.0001     evaluation reward: 10.23\n",
      "episode: 1851   score: 4.0   memory length: 473530   epsilon: 0.2604086200093386    steps: 261    lr: 0.0001     evaluation reward: 10.2\n",
      "episode: 1852   score: 10.0   memory length: 473907   epsilon: 0.2596621600093339    steps: 377    lr: 0.0001     evaluation reward: 10.18\n",
      "episode: 1853   score: 7.0   memory length: 474317   epsilon: 0.25885036000932876    steps: 410    lr: 0.0001     evaluation reward: 10.16\n",
      "episode: 1854   score: 8.0   memory length: 474794   epsilon: 0.2579059000093228    steps: 477    lr: 0.0001     evaluation reward: 10.15\n",
      "episode: 1855   score: 13.0   memory length: 475267   epsilon: 0.25696936000931686    steps: 473    lr: 0.0001     evaluation reward: 10.16\n",
      "episode: 1856   score: 12.0   memory length: 475806   epsilon: 0.2559021400093101    steps: 539    lr: 0.0001     evaluation reward: 10.24\n",
      "episode: 1857   score: 13.0   memory length: 476376   epsilon: 0.25477354000930297    steps: 570    lr: 0.0001     evaluation reward: 10.28\n",
      "episode: 1858   score: 13.0   memory length: 476954   epsilon: 0.2536291000092957    steps: 578    lr: 0.0001     evaluation reward: 10.25\n",
      "episode: 1859   score: 18.0   memory length: 477554   epsilon: 0.2524411000092882    steps: 600    lr: 0.0001     evaluation reward: 10.33\n",
      "episode: 1860   score: 9.0   memory length: 478024   epsilon: 0.2515105000092823    steps: 470    lr: 0.0001     evaluation reward: 10.34\n",
      "episode: 1861   score: 10.0   memory length: 478549   epsilon: 0.25047100000927575    steps: 525    lr: 0.0001     evaluation reward: 10.37\n",
      "episode: 1862   score: 10.0   memory length: 479032   epsilon: 0.2495146600092697    steps: 483    lr: 0.0001     evaluation reward: 10.4\n",
      "episode: 1863   score: 9.0   memory length: 479487   epsilon: 0.248613760009264    steps: 455    lr: 0.0001     evaluation reward: 10.41\n",
      "episode: 1864   score: 14.0   memory length: 480025   epsilon: 0.24754852000925726    steps: 538    lr: 0.0001     evaluation reward: 10.44\n",
      "episode: 1865   score: 13.0   memory length: 480622   epsilon: 0.24636646000924978    steps: 597    lr: 0.0001     evaluation reward: 10.45\n",
      "episode: 1866   score: 12.0   memory length: 481064   epsilon: 0.24549130000924424    steps: 442    lr: 0.0001     evaluation reward: 10.45\n",
      "episode: 1867   score: 14.0   memory length: 481707   epsilon: 0.24421816000923618    steps: 643    lr: 0.0001     evaluation reward: 10.44\n",
      "episode: 1868   score: 13.0   memory length: 482330   epsilon: 0.24298462000922838    steps: 623    lr: 0.0001     evaluation reward: 10.51\n",
      "episode: 1869   score: 9.0   memory length: 482801   epsilon: 0.24205204000922248    steps: 471    lr: 0.0001     evaluation reward: 10.51\n",
      "episode: 1870   score: 4.0   memory length: 483062   epsilon: 0.2415352600092192    steps: 261    lr: 0.0001     evaluation reward: 10.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1871   score: 7.0   memory length: 483466   epsilon: 0.24073534000921415    steps: 404    lr: 0.0001     evaluation reward: 10.45\n",
      "episode: 1872   score: 10.0   memory length: 483953   epsilon: 0.23977108000920805    steps: 487    lr: 0.0001     evaluation reward: 10.48\n",
      "episode: 1873   score: 10.0   memory length: 484457   epsilon: 0.23877316000920173    steps: 504    lr: 0.0001     evaluation reward: 10.48\n",
      "episode: 1874   score: 10.0   memory length: 484967   epsilon: 0.23776336000919535    steps: 510    lr: 0.0001     evaluation reward: 10.48\n",
      "episode: 1875   score: 10.0   memory length: 485400   epsilon: 0.23690602000918992    steps: 433    lr: 0.0001     evaluation reward: 10.52\n",
      "episode: 1876   score: 15.0   memory length: 486007   epsilon: 0.23570416000918232    steps: 607    lr: 0.0001     evaluation reward: 10.55\n",
      "episode: 1877   score: 9.0   memory length: 486447   epsilon: 0.2348329600091768    steps: 440    lr: 0.0001     evaluation reward: 10.61\n",
      "episode: 1878   score: 10.0   memory length: 486957   epsilon: 0.23382316000917042    steps: 510    lr: 0.0001     evaluation reward: 10.63\n",
      "episode: 1879   score: 18.0   memory length: 487740   epsilon: 0.2322728200091606    steps: 783    lr: 0.0001     evaluation reward: 10.64\n",
      "episode: 1880   score: 12.0   memory length: 488298   epsilon: 0.23116798000915362    steps: 558    lr: 0.0001     evaluation reward: 10.68\n",
      "episode: 1881   score: 9.0   memory length: 488765   epsilon: 0.23024332000914777    steps: 467    lr: 0.0001     evaluation reward: 10.66\n",
      "episode: 1882   score: 7.0   memory length: 489136   epsilon: 0.22950874000914312    steps: 371    lr: 0.0001     evaluation reward: 10.67\n",
      "episode: 1883   score: 7.0   memory length: 489584   epsilon: 0.2286217000091375    steps: 448    lr: 0.0001     evaluation reward: 10.62\n",
      "episode: 1884   score: 6.0   memory length: 489961   epsilon: 0.22787524000913278    steps: 377    lr: 0.0001     evaluation reward: 10.57\n",
      "episode: 1885   score: 13.0   memory length: 490544   epsilon: 0.22672090000912548    steps: 583    lr: 0.0001     evaluation reward: 10.63\n",
      "episode: 1886   score: 10.0   memory length: 491004   epsilon: 0.22581010000911972    steps: 460    lr: 0.0001     evaluation reward: 10.7\n",
      "episode: 1887   score: 6.0   memory length: 491344   epsilon: 0.22513690000911546    steps: 340    lr: 0.0001     evaluation reward: 10.58\n",
      "episode: 1888   score: 13.0   memory length: 491910   epsilon: 0.22401622000910837    steps: 566    lr: 0.0001     evaluation reward: 10.64\n",
      "episode: 1889   score: 9.0   memory length: 492419   epsilon: 0.223008400009102    steps: 509    lr: 0.0001     evaluation reward: 10.62\n",
      "episode: 1890   score: 17.0   memory length: 492936   epsilon: 0.22198474000909552    steps: 517    lr: 0.0001     evaluation reward: 10.71\n",
      "episode: 1891   score: 11.0   memory length: 493478   epsilon: 0.22091158000908873    steps: 542    lr: 0.0001     evaluation reward: 10.72\n",
      "episode: 1892   score: 6.0   memory length: 493840   epsilon: 0.2201948200090842    steps: 362    lr: 0.0001     evaluation reward: 10.63\n",
      "episode: 1893   score: 9.0   memory length: 494334   epsilon: 0.219216700009078    steps: 494    lr: 0.0001     evaluation reward: 10.6\n",
      "episode: 1894   score: 14.0   memory length: 494934   epsilon: 0.2180287000090705    steps: 600    lr: 0.0001     evaluation reward: 10.63\n",
      "episode: 1895   score: 18.0   memory length: 495459   epsilon: 0.2169892000090639    steps: 525    lr: 0.0001     evaluation reward: 10.69\n",
      "episode: 1896   score: 19.0   memory length: 496163   epsilon: 0.2155952800090551    steps: 704    lr: 0.0001     evaluation reward: 10.69\n",
      "episode: 1897   score: 12.0   memory length: 496648   epsilon: 0.21463498000904901    steps: 485    lr: 0.0001     evaluation reward: 10.75\n",
      "episode: 1898   score: 8.0   memory length: 497099   epsilon: 0.21374200000904336    steps: 451    lr: 0.0001     evaluation reward: 10.73\n",
      "episode: 1899   score: 6.0   memory length: 497461   epsilon: 0.21302524000903883    steps: 362    lr: 0.0001     evaluation reward: 10.69\n",
      "episode: 1900   score: 16.0   memory length: 497983   epsilon: 0.2119916800090323    steps: 522    lr: 0.0001     evaluation reward: 10.74\n",
      "episode: 1901   score: 20.0   memory length: 498744   epsilon: 0.21048490000902276    steps: 761    lr: 0.0001     evaluation reward: 10.81\n",
      "episode: 1902   score: 8.0   memory length: 499185   epsilon: 0.20961172000901723    steps: 441    lr: 0.0001     evaluation reward: 10.75\n",
      "episode: 1903   score: 12.0   memory length: 499759   epsilon: 0.20847520000901004    steps: 574    lr: 0.0001     evaluation reward: 10.78\n",
      "episode: 1904   score: 10.0   memory length: 500283   epsilon: 0.20743768000900348    steps: 524    lr: 0.0001     evaluation reward: 10.8\n",
      "episode: 1905   score: 21.0   memory length: 501092   epsilon: 0.20583586000899334    steps: 809    lr: 0.0001     evaluation reward: 10.87\n",
      "episode: 1906   score: 7.0   memory length: 501480   epsilon: 0.20506762000898848    steps: 388    lr: 0.0001     evaluation reward: 10.91\n",
      "episode: 1907   score: 11.0   memory length: 501993   epsilon: 0.20405188000898206    steps: 513    lr: 0.0001     evaluation reward: 10.89\n",
      "episode: 1908   score: 6.0   memory length: 502394   epsilon: 0.20325790000897703    steps: 401    lr: 0.0001     evaluation reward: 10.84\n",
      "episode: 1909   score: 7.0   memory length: 502764   epsilon: 0.2025253000089724    steps: 370    lr: 0.0001     evaluation reward: 10.82\n",
      "episode: 1910   score: 15.0   memory length: 503409   epsilon: 0.20124820000896432    steps: 645    lr: 0.0001     evaluation reward: 10.84\n",
      "episode: 1911   score: 9.0   memory length: 503916   epsilon: 0.20024434000895797    steps: 507    lr: 0.0001     evaluation reward: 10.8\n",
      "episode: 1912   score: 3.0   memory length: 504128   epsilon: 0.1998245800089553    steps: 212    lr: 0.0001     evaluation reward: 10.73\n",
      "episode: 1913   score: 7.0   memory length: 504484   epsilon: 0.19911970000895085    steps: 356    lr: 0.0001     evaluation reward: 10.69\n",
      "episode: 1914   score: 11.0   memory length: 504991   epsilon: 0.1981158400089445    steps: 507    lr: 0.0001     evaluation reward: 10.74\n",
      "episode: 1915   score: 11.0   memory length: 505382   epsilon: 0.1973416600089396    steps: 391    lr: 0.0001     evaluation reward: 10.75\n",
      "episode: 1916   score: 9.0   memory length: 505834   epsilon: 0.19644670000893394    steps: 452    lr: 0.0001     evaluation reward: 10.74\n",
      "episode: 1917   score: 22.0   memory length: 506611   epsilon: 0.1949082400089242    steps: 777    lr: 0.0001     evaluation reward: 10.88\n",
      "episode: 1918   score: 18.0   memory length: 507284   epsilon: 0.19357570000891577    steps: 673    lr: 0.0001     evaluation reward: 10.95\n",
      "episode: 1919   score: 14.0   memory length: 507907   epsilon: 0.19234216000890797    steps: 623    lr: 0.0001     evaluation reward: 11.03\n",
      "episode: 1920   score: 10.0   memory length: 508406   epsilon: 0.19135414000890172    steps: 499    lr: 0.0001     evaluation reward: 11.08\n",
      "episode: 1921   score: 9.0   memory length: 508907   epsilon: 0.19036216000889544    steps: 501    lr: 0.0001     evaluation reward: 11.04\n",
      "episode: 1922   score: 9.0   memory length: 509341   epsilon: 0.18950284000889    steps: 434    lr: 0.0001     evaluation reward: 11.05\n",
      "episode: 1923   score: 8.0   memory length: 509784   epsilon: 0.18862570000888446    steps: 443    lr: 0.0001     evaluation reward: 11.04\n",
      "episode: 1924   score: 23.0   memory length: 510489   epsilon: 0.18722980000887562    steps: 705    lr: 0.0001     evaluation reward: 11.16\n",
      "episode: 1925   score: 4.0   memory length: 510732   epsilon: 0.18674866000887258    steps: 243    lr: 0.0001     evaluation reward: 11.11\n",
      "episode: 1926   score: 15.0   memory length: 511274   epsilon: 0.1856755000088658    steps: 542    lr: 0.0001     evaluation reward: 11.14\n",
      "episode: 1927   score: 9.0   memory length: 511760   epsilon: 0.1847132200088597    steps: 486    lr: 0.0001     evaluation reward: 11.14\n",
      "episode: 1928   score: 15.0   memory length: 512430   epsilon: 0.1833866200088513    steps: 670    lr: 0.0001     evaluation reward: 11.22\n",
      "episode: 1929   score: 14.0   memory length: 512911   epsilon: 0.18243424000884528    steps: 481    lr: 0.0001     evaluation reward: 11.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1930   score: 6.0   memory length: 513249   epsilon: 0.18176500000884105    steps: 338    lr: 0.0001     evaluation reward: 11.19\n",
      "episode: 1931   score: 7.0   memory length: 513652   epsilon: 0.180967060008836    steps: 403    lr: 0.0001     evaluation reward: 11.19\n",
      "episode: 1932   score: 14.0   memory length: 514177   epsilon: 0.17992756000882942    steps: 525    lr: 0.0001     evaluation reward: 11.15\n",
      "episode: 1933   score: 11.0   memory length: 514734   epsilon: 0.17882470000882245    steps: 557    lr: 0.0001     evaluation reward: 11.11\n",
      "episode: 1934   score: 17.0   memory length: 515364   epsilon: 0.17757730000881455    steps: 630    lr: 0.0001     evaluation reward: 11.15\n",
      "episode: 1935   score: 11.0   memory length: 515807   epsilon: 0.176700160008809    steps: 443    lr: 0.0001     evaluation reward: 11.2\n",
      "episode: 1936   score: 19.0   memory length: 516394   epsilon: 0.17553790000880165    steps: 587    lr: 0.0001     evaluation reward: 11.27\n",
      "episode: 1937   score: 22.0   memory length: 517051   epsilon: 0.17423704000879342    steps: 657    lr: 0.0001     evaluation reward: 11.38\n",
      "episode: 1938   score: 9.0   memory length: 517498   epsilon: 0.17335198000878782    steps: 447    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 1939   score: 13.0   memory length: 518069   epsilon: 0.17222140000878067    steps: 571    lr: 0.0001     evaluation reward: 11.36\n",
      "episode: 1940   score: 8.0   memory length: 518500   epsilon: 0.17136802000877527    steps: 431    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 1941   score: 3.0   memory length: 518747   epsilon: 0.17087896000877217    steps: 247    lr: 0.0001     evaluation reward: 11.26\n",
      "episode: 1942   score: 7.0   memory length: 519137   epsilon: 0.1701067600087673    steps: 390    lr: 0.0001     evaluation reward: 11.2\n",
      "episode: 1943   score: 5.0   memory length: 519447   epsilon: 0.1694929600087634    steps: 310    lr: 0.0001     evaluation reward: 11.15\n",
      "episode: 1944   score: 10.0   memory length: 519843   epsilon: 0.16870888000875844    steps: 396    lr: 0.0001     evaluation reward: 11.1\n",
      "episode: 1945   score: 11.0   memory length: 520371   epsilon: 0.16766344000875183    steps: 528    lr: 0.0001     evaluation reward: 11.05\n",
      "episode: 1946   score: 12.0   memory length: 520960   epsilon: 0.16649722000874445    steps: 589    lr: 0.0001     evaluation reward: 11.11\n",
      "episode: 1947   score: 11.0   memory length: 521523   epsilon: 0.1653824800087374    steps: 563    lr: 0.0001     evaluation reward: 11.1\n",
      "episode: 1948   score: 27.0   memory length: 522357   epsilon: 0.16373116000872695    steps: 834    lr: 0.0001     evaluation reward: 11.28\n",
      "episode: 1949   score: 12.0   memory length: 522815   epsilon: 0.1628243200087212    steps: 458    lr: 0.0001     evaluation reward: 11.22\n",
      "episode: 1950   score: 14.0   memory length: 523440   epsilon: 0.16158682000871338    steps: 625    lr: 0.0001     evaluation reward: 11.28\n",
      "episode: 1951   score: 6.0   memory length: 523800   epsilon: 0.16087402000870887    steps: 360    lr: 0.0001     evaluation reward: 11.3\n",
      "episode: 1952   score: 7.0   memory length: 524211   epsilon: 0.16006024000870372    steps: 411    lr: 0.0001     evaluation reward: 11.27\n",
      "episode: 1953   score: 9.0   memory length: 524701   epsilon: 0.1590900400086976    steps: 490    lr: 0.0001     evaluation reward: 11.29\n",
      "episode: 1954   score: 18.0   memory length: 525246   epsilon: 0.15801094000869076    steps: 545    lr: 0.0001     evaluation reward: 11.39\n",
      "episode: 1955   score: 12.0   memory length: 525696   epsilon: 0.15711994000868512    steps: 450    lr: 0.0001     evaluation reward: 11.38\n",
      "episode: 1956   score: 9.0   memory length: 526160   epsilon: 0.1562012200086793    steps: 464    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 1957   score: 11.0   memory length: 526674   epsilon: 0.15518350000867287    steps: 514    lr: 0.0001     evaluation reward: 11.33\n",
      "episode: 1958   score: 11.0   memory length: 527211   epsilon: 0.15412024000866614    steps: 537    lr: 0.0001     evaluation reward: 11.31\n",
      "episode: 1959   score: 9.0   memory length: 527655   epsilon: 0.15324112000866058    steps: 444    lr: 0.0001     evaluation reward: 11.22\n",
      "episode: 1960   score: 10.0   memory length: 528145   epsilon: 0.15227092000865444    steps: 490    lr: 0.0001     evaluation reward: 11.23\n",
      "episode: 1961   score: 14.0   memory length: 528581   epsilon: 0.15140764000864898    steps: 436    lr: 0.0001     evaluation reward: 11.27\n",
      "episode: 1962   score: 14.0   memory length: 529227   epsilon: 0.1501285600086409    steps: 646    lr: 0.0001     evaluation reward: 11.31\n",
      "episode: 1963   score: 10.0   memory length: 529725   epsilon: 0.14914252000863465    steps: 498    lr: 0.0001     evaluation reward: 11.32\n",
      "episode: 1964   score: 19.0   memory length: 530414   epsilon: 0.14777830000862602    steps: 689    lr: 0.0001     evaluation reward: 11.37\n",
      "episode: 1965   score: 10.0   memory length: 530920   epsilon: 0.14677642000861968    steps: 506    lr: 0.0001     evaluation reward: 11.34\n",
      "episode: 1966   score: 12.0   memory length: 531483   epsilon: 0.14566168000861263    steps: 563    lr: 0.0001     evaluation reward: 11.34\n",
      "episode: 1967   score: 15.0   memory length: 532041   epsilon: 0.14455684000860564    steps: 558    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 1968   score: 17.0   memory length: 532680   epsilon: 0.14329162000859763    steps: 639    lr: 0.0001     evaluation reward: 11.39\n",
      "episode: 1969   score: 21.0   memory length: 533329   epsilon: 0.1420066000085895    steps: 649    lr: 0.0001     evaluation reward: 11.51\n",
      "episode: 1970   score: 14.0   memory length: 533926   epsilon: 0.14082454000858202    steps: 597    lr: 0.0001     evaluation reward: 11.61\n",
      "episode: 1971   score: 8.0   memory length: 534367   epsilon: 0.1399513600085765    steps: 441    lr: 0.0001     evaluation reward: 11.62\n",
      "episode: 1972   score: 18.0   memory length: 535067   epsilon: 0.13856536000856773    steps: 700    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 1973   score: 11.0   memory length: 535589   epsilon: 0.1375318000085612    steps: 522    lr: 0.0001     evaluation reward: 11.71\n",
      "episode: 1974   score: 13.0   memory length: 536184   epsilon: 0.13635370000855374    steps: 595    lr: 0.0001     evaluation reward: 11.74\n",
      "episode: 1975   score: 11.0   memory length: 536713   epsilon: 0.1353062800085471    steps: 529    lr: 0.0001     evaluation reward: 11.75\n",
      "episode: 1976   score: 6.0   memory length: 537059   epsilon: 0.13462120000854277    steps: 346    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 1977   score: 6.0   memory length: 537417   epsilon: 0.1339123600085383    steps: 358    lr: 0.0001     evaluation reward: 11.63\n",
      "episode: 1978   score: 8.0   memory length: 537877   epsilon: 0.13300156000853253    steps: 460    lr: 0.0001     evaluation reward: 11.61\n",
      "episode: 1979   score: 6.0   memory length: 538272   epsilon: 0.13221946000852758    steps: 395    lr: 0.0001     evaluation reward: 11.49\n",
      "episode: 1980   score: 8.0   memory length: 538686   epsilon: 0.1313997400085224    steps: 414    lr: 0.0001     evaluation reward: 11.45\n",
      "episode: 1981   score: 10.0   memory length: 539167   epsilon: 0.13044736000851637    steps: 481    lr: 0.0001     evaluation reward: 11.46\n",
      "episode: 1982   score: 11.0   memory length: 539752   epsilon: 0.12928906000850904    steps: 585    lr: 0.0001     evaluation reward: 11.5\n",
      "episode: 1983   score: 13.0   memory length: 540390   epsilon: 0.12802582000850105    steps: 638    lr: 0.0001     evaluation reward: 11.56\n",
      "episode: 1984   score: 14.0   memory length: 540917   epsilon: 0.12698236000849444    steps: 527    lr: 0.0001     evaluation reward: 11.64\n",
      "episode: 1985   score: 9.0   memory length: 541365   epsilon: 0.12609532000848883    steps: 448    lr: 0.0001     evaluation reward: 11.6\n",
      "episode: 1986   score: 16.0   memory length: 542063   epsilon: 0.1247132800084821    steps: 698    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 1987   score: 9.0   memory length: 542516   epsilon: 0.12381634000848271    steps: 453    lr: 0.0001     evaluation reward: 11.69\n",
      "episode: 1988   score: 14.0   memory length: 543097   epsilon: 0.1226659600084835    steps: 581    lr: 0.0001     evaluation reward: 11.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1989   score: 10.0   memory length: 543597   epsilon: 0.12167596000848417    steps: 500    lr: 0.0001     evaluation reward: 11.71\n",
      "episode: 1990   score: 14.0   memory length: 544215   epsilon: 0.120452320008485    steps: 618    lr: 0.0001     evaluation reward: 11.68\n",
      "episode: 1991   score: 15.0   memory length: 544745   epsilon: 0.11940292000848572    steps: 530    lr: 0.0001     evaluation reward: 11.72\n",
      "episode: 1992   score: 6.0   memory length: 545101   epsilon: 0.1186980400084862    steps: 356    lr: 0.0001     evaluation reward: 11.72\n",
      "episode: 1993   score: 8.0   memory length: 545542   epsilon: 0.1178248600084868    steps: 441    lr: 0.0001     evaluation reward: 11.71\n",
      "episode: 1994   score: 10.0   memory length: 546060   epsilon: 0.1167992200084875    steps: 518    lr: 0.0001     evaluation reward: 11.67\n",
      "episode: 1995   score: 17.0   memory length: 546691   epsilon: 0.11554984000848835    steps: 631    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 1996   score: 9.0   memory length: 547131   epsilon: 0.11467864000848894    steps: 440    lr: 0.0001     evaluation reward: 11.56\n",
      "episode: 1997   score: 10.0   memory length: 547540   epsilon: 0.1138688200084895    steps: 409    lr: 0.0001     evaluation reward: 11.54\n",
      "episode: 1998   score: 14.0   memory length: 548188   epsilon: 0.11258578000849037    steps: 648    lr: 0.0001     evaluation reward: 11.6\n",
      "episode: 1999   score: 8.0   memory length: 548640   epsilon: 0.11169082000849098    steps: 452    lr: 0.0001     evaluation reward: 11.62\n",
      "episode: 2000   score: 17.0   memory length: 549316   epsilon: 0.1103523400084919    steps: 676    lr: 0.0001     evaluation reward: 11.63\n",
      "episode: 2001   score: 22.0   memory length: 549860   epsilon: 0.10927522000849263    steps: 544    lr: 0.0001     evaluation reward: 11.65\n",
      "episode: 2002   score: 16.0   memory length: 550457   epsilon: 0.10809316000849344    steps: 597    lr: 0.0001     evaluation reward: 11.73\n",
      "episode: 2003   score: 11.0   memory length: 550871   epsilon: 0.107273440008494    steps: 414    lr: 0.0001     evaluation reward: 11.72\n",
      "episode: 2004   score: 9.0   memory length: 551315   epsilon: 0.1063943200084946    steps: 444    lr: 0.0001     evaluation reward: 11.71\n",
      "episode: 2005   score: 14.0   memory length: 551839   epsilon: 0.1053568000084953    steps: 524    lr: 0.0001     evaluation reward: 11.64\n",
      "episode: 2006   score: 11.0   memory length: 552333   epsilon: 0.10437868000849597    steps: 494    lr: 0.0001     evaluation reward: 11.68\n",
      "episode: 2007   score: 13.0   memory length: 552830   epsilon: 0.10339462000849664    steps: 497    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 2008   score: 6.0   memory length: 553152   epsilon: 0.10275706000849708    steps: 322    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 2009   score: 17.0   memory length: 553810   epsilon: 0.10145422000849796    steps: 658    lr: 0.0001     evaluation reward: 11.8\n",
      "episode: 2010   score: 12.0   memory length: 554362   epsilon: 0.10036126000849871    steps: 552    lr: 0.0001     evaluation reward: 11.77\n",
      "episode: 2011   score: 8.0   memory length: 554752   epsilon: 0.09958906000849924    steps: 390    lr: 0.0001     evaluation reward: 11.76\n",
      "episode: 2012   score: 19.0   memory length: 555466   epsilon: 0.0981753400085002    steps: 714    lr: 0.0001     evaluation reward: 11.92\n",
      "episode: 2013   score: 15.0   memory length: 555993   epsilon: 0.09713188000850091    steps: 527    lr: 0.0001     evaluation reward: 12.0\n",
      "episode: 2014   score: 16.0   memory length: 556622   epsilon: 0.09588646000850176    steps: 629    lr: 0.0001     evaluation reward: 12.05\n",
      "episode: 2015   score: 14.0   memory length: 557236   epsilon: 0.09467074000850259    steps: 614    lr: 0.0001     evaluation reward: 12.08\n",
      "episode: 2016   score: 11.0   memory length: 557793   epsilon: 0.09356788000850334    steps: 557    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 2017   score: 8.0   memory length: 558200   epsilon: 0.09276202000850389    steps: 407    lr: 0.0001     evaluation reward: 11.96\n",
      "episode: 2018   score: 10.0   memory length: 558652   epsilon: 0.0918670600085045    steps: 452    lr: 0.0001     evaluation reward: 11.88\n",
      "episode: 2019   score: 15.0   memory length: 559325   epsilon: 0.09053452000850541    steps: 673    lr: 0.0001     evaluation reward: 11.89\n",
      "episode: 2020   score: 8.0   memory length: 559744   epsilon: 0.08970490000850598    steps: 419    lr: 0.0001     evaluation reward: 11.87\n",
      "episode: 2021   score: 10.0   memory length: 560278   epsilon: 0.0886475800085067    steps: 534    lr: 0.0001     evaluation reward: 11.88\n",
      "episode: 2022   score: 17.0   memory length: 560879   epsilon: 0.08745760000850751    steps: 601    lr: 0.0001     evaluation reward: 11.96\n",
      "episode: 2023   score: 12.0   memory length: 561469   epsilon: 0.08628940000850831    steps: 590    lr: 0.0001     evaluation reward: 12.0\n",
      "episode: 2024   score: 9.0   memory length: 561893   epsilon: 0.08544988000850888    steps: 424    lr: 0.0001     evaluation reward: 11.86\n",
      "episode: 2025   score: 16.0   memory length: 562585   epsilon: 0.08407972000850981    steps: 692    lr: 0.0001     evaluation reward: 11.98\n",
      "episode: 2026   score: 25.0   memory length: 563266   epsilon: 0.08273134000851073    steps: 681    lr: 0.0001     evaluation reward: 12.08\n",
      "episode: 2027   score: 12.0   memory length: 563695   epsilon: 0.08188192000851131    steps: 429    lr: 0.0001     evaluation reward: 12.11\n",
      "episode: 2028   score: 11.0   memory length: 564230   epsilon: 0.08082262000851204    steps: 535    lr: 0.0001     evaluation reward: 12.07\n",
      "episode: 2029   score: 12.0   memory length: 564652   epsilon: 0.0799870600085126    steps: 422    lr: 0.0001     evaluation reward: 12.05\n",
      "episode: 2030   score: 18.0   memory length: 565316   epsilon: 0.0786723400085135    steps: 664    lr: 0.0001     evaluation reward: 12.17\n",
      "episode: 2031   score: 14.0   memory length: 565923   epsilon: 0.07747048000851432    steps: 607    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 2032   score: 17.0   memory length: 566576   epsilon: 0.0761775400085152    steps: 653    lr: 0.0001     evaluation reward: 12.27\n",
      "episode: 2033   score: 16.0   memory length: 567056   epsilon: 0.07522714000851585    steps: 480    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2034   score: 12.0   memory length: 567562   epsilon: 0.07422526000851654    steps: 506    lr: 0.0001     evaluation reward: 12.27\n",
      "episode: 2035   score: 8.0   memory length: 567983   epsilon: 0.0733916800085171    steps: 421    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 2036   score: 13.0   memory length: 568590   epsilon: 0.07218982000851792    steps: 607    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2037   score: 17.0   memory length: 569196   epsilon: 0.07098994000851874    steps: 606    lr: 0.0001     evaluation reward: 12.13\n",
      "episode: 2038   score: 14.0   memory length: 569884   epsilon: 0.06962770000851967    steps: 688    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2039   score: 9.0   memory length: 570372   epsilon: 0.06866146000852033    steps: 488    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 2040   score: 24.0   memory length: 571156   epsilon: 0.06710914000852139    steps: 784    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 2041   score: 13.0   memory length: 571745   epsilon: 0.06594292000852219    steps: 589    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2042   score: 9.0   memory length: 572183   epsilon: 0.06507568000852278    steps: 438    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2043   score: 10.0   memory length: 572669   epsilon: 0.06411340000852343    steps: 486    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2044   score: 14.0   memory length: 573320   epsilon: 0.06282442000852431    steps: 651    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2045   score: 24.0   memory length: 574021   epsilon: 0.06143644000852526    steps: 701    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2046   score: 10.0   memory length: 574492   epsilon: 0.060503860008525895    steps: 471    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2047   score: 16.0   memory length: 575178   epsilon: 0.05914558000852682    steps: 686    lr: 0.0001     evaluation reward: 12.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2048   score: 16.0   memory length: 575754   epsilon: 0.0580051000085276    steps: 576    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2049   score: 13.0   memory length: 576394   epsilon: 0.056737900008528463    steps: 640    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2050   score: 14.0   memory length: 576903   epsilon: 0.05573008000852915    steps: 509    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2051   score: 13.0   memory length: 577480   epsilon: 0.05458762000852993    steps: 577    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2052   score: 8.0   memory length: 577850   epsilon: 0.05385502000853043    steps: 370    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2053   score: 12.0   memory length: 578381   epsilon: 0.05280364000853115    steps: 531    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2054   score: 5.0   memory length: 578693   epsilon: 0.05218588000853157    steps: 312    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2055   score: 7.0   memory length: 579066   epsilon: 0.05144734000853207    steps: 373    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2056   score: 11.0   memory length: 579587   epsilon: 0.050415760008532776    steps: 521    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2057   score: 6.0   memory length: 579929   epsilon: 0.04973860000853324    steps: 342    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2058   score: 12.0   memory length: 580461   epsilon: 0.048685240008533956    steps: 532    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2059   score: 6.0   memory length: 580758   epsilon: 0.04809718000853436    steps: 297    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2060   score: 7.0   memory length: 581107   epsilon: 0.04740616000853483    steps: 349    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2061   score: 16.0   memory length: 581812   epsilon: 0.04601026000853578    steps: 705    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2062   score: 8.0   memory length: 582232   epsilon: 0.04517866000853635    steps: 420    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 2063   score: 16.0   memory length: 582804   epsilon: 0.04404610000853712    steps: 572    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2064   score: 13.0   memory length: 583318   epsilon: 0.043028380008537814    steps: 514    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 2065   score: 6.0   memory length: 583693   epsilon: 0.04228588000853832    steps: 375    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2066   score: 13.0   memory length: 584292   epsilon: 0.04109986000853913    steps: 599    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 2067   score: 9.0   memory length: 584780   epsilon: 0.04013362000853979    steps: 488    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 2068   score: 6.0   memory length: 585117   epsilon: 0.039466360008540244    steps: 337    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2069   score: 17.0   memory length: 585761   epsilon: 0.038191240008541114    steps: 644    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 2070   score: 10.0   memory length: 586247   epsilon: 0.03722896000854177    steps: 486    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 2071   score: 15.0   memory length: 586814   epsilon: 0.036106300008542536    steps: 567    lr: 0.0001     evaluation reward: 12.17\n",
      "episode: 2072   score: 11.0   memory length: 587385   epsilon: 0.03497572000854331    steps: 571    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 2073   score: 9.0   memory length: 587837   epsilon: 0.03408076000854392    steps: 452    lr: 0.0001     evaluation reward: 12.08\n",
      "episode: 2074   score: 23.0   memory length: 588557   epsilon: 0.03265516000854489    steps: 720    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2075   score: 15.0   memory length: 589122   epsilon: 0.03153646000854565    steps: 565    lr: 0.0001     evaluation reward: 12.22\n",
      "episode: 2076   score: 21.0   memory length: 589837   epsilon: 0.030120760008546618    steps: 715    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 2077   score: 12.0   memory length: 590435   epsilon: 0.028936720008547426    steps: 598    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2078   score: 20.0   memory length: 591141   epsilon: 0.02753884000854838    steps: 706    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2079   score: 19.0   memory length: 591810   epsilon: 0.026214220008549283    steps: 669    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2080   score: 12.0   memory length: 592382   epsilon: 0.025081660008550055    steps: 572    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2081   score: 18.0   memory length: 593099   epsilon: 0.023662000008551023    steps: 717    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2082   score: 13.0   memory length: 593588   epsilon: 0.022693780008551684    steps: 489    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 2083   score: 25.0   memory length: 594301   epsilon: 0.021282040008552647    steps: 713    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 2084   score: 10.0   memory length: 594807   epsilon: 0.02028016000855333    steps: 506    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2085   score: 10.0   memory length: 595341   epsilon: 0.01922284000855405    steps: 534    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 2086   score: 9.0   memory length: 595786   epsilon: 0.018341740008554652    steps: 445    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2087   score: 10.0   memory length: 596283   epsilon: 0.017357680008555323    steps: 497    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2088   score: 22.0   memory length: 596968   epsilon: 0.01600138000855625    steps: 685    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2089   score: 10.0   memory length: 597456   epsilon: 0.01503514000855639    steps: 488    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2090   score: 9.0   memory length: 597923   epsilon: 0.014110480008556211    steps: 467    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 2091   score: 5.0   memory length: 598271   epsilon: 0.013421440008556077    steps: 348    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2092   score: 15.0   memory length: 598837   epsilon: 0.01230076000855586    steps: 566    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 2093   score: 19.0   memory length: 599434   epsilon: 0.01111870000855563    steps: 597    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2094   score: 6.0   memory length: 599771   epsilon: 0.010451440008555501    steps: 337    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 2095   score: 10.0   memory length: 600213   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 2096   score: 15.0   memory length: 600779   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2097   score: 6.0   memory length: 601109   epsilon: 0.009998020008555413    steps: 330    lr: 0.0001     evaluation reward: 12.89\n",
      "episode: 2098   score: 8.0   memory length: 601552   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 2099   score: 5.0   memory length: 601839   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2100   score: 11.0   memory length: 602419   epsilon: 0.009998020008555413    steps: 580    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2101   score: 8.0   memory length: 602837   epsilon: 0.009998020008555413    steps: 418    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2102   score: 7.0   memory length: 603183   epsilon: 0.009998020008555413    steps: 346    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2103   score: 11.0   memory length: 603557   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2104   score: 12.0   memory length: 604146   epsilon: 0.009998020008555413    steps: 589    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2105   score: 21.0   memory length: 604756   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2106   score: 11.0   memory length: 605239   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 12.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2107   score: 17.0   memory length: 605847   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2108   score: 12.0   memory length: 606392   epsilon: 0.009998020008555413    steps: 545    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2109   score: 8.0   memory length: 606797   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2110   score: 7.0   memory length: 607172   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2111   score: 15.0   memory length: 607702   epsilon: 0.009998020008555413    steps: 530    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2112   score: 16.0   memory length: 608327   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2113   score: 11.0   memory length: 608871   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2114   score: 10.0   memory length: 609363   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2115   score: 23.0   memory length: 610112   epsilon: 0.009998020008555413    steps: 749    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2116   score: 12.0   memory length: 610720   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2117   score: 11.0   memory length: 611277   epsilon: 0.009998020008555413    steps: 557    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2118   score: 21.0   memory length: 612044   epsilon: 0.009998020008555413    steps: 767    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2119   score: 14.0   memory length: 612582   epsilon: 0.009998020008555413    steps: 538    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2120   score: 8.0   memory length: 612994   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2121   score: 15.0   memory length: 613674   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 2122   score: 9.0   memory length: 614102   epsilon: 0.009998020008555413    steps: 428    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2123   score: 19.0   memory length: 614767   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2124   score: 13.0   memory length: 615377   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 2125   score: 10.0   memory length: 615885   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2126   score: 5.0   memory length: 616197   epsilon: 0.009998020008555413    steps: 312    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2127   score: 16.0   memory length: 616814   epsilon: 0.009998020008555413    steps: 617    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2128   score: 13.0   memory length: 617140   epsilon: 0.009998020008555413    steps: 326    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2129   score: 21.0   memory length: 617819   epsilon: 0.009998020008555413    steps: 679    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2130   score: 11.0   memory length: 618391   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2131   score: 13.0   memory length: 618962   epsilon: 0.009998020008555413    steps: 571    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2132   score: 9.0   memory length: 619432   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2133   score: 14.0   memory length: 620019   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2134   score: 13.0   memory length: 620659   epsilon: 0.009998020008555413    steps: 640    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2135   score: 13.0   memory length: 621280   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2136   score: 12.0   memory length: 621908   epsilon: 0.009998020008555413    steps: 628    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2137   score: 34.0   memory length: 622675   epsilon: 0.009998020008555413    steps: 767    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2138   score: 15.0   memory length: 623361   epsilon: 0.009998020008555413    steps: 686    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2139   score: 7.0   memory length: 623740   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2140   score: 28.0   memory length: 624580   epsilon: 0.009998020008555413    steps: 840    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2141   score: 8.0   memory length: 625039   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2142   score: 12.0   memory length: 625648   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2143   score: 14.0   memory length: 626285   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2144   score: 6.0   memory length: 626625   epsilon: 0.009998020008555413    steps: 340    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2145   score: 15.0   memory length: 627340   epsilon: 0.009998020008555413    steps: 715    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2146   score: 9.0   memory length: 627845   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2147   score: 10.0   memory length: 628323   epsilon: 0.009998020008555413    steps: 478    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2148   score: 13.0   memory length: 628807   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2149   score: 10.0   memory length: 629219   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2150   score: 18.0   memory length: 629928   epsilon: 0.009998020008555413    steps: 709    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2151   score: 12.0   memory length: 630479   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2152   score: 8.0   memory length: 630894   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2153   score: 14.0   memory length: 631531   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2154   score: 15.0   memory length: 632133   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2155   score: 7.0   memory length: 632544   epsilon: 0.009998020008555413    steps: 411    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2156   score: 18.0   memory length: 633185   epsilon: 0.009998020008555413    steps: 641    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2157   score: 11.0   memory length: 633695   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2158   score: 11.0   memory length: 634239   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2159   score: 15.0   memory length: 634759   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2160   score: 11.0   memory length: 635309   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 12.89\n",
      "episode: 2161   score: 20.0   memory length: 635986   epsilon: 0.009998020008555413    steps: 677    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2162   score: 19.0   memory length: 636536   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 2163   score: 11.0   memory length: 637036   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 2164   score: 10.0   memory length: 637539   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 2165   score: 20.0   memory length: 638250   epsilon: 0.009998020008555413    steps: 711    lr: 0.0001     evaluation reward: 13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2166   score: 11.0   memory length: 638750   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2167   score: 18.0   memory length: 639379   epsilon: 0.009998020008555413    steps: 629    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2168   score: 10.0   memory length: 639844   epsilon: 0.009998020008555413    steps: 465    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2169   score: 5.0   memory length: 640155   epsilon: 0.009998020008555413    steps: 311    lr: 0.0001     evaluation reward: 13.09\n",
      "episode: 2170   score: 9.0   memory length: 640610   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2171   score: 8.0   memory length: 641057   epsilon: 0.009998020008555413    steps: 447    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2172   score: 10.0   memory length: 641506   epsilon: 0.009998020008555413    steps: 449    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 2173   score: 10.0   memory length: 642013   epsilon: 0.009998020008555413    steps: 507    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2174   score: 12.0   memory length: 642436   epsilon: 0.009998020008555413    steps: 423    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2175   score: 10.0   memory length: 642920   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2176   score: 14.0   memory length: 643448   epsilon: 0.009998020008555413    steps: 528    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2177   score: 8.0   memory length: 643834   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2178   score: 8.0   memory length: 644273   epsilon: 0.009998020008555413    steps: 439    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2179   score: 13.0   memory length: 644907   epsilon: 0.009998020008555413    steps: 634    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2180   score: 19.0   memory length: 645497   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2181   score: 10.0   memory length: 645985   epsilon: 0.009998020008555413    steps: 488    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2182   score: 23.0   memory length: 646685   epsilon: 0.009998020008555413    steps: 700    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2183   score: 9.0   memory length: 647130   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2184   score: 12.0   memory length: 647535   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2185   score: 7.0   memory length: 647928   epsilon: 0.009998020008555413    steps: 393    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2186   score: 12.0   memory length: 648480   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2187   score: 10.0   memory length: 648962   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2188   score: 5.0   memory length: 649291   epsilon: 0.009998020008555413    steps: 329    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2189   score: 9.0   memory length: 649759   epsilon: 0.009998020008555413    steps: 468    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 2190   score: 16.0   memory length: 650457   epsilon: 0.009998020008555413    steps: 698    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2191   score: 10.0   memory length: 650942   epsilon: 0.009998020008555413    steps: 485    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2192   score: 13.0   memory length: 651561   epsilon: 0.009998020008555413    steps: 619    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2193   score: 13.0   memory length: 652164   epsilon: 0.009998020008555413    steps: 603    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 2194   score: 10.0   memory length: 652661   epsilon: 0.009998020008555413    steps: 497    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2195   score: 16.0   memory length: 653362   epsilon: 0.009998020008555413    steps: 701    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2196   score: 14.0   memory length: 653872   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2197   score: 6.0   memory length: 654247   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2198   score: 13.0   memory length: 654833   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2199   score: 14.0   memory length: 655403   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2200   score: 9.0   memory length: 655878   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2201   score: 16.0   memory length: 656460   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2202   score: 3.0   memory length: 656724   epsilon: 0.009998020008555413    steps: 264    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2203   score: 7.0   memory length: 657133   epsilon: 0.009998020008555413    steps: 409    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2204   score: 8.0   memory length: 657558   epsilon: 0.009998020008555413    steps: 425    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2205   score: 14.0   memory length: 658188   epsilon: 0.009998020008555413    steps: 630    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2206   score: 12.0   memory length: 658755   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2207   score: 8.0   memory length: 659192   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 2208   score: 14.0   memory length: 659777   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2209   score: 11.0   memory length: 660306   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2210   score: 15.0   memory length: 660890   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2211   score: 13.0   memory length: 661386   epsilon: 0.009998020008555413    steps: 496    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2212   score: 14.0   memory length: 662006   epsilon: 0.009998020008555413    steps: 620    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2213   score: 13.0   memory length: 662528   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2214   score: 24.0   memory length: 663479   epsilon: 0.009998020008555413    steps: 951    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2215   score: 12.0   memory length: 664056   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2216   score: 10.0   memory length: 664554   epsilon: 0.009998020008555413    steps: 498    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2217   score: 12.0   memory length: 665123   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2218   score: 18.0   memory length: 665802   epsilon: 0.009998020008555413    steps: 679    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2219   score: 16.0   memory length: 666458   epsilon: 0.009998020008555413    steps: 656    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2220   score: 10.0   memory length: 667014   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2221   score: 10.0   memory length: 667473   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2222   score: 15.0   memory length: 668185   epsilon: 0.009998020008555413    steps: 712    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2223   score: 14.0   memory length: 668876   epsilon: 0.009998020008555413    steps: 691    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2224   score: 13.0   memory length: 669479   epsilon: 0.009998020008555413    steps: 603    lr: 0.0001     evaluation reward: 12.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2225   score: 10.0   memory length: 669987   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2226   score: 8.0   memory length: 670384   epsilon: 0.009998020008555413    steps: 397    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2227   score: 13.0   memory length: 670927   epsilon: 0.009998020008555413    steps: 543    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2228   score: 10.0   memory length: 671397   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2229   score: 17.0   memory length: 672179   epsilon: 0.009998020008555413    steps: 782    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2230   score: 14.0   memory length: 672827   epsilon: 0.009998020008555413    steps: 648    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2231   score: 16.0   memory length: 673492   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2232   score: 9.0   memory length: 673963   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2233   score: 15.0   memory length: 674586   epsilon: 0.009998020008555413    steps: 623    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2234   score: 15.0   memory length: 675146   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2235   score: 10.0   memory length: 675666   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2236   score: 15.0   memory length: 676184   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2237   score: 8.0   memory length: 676621   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 12.25\n",
      "episode: 2238   score: 22.0   memory length: 677434   epsilon: 0.009998020008555413    steps: 813    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2239   score: 7.0   memory length: 677856   epsilon: 0.009998020008555413    steps: 422    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2240   score: 26.0   memory length: 678466   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 2241   score: 24.0   memory length: 679232   epsilon: 0.009998020008555413    steps: 766    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2242   score: 12.0   memory length: 679840   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2243   score: 17.0   memory length: 680481   epsilon: 0.009998020008555413    steps: 641    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2244   score: 13.0   memory length: 681096   epsilon: 0.009998020008555413    steps: 615    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2245   score: 11.0   memory length: 681638   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2246   score: 12.0   memory length: 682271   epsilon: 0.009998020008555413    steps: 633    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2247   score: 10.0   memory length: 682811   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2248   score: 19.0   memory length: 683534   epsilon: 0.009998020008555413    steps: 723    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2249   score: 8.0   memory length: 683994   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2250   score: 10.0   memory length: 684475   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2251   score: 11.0   memory length: 685021   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2252   score: 17.0   memory length: 685657   epsilon: 0.009998020008555413    steps: 636    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2253   score: 12.0   memory length: 686237   epsilon: 0.009998020008555413    steps: 580    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2254   score: 11.0   memory length: 686636   epsilon: 0.009998020008555413    steps: 399    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2255   score: 18.0   memory length: 687404   epsilon: 0.009998020008555413    steps: 768    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2256   score: 11.0   memory length: 687926   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2257   score: 13.0   memory length: 688537   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2258   score: 8.0   memory length: 688940   epsilon: 0.009998020008555413    steps: 403    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2259   score: 10.0   memory length: 689435   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2260   score: 11.0   memory length: 689951   epsilon: 0.009998020008555413    steps: 516    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2261   score: 11.0   memory length: 690445   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2262   score: 20.0   memory length: 691129   epsilon: 0.009998020008555413    steps: 684    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2263   score: 13.0   memory length: 691762   epsilon: 0.009998020008555413    steps: 633    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2264   score: 10.0   memory length: 692283   epsilon: 0.009998020008555413    steps: 521    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2265   score: 15.0   memory length: 692974   epsilon: 0.009998020008555413    steps: 691    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2266   score: 7.0   memory length: 693362   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 12.36\n",
      "episode: 2267   score: 16.0   memory length: 693913   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2268   score: 10.0   memory length: 694417   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2269   score: 11.0   memory length: 694935   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2270   score: 11.0   memory length: 695509   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2271   score: 12.0   memory length: 696079   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2272   score: 10.0   memory length: 696578   epsilon: 0.009998020008555413    steps: 499    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 2273   score: 21.0   memory length: 697225   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2274   score: 18.0   memory length: 697870   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2275   score: 11.0   memory length: 698412   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2276   score: 13.0   memory length: 699047   epsilon: 0.009998020008555413    steps: 635    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2277   score: 17.0   memory length: 699641   epsilon: 0.009998020008555413    steps: 594    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2278   score: 12.0   memory length: 700174   epsilon: 0.009998020008555413    steps: 533    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2279   score: 14.0   memory length: 700818   epsilon: 0.009998020008555413    steps: 644    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2280   score: 36.0   memory length: 701668   epsilon: 0.009998020008555413    steps: 850    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 2281   score: 9.0   memory length: 702163   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2282   score: 8.0   memory length: 702586   epsilon: 0.009998020008555413    steps: 423    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2283   score: 12.0   memory length: 703022   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 12.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2284   score: 12.0   memory length: 703609   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2285   score: 10.0   memory length: 704106   epsilon: 0.009998020008555413    steps: 497    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2286   score: 12.0   memory length: 704642   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2287   score: 8.0   memory length: 705060   epsilon: 0.009998020008555413    steps: 418    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 2288   score: 13.0   memory length: 705642   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2289   score: 21.0   memory length: 706442   epsilon: 0.009998020008555413    steps: 800    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2290   score: 12.0   memory length: 707024   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2291   score: 17.0   memory length: 707670   epsilon: 0.009998020008555413    steps: 646    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2292   score: 10.0   memory length: 708178   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2293   score: 19.0   memory length: 708913   epsilon: 0.009998020008555413    steps: 735    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2294   score: 10.0   memory length: 709419   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2295   score: 7.0   memory length: 709799   epsilon: 0.009998020008555413    steps: 380    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 2296   score: 5.0   memory length: 710091   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2297   score: 8.0   memory length: 710513   epsilon: 0.009998020008555413    steps: 422    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2298   score: 22.0   memory length: 711139   epsilon: 0.009998020008555413    steps: 626    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2299   score: 16.0   memory length: 711765   epsilon: 0.009998020008555413    steps: 626    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2300   score: 13.0   memory length: 712257   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2301   score: 20.0   memory length: 712901   epsilon: 0.009998020008555413    steps: 644    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 2302   score: 18.0   memory length: 713465   epsilon: 0.009998020008555413    steps: 564    lr: 0.0001     evaluation reward: 13.26\n",
      "episode: 2303   score: 11.0   memory length: 713991   epsilon: 0.009998020008555413    steps: 526    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2304   score: 16.0   memory length: 714666   epsilon: 0.009998020008555413    steps: 675    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2305   score: 17.0   memory length: 715388   epsilon: 0.009998020008555413    steps: 722    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2306   score: 15.0   memory length: 715919   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2307   score: 8.0   memory length: 716318   epsilon: 0.009998020008555413    steps: 399    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2308   score: 9.0   memory length: 716748   epsilon: 0.009998020008555413    steps: 430    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2309   score: 10.0   memory length: 717265   epsilon: 0.009998020008555413    steps: 517    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2310   score: 10.0   memory length: 717737   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2311   score: 14.0   memory length: 718246   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2312   score: 10.0   memory length: 718768   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2313   score: 13.0   memory length: 719359   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2314   score: 15.0   memory length: 720062   epsilon: 0.009998020008555413    steps: 703    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2315   score: 8.0   memory length: 720469   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2316   score: 15.0   memory length: 721132   epsilon: 0.009998020008555413    steps: 663    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2317   score: 19.0   memory length: 721703   epsilon: 0.009998020008555413    steps: 571    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2318   score: 9.0   memory length: 722158   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2319   score: 11.0   memory length: 722655   epsilon: 0.009998020008555413    steps: 497    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2320   score: 15.0   memory length: 723189   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2321   score: 12.0   memory length: 723836   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2322   score: 8.0   memory length: 724238   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2323   score: 10.0   memory length: 724711   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 2324   score: 18.0   memory length: 725397   epsilon: 0.009998020008555413    steps: 686    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2325   score: 11.0   memory length: 725930   epsilon: 0.009998020008555413    steps: 533    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2326   score: 8.0   memory length: 726355   epsilon: 0.009998020008555413    steps: 425    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2327   score: 20.0   memory length: 727082   epsilon: 0.009998020008555413    steps: 727    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 2328   score: 16.0   memory length: 727772   epsilon: 0.009998020008555413    steps: 690    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2329   score: 10.0   memory length: 728226   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 13.23\n",
      "episode: 2330   score: 16.0   memory length: 728816   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2331   score: 10.0   memory length: 729299   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2332   score: 12.0   memory length: 729866   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2333   score: 11.0   memory length: 730393   epsilon: 0.009998020008555413    steps: 527    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2334   score: 7.0   memory length: 730788   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2335   score: 19.0   memory length: 731481   epsilon: 0.009998020008555413    steps: 693    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2336   score: 14.0   memory length: 732045   epsilon: 0.009998020008555413    steps: 564    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2337   score: 15.0   memory length: 732537   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2338   score: 15.0   memory length: 733042   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2339   score: 23.0   memory length: 733729   epsilon: 0.009998020008555413    steps: 687    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2340   score: 9.0   memory length: 734144   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2341   score: 21.0   memory length: 734659   epsilon: 0.009998020008555413    steps: 515    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2342   score: 14.0   memory length: 735218   epsilon: 0.009998020008555413    steps: 559    lr: 0.0001     evaluation reward: 13.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2343   score: 20.0   memory length: 735980   epsilon: 0.009998020008555413    steps: 762    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2344   score: 10.0   memory length: 736445   epsilon: 0.009998020008555413    steps: 465    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2345   score: 12.0   memory length: 737046   epsilon: 0.009998020008555413    steps: 601    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2346   score: 12.0   memory length: 737665   epsilon: 0.009998020008555413    steps: 619    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2347   score: 22.0   memory length: 738292   epsilon: 0.009998020008555413    steps: 627    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2348   score: 17.0   memory length: 739004   epsilon: 0.009998020008555413    steps: 712    lr: 0.0001     evaluation reward: 13.27\n",
      "episode: 2349   score: 14.0   memory length: 739536   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2350   score: 13.0   memory length: 740173   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 13.36\n",
      "episode: 2351   score: 16.0   memory length: 740613   epsilon: 0.009998020008555413    steps: 440    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2352   score: 9.0   memory length: 741080   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2353   score: 10.0   memory length: 741580   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.31\n",
      "episode: 2354   score: 10.0   memory length: 742071   epsilon: 0.009998020008555413    steps: 491    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2355   score: 6.0   memory length: 742426   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2356   score: 16.0   memory length: 742996   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 13.23\n",
      "episode: 2357   score: 7.0   memory length: 743378   epsilon: 0.009998020008555413    steps: 382    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2358   score: 11.0   memory length: 743903   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2359   score: 14.0   memory length: 744521   epsilon: 0.009998020008555413    steps: 618    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 2360   score: 9.0   memory length: 745016   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2361   score: 13.0   memory length: 745588   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 2362   score: 7.0   memory length: 745920   epsilon: 0.009998020008555413    steps: 332    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 2363   score: 10.0   memory length: 746397   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2364   score: 10.0   memory length: 746838   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2365   score: 13.0   memory length: 747408   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 13.06\n",
      "episode: 2366   score: 17.0   memory length: 748029   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2367   score: 8.0   memory length: 748439   epsilon: 0.009998020008555413    steps: 410    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2368   score: 14.0   memory length: 748940   epsilon: 0.009998020008555413    steps: 501    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2369   score: 14.0   memory length: 749536   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2370   score: 10.0   memory length: 750022   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2371   score: 18.0   memory length: 750532   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2372   score: 6.0   memory length: 750927   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2373   score: 14.0   memory length: 751502   epsilon: 0.009998020008555413    steps: 575    lr: 0.0001     evaluation reward: 13.09\n",
      "episode: 2374   score: 16.0   memory length: 752110   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2375   score: 9.0   memory length: 752553   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2376   score: 13.0   memory length: 753178   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2377   score: 10.0   memory length: 753650   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2378   score: 19.0   memory length: 754259   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2379   score: 12.0   memory length: 754864   epsilon: 0.009998020008555413    steps: 605    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2380   score: 11.0   memory length: 755416   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2381   score: 7.0   memory length: 755785   epsilon: 0.009998020008555413    steps: 369    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2382   score: 11.0   memory length: 756289   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 2383   score: 9.0   memory length: 756730   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2384   score: 11.0   memory length: 757275   epsilon: 0.009998020008555413    steps: 545    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2385   score: 11.0   memory length: 757681   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2386   score: 12.0   memory length: 758269   epsilon: 0.009998020008555413    steps: 588    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2387   score: 13.0   memory length: 758892   epsilon: 0.009998020008555413    steps: 623    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2388   score: 13.0   memory length: 759471   epsilon: 0.009998020008555413    steps: 579    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2389   score: 11.0   memory length: 760012   epsilon: 0.009998020008555413    steps: 541    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2390   score: 11.0   memory length: 760492   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 12.7\n",
      "episode: 2391   score: 12.0   memory length: 761068   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2392   score: 16.0   memory length: 761659   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2393   score: 10.0   memory length: 762166   epsilon: 0.009998020008555413    steps: 507    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2394   score: 17.0   memory length: 762830   epsilon: 0.009998020008555413    steps: 664    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2395   score: 21.0   memory length: 763622   epsilon: 0.009998020008555413    steps: 792    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 2396   score: 9.0   memory length: 764080   epsilon: 0.009998020008555413    steps: 458    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 2397   score: 24.0   memory length: 764871   epsilon: 0.009998020008555413    steps: 791    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2398   score: 17.0   memory length: 765595   epsilon: 0.009998020008555413    steps: 724    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2399   score: 13.0   memory length: 766216   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 12.95\n",
      "episode: 2400   score: 10.0   memory length: 766540   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2401   score: 9.0   memory length: 767013   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 12.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2402   score: 10.0   memory length: 767475   epsilon: 0.009998020008555413    steps: 462    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2403   score: 9.0   memory length: 767947   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2404   score: 22.0   memory length: 768735   epsilon: 0.009998020008555413    steps: 788    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2405   score: 12.0   memory length: 769290   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2406   score: 11.0   memory length: 769787   epsilon: 0.009998020008555413    steps: 497    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2407   score: 24.0   memory length: 770507   epsilon: 0.009998020008555413    steps: 720    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2408   score: 26.0   memory length: 771226   epsilon: 0.009998020008555413    steps: 719    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2409   score: 8.0   memory length: 771716   epsilon: 0.009998020008555413    steps: 490    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 2410   score: 2.0   memory length: 771897   epsilon: 0.009998020008555413    steps: 181    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 2411   score: 14.0   memory length: 772533   epsilon: 0.009998020008555413    steps: 636    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 2412   score: 11.0   memory length: 773070   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2413   score: 19.0   memory length: 773802   epsilon: 0.009998020008555413    steps: 732    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2414   score: 18.0   memory length: 774489   epsilon: 0.009998020008555413    steps: 687    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2415   score: 24.0   memory length: 775227   epsilon: 0.009998020008555413    steps: 738    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2416   score: 13.0   memory length: 775736   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2417   score: 16.0   memory length: 776344   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2418   score: 25.0   memory length: 776951   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 2419   score: 19.0   memory length: 777683   epsilon: 0.009998020008555413    steps: 732    lr: 0.0001     evaluation reward: 13.36\n",
      "episode: 2420   score: 12.0   memory length: 778251   epsilon: 0.009998020008555413    steps: 568    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2421   score: 13.0   memory length: 778785   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2422   score: 17.0   memory length: 779384   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 2423   score: 14.0   memory length: 779964   epsilon: 0.009998020008555413    steps: 580    lr: 0.0001     evaluation reward: 13.47\n",
      "episode: 2424   score: 25.0   memory length: 780765   epsilon: 0.009998020008555413    steps: 801    lr: 0.0001     evaluation reward: 13.54\n",
      "episode: 2425   score: 13.0   memory length: 781354   epsilon: 0.009998020008555413    steps: 589    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2426   score: 17.0   memory length: 781921   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2427   score: 14.0   memory length: 782391   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2428   score: 17.0   memory length: 783073   epsilon: 0.009998020008555413    steps: 682    lr: 0.0001     evaluation reward: 13.6\n",
      "episode: 2429   score: 13.0   memory length: 783657   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2430   score: 12.0   memory length: 784205   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2431   score: 9.0   memory length: 784678   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2432   score: 17.0   memory length: 785267   epsilon: 0.009998020008555413    steps: 589    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2433   score: 12.0   memory length: 785852   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2434   score: 10.0   memory length: 786303   epsilon: 0.009998020008555413    steps: 451    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2435   score: 16.0   memory length: 786942   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2436   score: 13.0   memory length: 787556   epsilon: 0.009998020008555413    steps: 614    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2437   score: 10.0   memory length: 788042   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2438   score: 7.0   memory length: 788436   epsilon: 0.009998020008555413    steps: 394    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 2439   score: 15.0   memory length: 789008   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 2440   score: 9.0   memory length: 789462   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 2441   score: 23.0   memory length: 790113   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2442   score: 14.0   memory length: 790772   epsilon: 0.009998020008555413    steps: 659    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2443   score: 11.0   memory length: 791306   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 2444   score: 16.0   memory length: 791932   epsilon: 0.009998020008555413    steps: 626    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2445   score: 11.0   memory length: 792473   epsilon: 0.009998020008555413    steps: 541    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2446   score: 6.0   memory length: 792841   epsilon: 0.009998020008555413    steps: 368    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2447   score: 10.0   memory length: 793308   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2448   score: 11.0   memory length: 793724   epsilon: 0.009998020008555413    steps: 416    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2449   score: 12.0   memory length: 794212   epsilon: 0.009998020008555413    steps: 488    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2450   score: 13.0   memory length: 794726   epsilon: 0.009998020008555413    steps: 514    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2451   score: 9.0   memory length: 795162   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2452   score: 15.0   memory length: 795737   epsilon: 0.009998020008555413    steps: 575    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2453   score: 7.0   memory length: 796151   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2454   score: 14.0   memory length: 796716   epsilon: 0.009998020008555413    steps: 565    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2455   score: 14.0   memory length: 797418   epsilon: 0.009998020008555413    steps: 702    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2456   score: 19.0   memory length: 798043   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2457   score: 11.0   memory length: 798573   epsilon: 0.009998020008555413    steps: 530    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2458   score: 20.0   memory length: 799306   epsilon: 0.009998020008555413    steps: 733    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2459   score: 17.0   memory length: 799802   epsilon: 0.009998020008555413    steps: 496    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2460   score: 17.0   memory length: 800412   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 13.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2461   score: 17.0   memory length: 801064   epsilon: 0.009998020008555413    steps: 652    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2462   score: 18.0   memory length: 801665   epsilon: 0.009998020008555413    steps: 601    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2463   score: 9.0   memory length: 802085   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2464   score: 12.0   memory length: 802633   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2465   score: 15.0   memory length: 803172   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2466   score: 16.0   memory length: 803881   epsilon: 0.009998020008555413    steps: 709    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 2467   score: 14.0   memory length: 804458   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2468   score: 9.0   memory length: 804952   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2469   score: 25.0   memory length: 805742   epsilon: 0.009998020008555413    steps: 790    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2470   score: 12.0   memory length: 806224   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 13.8\n",
      "episode: 2471   score: 10.0   memory length: 806699   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2472   score: 5.0   memory length: 806991   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2473   score: 26.0   memory length: 807932   epsilon: 0.009998020008555413    steps: 941    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2474   score: 21.0   memory length: 808763   epsilon: 0.009998020008555413    steps: 831    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2475   score: 19.0   memory length: 809471   epsilon: 0.009998020008555413    steps: 708    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2476   score: 22.0   memory length: 810216   epsilon: 0.009998020008555413    steps: 745    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 2477   score: 13.0   memory length: 810690   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 14.1\n",
      "episode: 2478   score: 7.0   memory length: 811059   epsilon: 0.009998020008555413    steps: 369    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2479   score: 11.0   memory length: 811430   epsilon: 0.009998020008555413    steps: 371    lr: 0.0001     evaluation reward: 13.97\n",
      "episode: 2480   score: 8.0   memory length: 811746   epsilon: 0.009998020008555413    steps: 316    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2481   score: 13.0   memory length: 812351   epsilon: 0.009998020008555413    steps: 605    lr: 0.0001     evaluation reward: 14.0\n",
      "episode: 2482   score: 18.0   memory length: 812915   epsilon: 0.009998020008555413    steps: 564    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 2483   score: 23.0   memory length: 813638   epsilon: 0.009998020008555413    steps: 723    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2484   score: 13.0   memory length: 814212   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2485   score: 13.0   memory length: 814860   epsilon: 0.009998020008555413    steps: 648    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2486   score: 9.0   memory length: 815305   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2487   score: 10.0   memory length: 815673   epsilon: 0.009998020008555413    steps: 368    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2488   score: 15.0   memory length: 816197   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2489   score: 7.0   memory length: 816600   epsilon: 0.009998020008555413    steps: 403    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2490   score: 12.0   memory length: 817149   epsilon: 0.009998020008555413    steps: 549    lr: 0.0001     evaluation reward: 14.18\n",
      "episode: 2491   score: 17.0   memory length: 817616   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2492   score: 14.0   memory length: 818102   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2493   score: 10.0   memory length: 818649   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2494   score: 8.0   memory length: 819055   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 14.12\n",
      "episode: 2495   score: 12.0   memory length: 819499   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 14.03\n",
      "episode: 2496   score: 6.0   memory length: 819866   epsilon: 0.009998020008555413    steps: 367    lr: 0.0001     evaluation reward: 14.0\n",
      "episode: 2497   score: 8.0   memory length: 820294   epsilon: 0.009998020008555413    steps: 428    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2498   score: 14.0   memory length: 820780   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 13.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assignment5/agent_double.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd6klEQVR4nO3de5QkZZnn8e+vupumoemGhoKB5tKgDI7rBbBkVVhEEAVUYB0ZEF1BUcazzoJzxmVBxxXPzK46u96d0UGucvUMIqLgCMso6IhAAQ02NAz3u3Q1177SXVXP/hGRU9FJZlZWVUZEZsbvc06eioyIjPd9M7uffPOJN95QRGBmZtUxUHYFzMysWA78ZmYV48BvZlYxDvxmZhXjwG9mVjEO/GZmFePAb11H0s8lndDhY54p6aJOHrNKJJ0v6W/Lrod1hgO/5ULSI5LWSVqdeXynnddGxOERcUHedewGkpZIisx79Iik08uul/W32WVXwPra+yLi/5VdiR6xdUSMShoCbpB0W0RcV0ZFJM2KiLEyyrZiuMdvhZN0oqR/lfRtSS9KulfSIZntv5L08XT51ZJuSPdbKemHmf3eJunWdNutkt6W2bZ7+rpVkq4Dtqurw1sk/VbSC5LulHRQXf0eSl/7sKQPNWjDTukvmkWZdfukdZzTqt6tRMQwcDewd+a4H5O0XNLzkn4habd0/RclfTtdniNpjaS/S5/Pk7Re0jbp83+S9Ie0PjdK+g+Z458v6buSrpG0BnhH2pbb0/fgh8Dm7dTfeoMDv5XlPwIPkQTkLwBXZINoxt8A1wLbADsDtUC3CLga+BawLfA14GpJ26avuwS4LT3+3wD/fs5A0uL0tX8LLAI+A/xI0qCkLdNjHh4RWwFvA5bWVyoingJuAv40s/p44PKI2Nis3pOR9BbgdcAD6fOjgc8C7wcGgV8Dl6a73wAclC6/GfgD8Pb0+VuB+yLi+fT5z4E9ge2B24GL64o+HvhfwFbALcCVwIUk788/1bXTepwDv+XpyrRHXXt8IrNtBfCNiNgYET8E7gPe0+AYG4HdgJ0iYn1E/CZd/x7g/oi4MCJGI+JS4F7gfZJ2JQmEn4+IlyPiRuCnmWN+GLgmIq6JiPE0pTIMHJFuHwdeJ2leRDwdEXc3ad8lwAcBJAk4Ll3Xqt7NrJS0juTL5B9IAi/AnwNfiojlETEK/G9g77TXfxOwZ/pldyBwDrBY0nySL4AbagePiHMjYlVEvAycCbxR0sJM+T+JiH+NiHGSXxtzmPh8LgdunaT+1kMc+C1PR0fE1pnH9zPbnoxNZwh8FNipwTFOAwTcIuluSR9L1++UvibrUWBxuu35iFhTt61mN+CY7JcScACwY/qaY4FPAk9LulrSa5q073LgrZJ2Igm8QdIjb1XvZrYD5pP8+jiIJPDW6vrNTD2fS4+7OCLWkXxhvT0t/wbgt8D+ZAK/pFmSvizpQUkvAY9kyqx5PLO8E40/H+sTDvxWlsVpL7lmV+Cp+p0i4g8R8YmI2Imk9/sPkl6d7rtb3e67Ak8CTwPbpGmb7Laax4EL676UtoyIL6dl/iIiDgV2JPkVkf3CytbtBZJ0zp+RpEourQXLFvVuKiLGIuKrwHrgv2bq+ud1dZ0XEb9Nt98AHAzsQ9IrvwF4N7AfcGO6z/HAUcA7gYXAknR99v3PBvmnafz5WJ9w4LeybA+ckp6UPAb4E+Ca+p0kHSNp5/Tp8yQBaizd948lHS9ptqRjgdcCP4uIR0l6wl+UtJmkA4D3ZQ57EUlK6N1pb3hzSQdJ2lnSDpKOTL80XgZWp+U1cwnwEZIceC3N06re7fgycJqkzYHvAWfUTsZKWpi+XzU3pOXfExEbgF8BHwcejoiRdJ+t0rY8C2xBki5q5SZglOTzmS3p/SRfJNYnHPgtTz/VpuP4f5zZdjPJycaVJCcVPxARzzY4xpuBmyWtBq4CTo2Ih9N93wv8FUlAOw14b0SsTF93PMkJ5OdITh7/oHbAiHicpAf8WWCEpFf930n+Pwykx3wqfe3bmeh9N3JV2o5nIuLOyerd4jhZV5N8WXwiIn4MfAW4LE3TLAMOz+z7W2AeE737e0h+MdyY2ecHJKmaJ9Ptv2tVePoF8n7gxLQexwJXtFl36wHyjVisaJJOBD4eEQeUXRezKnKP38ysYhz4zcwqxqkeM7OKcY/fzKxiemKStu222y6WLFlSdjXMzHrKbbfdtjIiBuvX90TgX7JkCcPDw2VXw8ysp0hqeMW1Uz1mZhXjwG9mVjEO/GZmFePAb2ZWMQ78ZmYV48BvZlYxDvxmZhXjwG9m1mUGBpLHoznd98yB38ysi6xfDxHJI68JC3IL/JLOlbRC0rIG2z4jKSRt1+i1ZmZVtf32+ZeRZ4//fOCw+pWSdgEOBR7LsWwzs560alX+ZeQW+CPiRpJb19X7Oslt8jwftJnZJMbavVPzFBSa45d0JPBk3b1Jm+17sqRhScMjIyOT7W5m1vOOOOKV6558svPlFBb4JW0BfA74n+3sHxFnRcRQRAwNDr5iVlEzs77zi19MLB95ZPL3iSc6X06RPf5XAbsDd0p6BNgZuF3SHxVYBzOzrjU+PrG8997J3+XLO19OYfPxR8TvgX8/X50G/6GIWFlUHczMukkESI23HX98kubZb7/Ol5vncM5LgZuAvSQ9IemkvMoyM+s1UnKRVi3wZ78AdtoJ9toLzj4bXv/6zpedW48/Ij44yfYleZVtZtbNJjttmccJ3SxfuWtmVrCVdQnuiMbLeXHgNzObJil5jI5OrIuAl1+e2nEGCo7EPXGzdTOzbjZnzkRPPRvEi+i9T4d7/GZmHVDr/U/F7JK63g78ZlZJGzZsOm5+qtoJ8pPts3Hj9MufCad6zKyS5s6dWM4zJSNtevyFC/Mrq13u8ZuZTVH2ZG47dtttYvmll5rvd8kl06vPVDnwm1nlTDUXX2/OnKnt/9hjzc8BLF48sfzBllc/dY4Dv5n1ldo0CDMN7lOxbh2sWZMsr107tdRRHpOwTcY5fjPrK9nhlPX59U6o/0IZH59YV38hVkTzMfqXXbbpvkVyj9/M+l7tF0AeAbbVL4tW2445pvN1aZcDv5n1jUaBNrsuOynaZK/LW9FX625SdnlFm5nNTAQ8/3xxgbu+nG69MncyDvxm1rMGBmDRomR5psG/lg6qPzH84ouNTxa3G/Sz1wt0Cwd+M7Mm1q6Frbee2THWr3/lulZj+YvgUT1mZg20+gUx1RRP9k5b3ZAeco/fzCprqlfgzkRteGc3cI/fzPpSbXz92BjMmjWxvpt63mVxj9/MetJkqZja9mzQr22bLOjXby9rFs28OPCbWc/p5LDKRlfb1itzzH0enOoxs8qb7ItjYKC/UkO5fY9JOlfSCknLMuv+j6R7Jd0l6ceSts6rfDPrT0VdrFXr/fdTwK/J8wfM+cBhdeuuA14XEW8A/g04I8fyzazPlDG1Qj/KLfBHxI3Ac3Xrro2I2gCq3wE751W+mVVDrUde9kVRvaTMUxYfA37ebKOkkyUNSxoeGRkpsFpm1iuyaZitturf1EynlRL4JX0OGAUubrZPRJwVEUMRMTQ4OFhc5czM+lzho3oknQC8Fzgkwt/NZtacL7bKR6E9fkmHAf8DODIi1hZZtpn1luyJ3PrZMVev9pfBTOQ5nPNS4CZgL0lPSDoJ+A6wFXCdpKWSvpdX+WbWv7bcsuwa9LbcUj0R0eh+8efkVZ6ZmbWnzy5ENrN+5xTPzDnwm5lVjOfqMbOuMNlVue7pd44Dv5mVbtWq5tsc8DvPqR4zK92CBWXXoFoc+M2sVK3m2BkfL64eVeLAb2alWriw+TbPxpkPB34z60rO7efHgd/Muo5TPPnyqB4zK0Un75trU+Mev5lZxTjwm1nhfNK2XE71mFlhHPC7g3v8ZmYV48BvZqXzid1iOdVjZoVolOZxwC+HA7+Z5c5DN7uLUz1mlisH+e7jHr+Z5Wog0730l0B3cI/fzHLj4ZvdKbfAL+lcSSskLcusWyTpOkn3p3+3yat8MzNrLM8e//nAYXXrTgeuj4g9gevT52ZWAStXll0Dq8kt8EfEjcBzdauPAi5Ily8Ajs6rfDPrLttuW3YNrKboHP8OEfE0QPp3+4LLN7MS+KRud+nak7uSTpY0LGl4ZGSk7OqY2RT5xG73KjrwPyNpR4D074pmO0bEWRExFBFDg4ODhVXQzKzfFR34rwJOSJdPAH5ScPlmZpWX53DOS4GbgL0kPSHpJODLwKGS7gcOTZ+bWZ/ZsKHsGlgruV25GxEfbLLpkLzKNLPuMHfuxLJP7Hafrj25a2Zm+XDgNzOrGAd+M7OKceA3s9w4v9+dHPjNrKN84Vb3c+A3M6sYB34z6xj39nuDA7+ZWcU48JtZR9SfyPWJ3e7lwG9mHZG9t+7YWHn1sMk58JvZjNXn9gccWbqaPx4zs4ppK/BLOlXSAiXOkXS7pHflXTkz6z3O7Xe/dnv8H4uIl4B3AYPAR/GUymZmPandwF/L4B0BnBcRd2bWmZkB7u33inYD/22SriUJ/L+QtBUwnl+1zKxXjI6WXQObqnZvxHISsDfwUESslbQtSbrHzCpuzpyya2BT1TLwS9q3btUe8jXZZoanZ+hlk/X4v5r+3Rx4E3AXSW7/DcDNwAH5Vc3MzPLQMscfEe+IiHcAjwJvioihiHgTsA/wQBEVNLPu06i37xO7vaPdk7uviYjf155ExDKSnL+ZmfWYdgP/vZLOlnSQpLdL+j6wfLqFSvpLSXdLWibpUkmbT/dYZmY2Ne0G/hOBu4FTgU8D9zDNUT2SFgOnAEMR8TpgFnDcdI5lZuXbsMFpnl4z6XBOSbOAn0XEO4Gvd7DceZI2AlsAT3XouGZWIAf83jRpjz8ixoC1khZ2osCIeBL4v8BjwNPAixFxbf1+kk6WNCxpeGRkpBNFm1kHeBhn72s31bMe+H06Qdu3ao/pFChpG+AoYHdgJ2BLSR+u3y8izkpHEQ0NDg5OpygzM2ug3St3r04fnfBO4OGIGAGQdAXwNuCiDh3fzMxaaCvwR8QFHSzzMeAtkrYA1gGHAMMdPL6ZmbXQVuCXtCfwJeC1JFfxAhARe0y1wIi4WdLlwO3AKHAHcNZUj2Nmxcvm9zduLK8eNjPtpnrOA75AMqrnHSRDOad9iicivpAez8x6RP1J3dntRg/rOu2e3J0XEdcDiohHI+JM4OD8qmVm3ULySJ5+0+539npJA8D9kv4CeBLYPr9qmVk3aBbwV6woth7WWe32+D9NcqHVKSSzdH4YOCGnOplZF4sAj7Dube32+J+NiNXAanwDFrNKcHqnf7Ub+M9P59i5FbgR+HV2tk4z6y/PPtt4vado6A/tjuM/UNJmwJuBg4CrJc2PiEV5Vs7M8pXt1deCuufa73/tjuM/APhP6WNr4GfAr/Orlpnlbc2a9vZbvz7feljx2k313EByde2XgGsiYkN+VTKzIsyf/8p1jXr2c+fmXxcrVruBf1tgf+BA4BRJ48BNEfH53GpmZoUbaHecn/W0dnP8L0h6CNgF2JlkUrU5eVbMzPLTzogd5/X7V7s5/geB+4DfAN8DPup0j1lvqQX7ZgHdwzero91Uz54RMZ5rTcysEO7tW7sZvVdLul7SMgBJb5D01znWy8w6qFmwj4C1a4uti5Wv3cD/feAMYCNARNyFb5Bu1hfmzdv0uXv7/a/dwL9FRNxSt26005Uxs+JkA3wEPPecg35VtBv4V0p6FRAAkj5AcqN0M+tSY2NTm1J5m23yrY91j3ZP7n6K5C5Zr5H0JPAw8KHcamVmM9boRiljYx6rb+2P438IeKekLUl+JawDjgUezbFuZtZhDvoGk6R6JC2QdIak70g6FFhLMg//A8CfFVFBM+uMcQ/IttRkPf4LgeeBm4BPAKcBmwFHR8TSfKtmZtM1mhl64RO2Vm+ywL9HRLweQNLZwEpg14hYlXvNzGxaxsdhjidUsRYmy/htrC1ExBjwcCeCvqStJV0u6V5JyyW9dabHNLNkBM+sWWXXwrrdZD3+N0p6KV0WMC99LiAiYsE0y/0m8M8R8YH0Bi9bTPM4ZmY2RS0Df0R0vO8gaQHJ9M4npmVsADzhm9kMjTa4pNL5fWukjMFdewAjwHmS7pB0djpMdBOSTpY0LGl4ZGSk+Fqa9YjaRVr1eX0HfWumjMA/G9gX+G5E7AOsAU6v3ykizoqIoYgYGhwcLLqOZj3BUynbdJQR+J8AnoiIm9Pnl5N8EZhZh2zcOPk+Vl2FB/6I+APwuKS90lWHAPcUXQ+zfjQ+nqR4Gk3XYFZT1j+P/wZcnI7oeQj4aEn1MOtZ9b165/StXaUE/vSq36EyyjbrZRs3Jr36uXNhs80m1jvo21T4B6FZD8kGe7Pp8lx9ZmYV48Bv1iOaDd18+eVi62G9z4HfrMutWtV6vL7TPzZVDvxmXW7BdGfEMmvCJ3fNeoxH8NhMucdv1sWyKZ41axz0rTMc+M16xBaevNw6xIHfzKxiHPjNukhtimWAZ54pty7Wv3xy16xLZPP5nm7Z8uQev1kP8Eld6yQHfrMu0KqHv2pVcfWwanCqx6xkrYL+6tWw5StuTGo2Mw78ZiVqFPSd1rG8OdVjVoAVK5K59Fevbr2fg74VwT1+s5y1O0LHQd+K4h6/WY48LNO6kQO/WU6mEvTd27ciOfCbdVj26tt62RukP/ZYMfUxq+ccv1kHNQv42R59dnl83OkgK15pPX5JsyTdIelnZdXBrGwO+laGMlM9pwLLSyzfrKMaBfHxcefvrfuUEvgl7Qy8Bzi7jPLNOq0+uEckD/forRuV1eP/BnAaMN5sB0knSxqWNDwyMlJYxcymY8DDJKyHFP7PVdJ7gRURcVur/SLirIgYioihwcHBgmpnNnX1vfrJrs41K1sZo3r2B46UdASwObBA0kUR8eES6mI2I2vWbPrco3SsFxTe44+IMyJi54hYAhwH/IuDvvWiVatg/vxN1znoWy/wOH6zafCsmtbLSg38EfEr4Fdl1sGsmRdegM02g1mzYPPNJ9I47tVbr3OP36yBRsG92cgd5/Wt13gQmlmdqQZxB33rNe7xm2V4Rk2rAvf4zdo0Njax7KBvvcw9fjPaH6XjgG/9wIHfKs35easip3qssloFfffsrZ+5x2+W4YBvVdDXgb/Wo/N/ZqtX39v3vxGrEqd6rPIc9K1qHPjNzCrGgd8qJ5vmcW/fqsiB38ysYhz4radt3NjefrVZNT1u36wigf/gg8uugeVBSqZNniygN9s23vSOz2b9rRKB/5e/LLsGVpZWwd29f6uqvh7Hb/1lskAtTZys9VW5Zs1Vosdv1dHqpigRDvpm4MBvPaJVD350dGJ51qzG+6xd29n6mPUyB37rai+9NHnaplmwz+4zb15n62XWyxz4rSuNjsLSpbBw4abra+ma+rRNs7nzndoxe6XCT+5K2gX4AfBHwDhwVkR8s+h6WPea7mgbB3mz9pQxqmcU+KuIuF3SVsBtkq6LiHtKqIt1mVbB24HdrDMKT/VExNMRcXu6vApYDiwuuh7WnQYa/It0ysass0rN8UtaAuwD3Nxg28mShiUNj4yMFF43K16jOfId8M06r7TAL2k+8CPg0xHxUv32iDgrIoYiYmhwcLD4ClohIhpPufDyy+XUx6wKSgn8kuaQBP2LI+KKMupg5RsdbZzagWQOHjPLRxmjegScAyyPiK8VW3bytz59UOt1Zp/XLgqaM6eYulXJZKN2nN4xy1cZPf79gf8CHCxpafo4osgKZKfoXbUq6XVm1w0MJD3O2syPK1duun26ww1HR+HZZ2FsbOZtyAbHbL0aHfvFF7tjQrLJ3rvVqx30zYpQeI8/In4DFB6GmgWcBQsmf22jUwzZCcGmW3bt9aOjMLvBJ/HCC7DFFhNfQO2YPTsJ/hs2vPJq1doxsvPZ1H7t1NZl92mWhpmOyaZcmOzqWzPrHM/OOQMbNiRBec0amD8/Wff447DDDkmKqJ3ZJBupfSFss8306jVZEJ1KQG816Vm7Gr1+bKyzXyxm1j7/10uNjW3ag1+/fvIbdcydmwS1WtAH2GWXqfXQG5lOOmnVqumX10p9Gmz9+k3fpw0bXpkGmywtFuGgb1amSvb4p3J1aKN9ZzqlQKde325vvPa68XF47DHYfffplQ8zm+zMvXyz7lC5wF/GycPJRhFN5/VT2b9mYACWLGn9+jxPAjvom3WHvv6vmFeQbzQ7ZPaCo2eeSZ63uvK0fpbJZvtO5b6wnbjStVm92j1udm78jRuTR30azczKVbkef6dNNjXwTI7XbWpfAOvXJ6ON1q2bOM9Rv5+ZdS8HfpsSKcnzO7ib9a6+TvXUm0raxMysX1Uq8HfD1atmZmWrVOA3M7MK5PidizYz25R7/GZmFePAb2ZWMQ78ZmYV48BvZlYxDvxmZhXjwG9mVjEO/GZmFePAb2ZWMYoeuMJJ0gjw6DRfvh2wsoPV6QVuczW4zdUwkzbvFhGvuGt4TwT+mZA0HBFDZdejSG5zNbjN1ZBHm53qMTOrGAd+M7OKqULgP6vsCpTAba4Gt7kaOt7mvs/xm5nZpqrQ4zczswwHfjOziunrwC/pMEn3SXpA0ull16dTJD0i6feSlkoaTtctknSdpPvTv9tk9j8jfQ/uk/Tu8mrePknnSlohaVlm3ZTbKOlN6Xv1gKRvSd17A84mbT5T0pPpZ71U0hGZbf3Q5l0k/VLSckl3Szo1Xd+3n3WLNhf3WUdEXz6AWcCDwB7AZsCdwGvLrleH2vYIsF3dur8DTk+XTwe+ki6/Nm37XGD39D2ZVXYb2mjjgcC+wLKZtBG4BXgrIODnwOFlt22KbT4T+EyDffulzTsC+6bLWwH/lratbz/rFm0u7LPu5x7/fsADEfFQRGwALgOOKrlOeToKuCBdvgA4OrP+soh4OSIeBh4geW+6WkTcCDxXt3pKbZS0I7AgIm6K5H/JDzKv6TpN2txMv7T56Yi4PV1eBSwHFtPHn3WLNjfT8Tb3c+BfDDyeef4Erd/cXhLAtZJuk3Ryum6HiHgakn9YwPbp+n56H6baxsXpcv36XvMXku5KU0G1lEfftVnSEmAf4GYq8lnXtRkK+qz7OfA3ynX1y9jV/SNiX+Bw4FOSDmyxbz+/DzXN2tgPbf8u8Cpgb+Bp4Kvp+r5qs6T5wI+AT0fES612bbCuJ9vdoM2Ffdb9HPifAHbJPN8ZeKqkunRURDyV/l0B/JgkdfNM+tOP9O+KdPd+eh+m2sYn0uX69T0jIp6JiLGIGAe+z0Sarm/aLGkOSQC8OCKuSFf39WfdqM1Fftb9HPhvBfaUtLukzYDjgKtKrtOMSdpS0la1ZeBdwDKStp2Q7nYC8JN0+SrgOElzJe0O7ElyQqgXTamNaYpglaS3pKMdPpJ5TU+oBb/Ufyb5rKFP2pzW8RxgeUR8LbOpbz/rZm0u9LMu+wx3zmfPjyA5Y/4g8Lmy69OhNu1Bcob/TuDuWruAbYHrgfvTv4syr/lc+h7cR5eOdGjQzktJfu5uJOnZnDSdNgJD6X+gB4HvkF6t3o2PJm2+EPg9cFcaAHbsszYfQJKeuAtYmj6O6OfPukWbC/usPWWDmVnF9HOqx8zMGnDgNzOrGAd+M7OKceA3M6sYB34zs4px4LfKkDSWmflwqSaZsVXSJyV9pAPlPiJpu5kex6xTPJzTKkPS6oiYX0K5jwBDEbGy6LLNGnGP3yov7ZF/RdIt6ePV6fozJX0mXT5F0j3pBFqXpesWSboyXfc7SW9I128r6VpJd0j6RzJzqkj6cFrGUkn/KGlW+jhf0rJ0bvW/LOFtsApx4LcqmVeX6jk2s+2liNiP5OrHbzR47enAPhHxBuCT6bovAnek6z5LMi0uwBeA30TEPiRXYO4KIOlPgGNJJtnbGxgDPkQyKdfiiHhdRLweOK9TDTZrZHbZFTAr0Lo04DZyaebv1xtsvwu4WNKVwJXpugOAPwWIiH9Je/oLSW6o8v50/dWSnk/3PwR4E3BreqOkeSSTj/0U2EPSt4GrgWun2T6ztrjHb5aIJss17wH+niRw3yZpNq2nxW10DAEXRMTe6WOviDgzIp4H3gj8CvgUcPY022DWFgd+s8Sxmb83ZTdIGgB2iYhfAqcBWwPzgRtJUjVIOghYGcm86tn1hwO1G2pcD3xA0vbptkWSdktH/AxExI+Az5PcftEsN071WJXMk7Q08/yfI6I2pHOupJtJOkMfrHvdLOCiNI0j4OsR8YKkM4HzJN0FrGViGuEvApdKuh24AXgMICLukfTXJHdPGyCZhfNTwLr0OLWO2Bkda7FZAx7OaZXn4ZZWNU71mJlVjHv8ZmYV4x6/mVnFOPCbmVWMA7+ZWcU48JuZVYwDv5lZxfx/mcu9XPR+KBYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 12] Cannot allocate memory return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             self.popen = subprocess.Popen(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1483\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-59506199960c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyvirtualdisplay/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mmanage_global_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanage_global_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyvirtualdisplay/xvfb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mmanage_global_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanage_global_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retries_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_helptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-displayfd\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyvirtualdisplay/util.py\u001b[0m in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stdout_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stderr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OSError exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moserror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEasyProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process was started (pid=%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 12] Cannot allocate memory return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
