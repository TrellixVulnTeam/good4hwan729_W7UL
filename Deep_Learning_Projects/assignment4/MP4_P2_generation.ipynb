{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "train len:  1003854\n",
      "test len:  111540\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = 'language_data/shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That for thy mother's fault art thus exposed\n",
      "To loss and what may follow! Weep I cannot,\n",
      "But my heart bleeds; and most accursed am I\n",
      "To be by oath enjoin'd to this. Farewell!\n",
      "The day frowns more and mo\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 2500\n",
    "hidden_size = 150\n",
    "n_layers = 3\n",
    "learning_rate = 0.01\n",
    "model_type = 'lstm'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(batch_size, device = device)\n",
    "    rnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i in range(chunk_len):\n",
    "        pred, hidden = rnn(input[:, i], hidden)\n",
    "        loss += criterion(pred.view(batch_size, -1), target[:, i])\n",
    "        \n",
    "    loss /= chunk_len\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 epochs...\n",
      "[3m 37s (50 2%) train loss: 3.2936, test_loss: 3.3917]\n",
      "WhLtdhe\n",
      "inaln,rPate etwndeadt  asrthee  rdn:dsOe.p\n",
      "fh\n",
      ",  weiisy \n",
      "\n",
      "e h kk tesroah,,eay\n",
      " s  rrTok \n",
      "\n",
      " i.\n",
      " \n",
      "\n",
      "[7m 16s (100 4%) train loss: 2.7673, test_loss: 2.7493]\n",
      "Whn eent oce ae miiet wiad uicw ihe md nf yol mseek popd, oao aae ghae pboaoie co ost-h her ihu tb erf \n",
      "\n",
      "[10m 56s (150 6%) train loss: 2.3043, test_loss: 2.3946]\n",
      "Whhan ady?\n",
      "\n",
      "DUKE RENLAS:\n",
      "Thit , ne elar houtles: he hon, uf batis heasos eath I am len oe thre, sit to \n",
      "\n",
      "[14m 35s (200 8%) train loss: 2.0715, test_loss: 2.1252]\n",
      "Whinss,\n",
      "Jeridt wang fusor o, litane to in, thaves\n",
      "with thave moude slape, bust let witon.\n",
      "\n",
      "VEORK:\n",
      "A! t \n",
      "\n",
      "[18m 15s (250 10%) train loss: 1.8892, test_loss: 1.9945]\n",
      "Who and torand; sharps thlolge.\n",
      "\n",
      "QUEEN LARGARET:\n",
      "Ay, 'Nheil in fake are be.\n",
      "\n",
      "First you, the shath.\n",
      "\n",
      "HI \n",
      "\n",
      "[21m 53s (300 12%) train loss: 1.7806, test_loss: 1.9284]\n",
      "Whan his debpon in we feans we in the ontes king\n",
      "That the om undip him he!\n",
      "On 'the she breat so store  \n",
      "\n",
      "[25m 34s (350 14%) train loss: 1.7161, test_loss: 1.8467]\n",
      "Whis wames the prain, am late take a kinters,\n",
      "Where tall than my can the brother oung come most it mon \n",
      "\n",
      "[29m 14s (400 16%) train loss: 1.6394, test_loss: 1.7490]\n",
      "Which yet to Charthy deajest the san\n",
      "That the prace show the tonger thy deap opt his it;\n",
      "Thou some the \n",
      "\n",
      "[32m 53s (450 18%) train loss: 1.5743, test_loss: 1.7352]\n",
      "Whis no disk to dole.\n",
      "\n",
      "MARIANA:\n",
      "Perain, it me in the from the stain\n",
      "And wrebellest youst ear of unfect \n",
      "\n",
      "[36m 33s (500 20%) train loss: 1.5346, test_loss: 1.7719]\n",
      "Whas remand, I will the hath stay let if\n",
      "The than be best you have that padpose.\n",
      "\n",
      "KING RICHARD III:\n",
      "Th \n",
      "\n",
      "[40m 13s (550 22%) train loss: 1.5183, test_loss: 1.7003]\n",
      "What say to a offer, so and epitors\n",
      "To breathe to thouse your dissing are your brother.\n",
      "\n",
      "BRUTUS:\n",
      "What  \n",
      "\n",
      "[43m 53s (600 24%) train loss: 1.5065, test_loss: 1.7012]\n",
      "What doth be sigal may with remain.\n",
      "\n",
      "LORD BERKELEY:\n",
      "I will we prince with good not\n",
      "Give to steathe him \n",
      "\n",
      "[47m 32s (650 26%) train loss: 1.4737, test_loss: 1.6783]\n",
      "Why lord?\n",
      "\n",
      "CAMILLO:\n",
      "If o'clack;\n",
      "When all not I shill all teard! The strength, and \n",
      "Tell ever coubles n \n",
      "\n",
      "[51m 11s (700 28%) train loss: 1.4634, test_loss: 1.6524]\n",
      "What well, whom are the part with never gona, \n",
      "Which therefore thou brearn to we'll place:\n",
      "But fearlit \n",
      "\n",
      "[54m 51s (750 30%) train loss: 1.4350, test_loss: 1.6707]\n",
      "Whist this no to his ends? he is the crown?\n",
      "\n",
      "First Murderer:\n",
      "And wremest force God is the power'd of t \n",
      "\n",
      "[58m 30s (800 32%) train loss: 1.4465, test_loss: 1.6552]\n",
      "When I bear this both and my danger him that\n",
      "Do dispain and had stands not them thee true:\n",
      "Sinhor that \n",
      "\n",
      "[62m 11s (850 34%) train loss: 1.4192, test_loss: 1.6463]\n",
      "Whisband, if I am be to this as,\n",
      "As in those two, and weary is--\n",
      "I'll being than with wast of and hear \n",
      "\n",
      "[65m 49s (900 36%) train loss: 1.3945, test_loss: 1.6498]\n",
      "Which away's entrainess be the saw their grief;\n",
      "But thank are someting of your friends.\n",
      "\n",
      "RICHARD:\n",
      "Here \n",
      "\n",
      "[69m 28s (950 38%) train loss: 1.3934, test_loss: 1.6396]\n",
      "Where no majesty may seegin my genelites,\n",
      "Which contended the look; which is a grace\n",
      "For heart's dear  \n",
      "\n",
      "[73m 8s (1000 40%) train loss: 1.3930, test_loss: 1.5760]\n",
      "Whe may both as a worthen, and in surmonous do have this thee them not\n",
      "A pride up win the pleasure the \n",
      "\n",
      "[76m 47s (1050 42%) train loss: 1.3562, test_loss: 1.6378]\n",
      "Which double from his thanks: and, their dreases,\n",
      "By my house, thou shalt be your fitle in the foul li \n",
      "\n",
      "[80m 27s (1100 44%) train loss: 1.3668, test_loss: 1.6160]\n",
      "What you come where ever this heaven way;\n",
      "Can mark thee in thou very settled the hand to have\n",
      "person a \n",
      "\n",
      "[84m 6s (1150 46%) train loss: 1.3434, test_loss: 1.6124]\n",
      "Why be changed her secquences and the majesty:\n",
      "A good good the conveyance jested nor more!\n",
      "Shall not t \n",
      "\n",
      "[87m 47s (1200 48%) train loss: 1.3451, test_loss: 1.6442]\n",
      "When all her! Let her business spices friends,\n",
      "That deeds thing dew\n",
      "Where a bunts of England, that all \n",
      "\n",
      "[91m 26s (1250 50%) train loss: 1.3671, test_loss: 1.5937]\n",
      "What, having I am worships of the semble,\n",
      "I did me, out of our father.\n",
      "\n",
      "DORCAS:\n",
      "I slain hands, she is  \n",
      "\n",
      "[95m 6s (1300 52%) train loss: 1.3226, test_loss: 1.5839]\n",
      "Whether you see, and she in the fuuntating our sons,\n",
      "And did for it had not but the brother,\n",
      "That we s \n",
      "\n",
      "[98m 47s (1350 54%) train loss: 1.3264, test_loss: 1.5808]\n",
      "Whre more speak the common father's gentle doth den's own\n",
      "pernal, not the birth and been speak?\n",
      "\n",
      "Nurse \n",
      "\n",
      "[102m 27s (1400 56%) train loss: 1.3270, test_loss: 1.5475]\n",
      "Who slain from the ancient of her well other aged\n",
      "In those is come crean'd the brace:\n",
      "And I have a bou \n",
      "\n",
      "[106m 8s (1450 57%) train loss: 1.3383, test_loss: 1.6231]\n",
      "What it may I will be gone my lips.\n",
      "\n",
      "Lord:\n",
      "'Ya, fellow, as I cannot want-timbs\n",
      "The steal to her and ho \n",
      "\n",
      "[109m 49s (1500 60%) train loss: 1.3116, test_loss: 1.6004]\n",
      "What we do not in any determines beats,\n",
      "Then your thing of man lives\n",
      "Of our breast anjoys and comfort: \n",
      "\n",
      "[113m 30s (1550 62%) train loss: 1.3080, test_loss: 1.5801]\n",
      "What's thy hands and then their waste is not your majesty.\n",
      "\n",
      "First Sonspire:\n",
      "It is me to the folly lady \n",
      "\n",
      "[117m 12s (1600 64%) train loss: 1.2976, test_loss: 1.6054]\n",
      "Which comth, and love yours; how banish'd glorse,\n",
      "And not in service of good holy words\n",
      "I prince back  \n",
      "\n",
      "[120m 52s (1650 66%) train loss: 1.3150, test_loss: 1.6332]\n",
      "What heavy houses to be down like\n",
      "be means to thing, to your tafession!\n",
      "\n",
      "GLOUCESTER:\n",
      "Nay, let them dot \n",
      "\n",
      "[124m 34s (1700 68%) train loss: 1.3205, test_loss: 1.6151]\n",
      "Which will I did banished with the straiger court.\n",
      "\n",
      "Third Consprised:\n",
      "What, some he that say your eyes \n",
      "\n",
      "[128m 14s (1750 70%) train loss: 1.2954, test_loss: 1.6395]\n",
      "What by brother's proclaim but thee some both,\n",
      "Becauses a speak and be attend the house of heaven:\n",
      "And \n",
      "\n",
      "[131m 56s (1800 72%) train loss: 1.2948, test_loss: 1.5455]\n",
      "Where by words than thou lies so thou hast not sleam'd\n",
      "Shall prosperate stand so beggarons\n",
      "And sevence \n",
      "\n",
      "[135m 36s (1850 74%) train loss: 1.3038, test_loss: 1.6364]\n",
      "Whe more become of her field he and report\n",
      "That have no more sense, sir, and he not stand been,\n",
      "Was th \n",
      "\n",
      "[139m 16s (1900 76%) train loss: 1.2648, test_loss: 1.5171]\n",
      "What you be dare to see the daughter there?\n",
      "\n",
      "Gardener:\n",
      "It home contempen to the justice.\n",
      "\n",
      "First Murder \n",
      "\n",
      "[142m 58s (1950 78%) train loss: 1.2706, test_loss: 1.5754]\n",
      "Why state you come himself too means,\n",
      "Maintaccupails, will never prayed in revenge\n",
      "Into me gone the be \n",
      "\n",
      "[146m 39s (2000 80%) train loss: 1.2789, test_loss: 1.6089]\n",
      "Whresh I live heard down hither king, as I tell written\n",
      "And save and lows not now he the how of the po \n",
      "\n",
      "[150m 21s (2050 82%) train loss: 1.2731, test_loss: 1.6594]\n",
      "Whild it be lawmongs to warm hearts;\n",
      "And follow for unappeal of men!\n",
      "Why have late his part the bajest \n",
      "\n",
      "[154m 0s (2100 84%) train loss: 1.2573, test_loss: 1.5629]\n",
      "What is nothing in Baster hour enemy.\n",
      "\n",
      "KING EDWARD IV:\n",
      "My Lord of York to my mortal two shoil avoided\n",
      " \n",
      "\n",
      "[157m 40s (2150 86%) train loss: 1.2948, test_loss: 1.5573]\n",
      "Where I have not had man to be done and nid not to-news,\n",
      "And I am honour? Did not lives might so well\n",
      " \n",
      "\n",
      "[161m 21s (2200 88%) train loss: 1.2841, test_loss: 1.5435]\n",
      "What, do then you all flattering there\n",
      "In thrives unto the very blows,\n",
      "Or stand the life.\n",
      "\n",
      "VOLUMNIA:\n",
      "W \n",
      "\n",
      "[165m 2s (2250 90%) train loss: 1.2831, test_loss: 1.6619]\n",
      "What in the new was say, if the endure have procetment\n",
      "Who will make high hath wounds restivant:\n",
      "By ma \n",
      "\n",
      "[168m 43s (2300 92%) train loss: 1.2743, test_loss: 1.6664]\n",
      "What, much it well that you shall not;\n",
      "The strokes their firm more that kind shepherd of it,\n",
      "The one c \n",
      "\n",
      "[172m 23s (2350 94%) train loss: 1.2671, test_loss: 1.5620]\n",
      "Which persuades and bloods, and this office.\n",
      "Wherefore, my father with me.\n",
      "\n",
      "LEONTES:\n",
      "I hear me to me i \n",
      "\n",
      "[176m 3s (2400 96%) train loss: 1.2493, test_loss: 1.6064]\n",
      "Wherefore us thou art thou then borrow so;\n",
      "And my lords, that thou art not that one\n",
      "Than she are most  \n",
      "\n",
      "[179m 45s (2450 98%) train loss: 1.2899, test_loss: 1.5931]\n",
      "What my news\n",
      "Would not but insultise your child, may\n",
      "treat a night to be rest, whence a wise that mean \n",
      "\n",
      "[183m 25s (2500 100%) train loss: 1.2451, test_loss: 1.5818]\n",
      "Wherein they so mouds valiant the former\n",
      "your fear,--which is already thus\n",
      "He was a flattering graves  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "# torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1250ebef10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou hadst sleep out thee, and sixteen many and me\n",
      "To slain to make him with thee so long;\n",
      "And yet I before your most nature; which will poor hour?\n",
      "I will this Romeo earth and here he eyes!\n",
      "When rest we low the children, I'll have a turn\n",
      "With this lady's house of you and a grown.\n",
      "\n",
      "Nurse:\n",
      "O thou know and purchance promotion:\n",
      "Withal doar'd her!\n",
      "I know it before affection:\n",
      "Shall then your noble father could have to keeps\n",
      "A warrant me to bring him speak to thy slain.\n",
      "\n",
      "GLOUCESTER:\n",
      "Where's the greeting that blood before me,\n",
      "Become him with to use the party of the duke;\n",
      "Nor every reasons and graces: one talk?\n",
      "\n",
      "GREMIO:\n",
      "Which very thing a country is duty.\n",
      "\n",
      "JULIET:\n",
      "The willo colours nor the kings; we'll not most a-graod;\n",
      "And so, by your sinelling with enjoying,\n",
      "Call the king? Romeo father, that makes me come\n",
      "Shall be so offended bolts here our country,\n",
      "That I remember thy souls for it of court.\n",
      "\n",
      "KING RICHARD II:\n",
      "Swaul call the people, what, there is bones reconcile?\n",
      "Therefore now thou diest to dea\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
